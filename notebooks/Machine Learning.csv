,title,url,id,start,end,duration,text
0,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,0.0,20.0,20.0," Gonna start this stack quest with the silly song. But if you don't like silly songs, that's okay. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to do a gentle introduction to machine learning."
1,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,21.0,41.0,20.0," Note, this stack quest was originally prepared for and presented at the Society for Scientific Advancements Annual Conference. One of the things that Sosa does is promote science and technology in Jamaica. Let's start with a silly example. Do you like silly songs?"
2,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,42.0,69.0,27.0," If you like silly songs, are you interested in machine learning? If you like silly songs and machine learning, then you'll love Stack Quest. If you like silly songs, but not machine learning, are you interested in statistics? If you like silly songs and statistics, but not machine learning, then you'll still love Stack Quest. Otherwise, you might not like Stack Quest."
3,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,69.0,97.0,28.0," Why not why? If you don't like silly songs, are you interested in machine learning? If you don't like silly songs, but you like machine learning, then you'll love Stack Quest. If you don't like silly songs or machine learning, are you interested in statistics? If you don't like silly songs or machine learning, but you're interested in statistics, then you will love Stack Quest."
4,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,98.0,127.0,29.0," Otherwise, you might not like Stack Quest. Why not why? This is a silly example, but it illustrates a decision tree, a simple machine learning method. The purpose of this particular decision tree is to predict whether or not someone will love Stack Quest. Alternatively, we could say that this decision tree classifies a person as either someone who loves Stack Quest or someone who doesn't."
5,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,128.0,155.0,27.0," Since decision trees are a type of machine learning, then if you understand how we use this tree to predict or classify if someone would love Stack Quest, you're well on your way to understand it. This is a very simple machine learning machine learning. Bam! Here's another silly example of machine learning. Imagine we measured how quickly someone could run 100 meters."
6,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,156.0,172.0,16.0," And how much EM they ate. This is me. I'm not very fast, and I don't eat much EM. And this is Usain Bolt. Usain Bolt is very fast, and he eats a lot of EM."
7,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,173.0,199.0,26.0," Given this pretend data, we see that the more EM someone eats, the faster they run the 100 meter dash. We can fit a black line to the data to show the trend. But we can also use the black line to make predictions. For example, if someone told us they ate this much EM, then we could use the black line to predict how fast that person might run."
8,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,200.0,221.0,21.0," This is the predicted speed. The black line is a type of machine learning because we can use it to make predictions. In general, machine learning is all about making predictions and classifications. Bam! Now that we can make predictions and classifications,"
9,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,221.0,245.0,24.0," let's talk about some of the main ideas in machine learning. First of all, in machine learning Lingo, the original data is called training data. So the black line is fit to training data. Alternatively, we could have fit a green squiggle to the training data. The green squiggle fits the training data better than the black line,"
10,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,245.0,273.0,28.0," but remember, the goal of machine learning is to make predictions. So we need a way to decide if the green squiggle is better or worse than the black line at making predictions. So we find a new person and measure how fast they run and how much EM they eat. And then we find another and another and another. Altogether, the blue dots represent testing data."
11,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,273.0,305.0,32.0," We use the testing data to compare the predictions made by the black line to the predictions made by the green squiggle. Let's start by seeing how well the black line predicts the speed of each person in the testing data. Here's the first person in the testing data. They ate this much EM and they ran this fast. However, the black line predicts that someone who ate this much EM should run a little slower."
12,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,306.0,329.0,23.0, So let's measure the distance between the actual speed and the predicted speed. And save the distance on the right while we focus on the other people in the testing data. Here's the second person in the testing data. They ate this much EM and they ran this fast. But the black line predicts that they will run a little faster.
13,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,330.0,364.0,34.0, So we measure the distance between the actual speed and the predicted speed and add it to the one we measured for the first person in the testing data. Then we measure the distance between the real and the predicted speed for the third person in the testing data. And add it to our running total of distances between the real and predicted speeds for the black line. Then we do the same thing for the fourth person in the testing data. And add that distance to our running total for the black line.
14,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,365.0,402.0,37.0," This is the sum of all the distances between the real and predicted speeds for the black line. Now let's calculate the distances between the real and predicted speeds using the green squiggle. Remember, the green squiggle did a great job fitting the training data. But when we are doing machine learning, we are more interested in how well the green squiggle can make predictions with new data. So just like before, we determine this person's real speed and their predicted speed and measure the distance between them."
15,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,403.0,434.0,31.0," And just like we did for the black line, we'll keep track of the distances for the green squiggle over here. Then we do the same thing for the second person in the testing data. And the third person and the fourth person. This is the sum of the distances between the real and predicted speeds for the green squiggle. The sum of the distances is larger for the green squiggle than the black line."
16,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,434.0,469.0,35.0," In other words, even though the green squiggle fit the training data way better than the black line, the black line did a better job predicting speeds with the testing data. So if we had to choose between using the black line or the green squiggle to make predictions, we would choose the black line. Bam! This example teaches two main ideas about machine learning. First, we use testing data to evaluate machine learning methods."
17,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,470.0,508.0,38.0," Second, don't be fooled by how well a machine learning method fits the training data. Note, fitting the training data well that making poor predictions is called the bias variance tradeoff. Oh no, a shameless self-promotion. If you want to learn more about the bias variance tradeoff, there's a stat quest that will walk you through it one step at a time. Before we move on, you may be wondering why we used a simple black line in a silly green squiggle instead of a deep learning compositional neural network work."
18,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,508.0,540.0,32.0," Or, insert or twist with fastest, most fancy machine learning method here. There are tons of fancy sounding machine learning methods, and each year something new and exciting comes on the scene. But regardless of what you use, the most important thing isn't how fancy it is, but how it performs with testing data. Double-bowl. Now let's go back to the decision tree that we started with."
19,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,541.0,564.0,23.0," Remember, we wanted to classify if someone loves stat quest based on a few questions. To create the decision tree, we collected data from people who love stat quest. And from people who did not love stat quest. All together, this was the training data. And we used it to build the decision tree."
20,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,565.0,589.0,24.0," Then we got data from a few more people who love stat quest. And a few more people who did not love stat quest. All together, this forms the testing data. We can use the testing data to see how well our decision tree predicts if someone will love stat quest. The first person in the testing data did not like silly songs."
21,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,590.0,609.0,19.0, So we go to the right side of the decision tree. They didn't like machine learning either. So we just keep on going down the right side of the decision tree. They didn't like statistics either. So the decision tree predicts that this person will not love stat quest.
22,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,609.0,636.0,27.0," However, this person loves stat quest so the decision tree made a mistake. The second person in the testing data liked silly songs. And that takes us down the left side of the decision tree. They were also interested in machine learning. So we predict that that person loves stat quest."
23,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,637.0,668.0,31.0," And since this person actually loves stat quest, the decision tree did a good job. Hey! Now we just run all of the other people in the testing data down the decision tree and compare the predictions to reality. Then we can compare this decision tree to the latest greatest machine learning method. Ultimately, we pick the method that does the best job predicting if someone will love stat quest or not."
24,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,669.0,695.0,26.0," Triple BAM In summary, machine learning is all about making predictions and classifications. There are tons of fancy machine learning methods, but the most important thing to know about them isn't what makes them so fancy. It's that we decide which method fits our needs the best by using testing data. One last thing before we go."
25,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,696.0,729.0,33.0," You may be wondering how we decide which data go into the training set and which data go into the testing set. Earlier, we just arbitrarily decided that these red dots were the training data. But the blue dots could have, just as easily, been the training data. The good news is that there are ways to determine which samples should be used for training data and which samples should be used for testing data. And if you're interested in learning more about this, check out the stat quest."
26,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,730.0,761.0,31.0," And there are lots more stat quests that walk you through machine learning concepts step by step, so check them out. Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, consider buying one or two of my original songs, or getting a t-shirt or a hoodie or some other slick merchandise. There's links on the screen and there's links in the description below."
27,A Gentle Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,Gv9_4yMHFhI,762.0,764.0,2.0," Alright, until next time, quest on."
28,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,0.0,25.0,25.0," SetQuest, check it out, talking about machine learning. SetQuest, check it out, talking about cross validation. SetQuest. Hello, I'm Josh Starmer and welcome to SetQuest. Today we're going to talk about cross validation and it's going to be clearly explained."
29,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,25.0,54.0,29.0," Okay, let's start with some data. We want to use the variables, chest pain, good blood circulation, etc. To predict if someone has heart disease, then when a new patient shows up, we can measure these variables and predict if they have heart disease or not. However, first we have to decide which machine learning method would be best."
30,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,54.0,79.0,25.0, We could use logistic regression or k-nearist neighbors or support vector machines and many more machine learning methods. How do we decide which one to use? Cross validation allows us to compare different machine learning methods and get a sense of how well they will work in practice.
31,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,79.0,111.0,32.0," Imagine that this blue column represented all of the data that we have collected about people with and without heart disease. We need to do two things with this data. One, we need to estimate the parameters for the machine learning methods. In other words, to use logistic regression, we have to use some of the data to estimate the shape of this curve. In machine learning lingo, estimating parameters is called training the algorithm."
32,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,111.0,155.0,44.0," The second thing we need to do with this data is evaluate how well the machine learning methods work. In other words, we need to find out if this curve will do a good job categorizing new data. In machine learning lingo, evaluating a method is called testing the algorithm. Thus, using machine learning lingo, we need the data to one train the machine learning methods and to test the machine learning methods. A terrible approach would be to use all the data to estimate the parameters i.e. to train the algorithm."
33,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,155.0,201.0,46.0," Because then we wouldn't have any data left to test the method. We're using the same data for both training and testing is a bad idea because we need to know how the method will work on data it wasn't trained on. A slightly better idea would be to use the first 75% of the data for training, and the last 25% of the data for testing. We could then compare methods by seeing how well each one categorized the test data. But how do we know that using the first 75% of the data for training in the last 25% of the data for testing is the best way to divide up the data?"
34,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,201.0,234.0,33.0," What if we use the first 25% of the data for testing? Or what about one of these metal blocks? Rather than worry too much about which block would be best for testing, cross validation uses them all one at a time and summarizes the results at the end. For example, cross validation would start by using the first three blocks to train the method. And then use the last block to test the method."
35,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,234.0,256.0,22.0," And then it keeps track of how well the method did with the test data. Then it uses this combination of blocks to train the method. And this block is used for testing. And then it keeps track of how well the method did with the test data. Et cetera, et cetera, et cetera."
36,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,256.0,285.0,29.0," In the end, every block of data is used for testing, and we can compare methods by seeing how well they performed. In this case, since the support vector machine did the best job classifying the test data sets, we'll use it. Bam. Note, in this example, we divided the data into four blocks. This is called four fold cross validation."
37,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,285.0,310.0,25.0," However, the number of blocks is arbitrary. In an extreme case, we could call each individual patient or sample a block. This is called leaf one out cross validation. Each sample is tested individually. That said, in practice, it is very common to divide the data into 10 blocks."
38,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,310.0,329.0,19.0," This is called 10 fold cross validation. Double bam. One last note, before we're done. Say like we wanted to use a method that involved a tuning parameter. A parameter that isn't estimated, but is just sort of guessed."
39,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,329.0,351.0,22.0," For example, Ridge regression has a tuning parameter. Then we could use 10 fold cross validation to help find the best value for that tuning parameter. Tiny bam. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
40,Machine Learning Fundamentals: Cross Validation,https://www.youtube.com/watch?v=fSytzGwwBVw,fSytzGwwBVw,351.0,363.0,12.0," And if you want to support stat quest, well, please click the like button down below, and consider buying one of my original songs. All right. Until next time, quest on."
41,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,0.0,25.0,25.0," If you feel confused, don't sweat it. Stack Quest is here. Stack Quest. Hello, I'm Josh Stomor and welcome to Stack Quest. Today we're going to cover another machine learning fundamental, the confusion matrix, and it's going to be clearly explained."
42,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,25.0,60.0,35.0," Imagine that we have this medical data. We've got some clinical measurements, like chest pain, good blood circulation, blocked arteries, and weight. And we want to apply a machine learning method to them to predict whether or not someone will develop heart disease. To do this, we could use logistic regression, or k-nearest neighbors, or random forest, or some other method, there are tons to choose from. How do we decide which one works best with our data?"
43,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,60.0,92.0,32.0," We start by dividing the data into training and testing sets. Note, this would be an excellent opportunity to use cross-validation, and if you're not familiar with that, we'll check out the stack quest. Then we train all of the methods we're interested in with the training data. And then test each method on the testing set. Now we need to summarize how each method performed on the testing data."
44,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,92.0,123.0,31.0," One way to do this is by creating a confusion matrix for each method. The rows in a confusion matrix correspond to what the machine learning algorithm predicted. And the columns correspond to the known truth. Since there are only two categories to choose from, heart disease, or does not have heart disease, then the top left corner contains true positives."
45,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,123.0,152.0,29.0," These are patients that had heart disease that were correctly identified by the algorithm. The true negatives are in the bottom right hand corner. These are patients that did not have heart disease that were correctly identified by the algorithm. The bottom left hand corner contains false negatives. False negatives are when a patient has heart disease, but the algorithm said they didn't."
46,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,152.0,184.0,32.0," Lastly, the top right hand corner contains false positives. False positives are patients that do not have heart disease, but the algorithm says they do. For example, when we applied the random forest to the testing data, there were 142 true positives, patients with heart disease that were correctly classified. And 110 true negatives, patients without heart disease that were correctly classified."
47,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,185.0,213.0,28.0," However, the algorithm misclassified 29 patients that did have heart disease by saying they did not. These are false negatives. In the algorithm misclassified 22 patients that did not have heart disease by saying that they did. These are false positives. The numbers along the diagonal, the green boxes, tell us how many times the samples were correctly classified."
48,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,214.0,246.0,32.0," The numbers not on the diagonal, the red boxes, are samples that the algorithm messed up. Now we can compare the random forest confusion matrix to the confusion matrix we get when we use K nearest neighbors. K nearest neighbors was worse than the random forest at predicting patients with the heart disease. 107 versus 142. And worse at predicting patients without heart disease."
49,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,246.0,276.0,30.0," 79 versus 110. So if we had to choose between using the random forest and K nearest neighbors, we would choose the random forest. Bam. Lastly, we can apply logistic regression to the testing data set and create a confusion matrix. These two confusion matrices are very similar and make it hard to choose which machine learning method is a better fit for this data."
50,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,276.0,315.0,39.0," We'll talk about more sophisticated metrics like sensitivity, specificity, ROC and AOC that can help us make a decision in the next stat quests. Now that we have the basic confusion matrix figured out, let's look at a more complicated one. Here's a new data set. Now the question is, based on what people think of these movies, Jurassic Park 3, run for your wife, out cold spelled with a K and Howard the Duck, can we use a machine learning method to predict their favorite movie?"
51,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,315.0,351.0,36.0," If the only options for favorite movie were troll 2, gore police, or cool as ice, then the confusion matrix would have three rows and three columns. But just like before, the diagonal, the green boxes, are where the machine learning algorithm did the right thing. And everything else is where the algorithm messed up. In this case, the machine learning algorithm didn't do very well, but can you blame it? These are all terrible movies. Bam."
52,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,351.0,388.0,37.0," Ultimately, the size of the confusion matrix is determined by the number of things we want to predict. In the first example, we were only trying to predict two things if someone had hard disease or if they didn't. In that gave us a confusion matrix with two rows and two columns. In the second example, we had three things to choose from, and a confusion matrix with three rows and three columns. If we had four things to choose from, we get a confusion matrix with four rows and four columns."
53,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,388.0,415.0,27.0," And if we had four things to choose from, we get a confusion matrix with 40 rows and 40 columns. Double bam. In summary, a confusion matrix tells you what your machine learning algorithm did right and what it did wrong. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
54,Machine Learning Fundamentals: The Confusion Matrix,https://www.youtube.com/watch?v=Kdsp6soqA7o,Kdsp6soqA7o,415.0,424.0,9.0," And if you want to support stat quest, well, consider buying one or two of my original songs. All right, until next time, quest on."
55,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,0.0,22.0,22.0," When the well runs dry, you might be thirsty, but there's still stack quest you can watch it. Stack Quest Hello, I'm Josh Starman, welcome to Stack Quest. Today we're going to continue our series on machine learning fundamentals, and we're going to talk about sensitivity and specificity."
56,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,22.0,45.0,23.0," They're going to be clearly explained. This Stack Quest follows up on the one that describes the confusion matrix. So if you're not already down with that, check out the Quest. The first half of this video will explain how to calculate and interpret sensitivity and specificity when you have a confusion matrix with two rows and two columns."
57,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,45.0,67.0,22.0," And the second half will show you how to calculate and interpret sensitivity and specificity when you have three or more rows and columns. Even if you're already down with the confusion matrix, let's remember that rows correspond to what was predicted. And columns correspond to the known truth."
58,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,67.0,88.0,21.0," When there are only two categories to choose from, in this case, the two choices were has heart disease or does not have heart disease. Then, the top left hand corner contains the true positives. True positives are patients that have heart disease that were also predicted to have heart disease."
59,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,89.0,107.0,18.0, ChurNextGuys are in the bottom right hand corner. true negatives are patients that did not have heart disease and were predicted not to have heart disease. The bottom left hand corner contains the false negatives. False negatives are when a patient has heart disease
60,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,107.0,125.0,18.0," but the prediction said they didn't. Lastly, the top right hand corner contains the false positives. False positives are patients that do not have heart disease but the prediction says that they do. Once we filled out the confusion matrix,"
61,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,125.0,147.0,22.0," we can calculate two useful metrics, sensitivity and specificity. In this case, sensitivity tells us what percentage of patients with heart disease were correctly identified. sensitivity is the true positives, divided by the sum of the true positives and the false negatives."
62,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,149.0,170.0,21.0," Specificity tells us what percentage of patients without heart disease were correctly identified. Specificity are the true negatives divided by the sum of the true negatives and the false positives. In the stat quest on the confusion matrix,"
63,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,170.0,191.0,21.0," we applied logistic regression to a testing data set and ended up with this confusion matrix. Let's start by calculating sensitivity for this logistic regression. Here's the formula for sensitivity. And for true positives, we plug in 139."
64,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,192.0,213.0,21.0," And for false negatives, we plug in 32. When we do the math, we get 0.81. sensitivity tells us that 81% of the people with heart disease were correctly identified by the logistic regression model. Now let's calculate the specificity."
65,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,214.0,236.0,22.0," Here's the formula for specificity. And for true negatives, we'll plug in 112. And for false positives, we'll plug in 20. When we do the math, we get 0.85. Specificity tells us that 85% of the people without heart disease"
66,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,237.0,255.0,18.0, were correctly identified by the logistic regression model. Now let's calculate sensitivity and specificity for the random forest model that we used in the confusion matrix stat quest. Here's the confusion matrix. Here's the formula for sensitivity.
67,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,256.0,275.0,19.0," And when we plug in the numbers, we get 0.83. Here's the formula for specificity. And when we plug in the numbers, we get 0.83 again. Now we can compare the sensitivity and specificity values that we calculated for the logistic regression. To the values we calculated for the random forest."
68,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,276.0,300.0,24.0," sensitivity tells us that the random forest is slightly better at correctly identifying positives, which, in this case, are patients with heart disease. And for false positives, we can use the formula for specificity. We're correctly identifying positives, which, in this case, are patients with heart disease. Specificity tells us that logistic regression is slightly better at correctly identifying negatives,"
69,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,301.0,330.0,29.0," which, in this case, are patients without heart disease. We would choose the logistic regression model if correctly identifying patients without heart disease was more important than correctly identifying patients with heart disease. Alternatively, we would choose the random forest model if correctly identifying patients with heart disease was more important than correctly identifying patients without heart disease."
70,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,331.0,370.0,39.0," Bam. In the confusion matrix stat quest, we calculated this confusion matrix when we tried to predict someone's favorite movie. Now let's talk about how to calculate sensitivity and specificity when we have a confusion matrix with three rows and three columns. The big difference when calculating sensitivity and specificity for larger confusion matrices is that there are no single values that work for the entire matrix. Instead, we calculated different sensitivity and specificity for each category."
71,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,371.0,406.0,35.0," So for this confusion matrix, we'll need to calculate sensitivity and specificity for the movie troll 2 for the movie GORPOLACE and for the movie CUL as ICE. Let's start by calculating sensitivity for TURL 2. For TURL 2, there were 12 true positives. People that were correctly predicted to love TURL 2 more than GORPOLACE and CUL as ICE. So for true positives, we'll plug in 12."
72,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,407.0,442.0,35.0," And there were 112 plus 83, which equals 195 false negatives. People that loved TURL 2, but were predicted to love GORPOLACE or CUL as ICE. So for false negatives, we'll plug in 195. And when we do the math, we get 0.06. Sensitivity for TURL 2 tells us that only 6% of the people that loved the movie TURL 2 more than GORPOLACE or CUL as ICE or correctly identified."
73,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,443.0,474.0,31.0," Now let's calculate the specificity for TURL 2. There were 23 plus 77 plus 92 plus 17 equals 209 true negatives. People that were correctly predicted to like GORPOLACE or CUL as ICE more than TURL 2. So for true negatives, we'll plug in 209. And there were 102 plus 93 equals 195 false positives."
74,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,475.0,509.0,34.0," People that loved GORPOLACE or CUL as ICE the most, but were predicted to love TURL 2. So for false positives, we'll plug in 195. And when we do the math, we get 0.52. Specificity for TURL 2 tells us that 52% of the people who loved GORPOLACE or CUL as ICE more than TURL 2 were correctly identified. Calculating sensitivity and specificity for GORPOLACE is very similar."
75,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,510.0,535.0,25.0," Let's start by calculating sensitivity. There are 23 true positives. People that were correctly predicted to love GORPOLACE the most. And 102 plus 92 equals 194 false negatives. People who loved GORPOLACE the most, but were predicted to love TURL 2 or CUL as ICE more."
76,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,536.0,566.0,30.0," When we do the math, we get 0.11. Since the activity for GORPOLACE tells us that only 11% of the people that loved GORPOLACE were correctly identified. Now let's calculate specificity. They were 12 plus 93 plus 83 plus 17 equals 200 5 true negatives. People correctly identified as loving TURL 2 or CUL as ICE more than GORPOLACE."
77,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,567.0,601.0,34.0," And 112 plus 77 equals 189 false positives. People predicted to love GORPOLACE even though they didn't. And when we do the math, we get 0.52. Specificity for GORPOLACE tells us that 52% of the people that loved TURL 2 are CUL as ICE more than GORPOLACE were correctly identified. Lastly, calculating sensitivity and specificity for CUL as ICE follows the same steps."
78,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,602.0,633.0,31.0," We identify the true positives, the false positives, the true negatives, and the false negatives. And then plug in the numbers. First, for sensitivity, then for specificity. Double bam. If we had a confusion matrix with 4 rows and 4 columns, then we would have to calculate sensitivity and specificity for 4 different categories."
79,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,634.0,663.0,29.0," Little bam. In summary, sensitivity equals the true positives divided by the sum of the true positives and the false negatives. And specificity equals the true negatives divided by the sum of the true negatives and the false positives. We can use sensitivity and specificity to help us decide which machine learning method would be best for our data."
80,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,664.0,694.0,30.0," If correctly identifying positives is the most important thing to do with the data, we should choose a method with higher sensitivity. If correctly identifying negatives is more important, then we should put more emphasis on specificity. We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, consider buying one or two of my original songs."
81,Machine Learning Fundamentals: Sensitivity and Specificity,https://www.youtube.com/watch?v=vP06aMoz4v8,vP06aMoz4v8,695.0,704.0,9.0," All right. Until next time, quest on. Thanks for watching."
82,"The Sensitivity, Specificity, Precision, Recall Sing-a-Long!!! #Shorts",https://www.youtube.com/watch?v=PWvfrTgaPBI,PWvfrTgaPBI,0.0,33.04,33.04, sensitivity is the percentage of actual positive correctly predicted. Specificity is the percentage of actual negatives correctly predicted. Precision is something different. It's the percentage of predicted positives correctly predicted. And recolates us back to the story is the same as sensitivity.
83,"The Sensitivity, Specificity, Precision, Recall Sing-a-Long!!! #Shorts",https://www.youtube.com/watch?v=PWvfrTgaPBI,PWvfrTgaPBI,33.04,41.6,8.560000000000002, Is the percentage of actual positives correctly predicted? Bam!
84,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,0.0,42.0,42.0," Hurrah cane Florence came by while I was working on stat quest dark clouds filled the sky but that didn't stop stat quest stat quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to be talking about some machine learning fundamentals, bias and variance and they're going to be clearly explained. Imagine we measured the weight and height of a bunch of mice and plotted the data on a graph. Light mice tend to be short and heavier mice tend to be taller."
85,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,42.0,69.0,27.0," But after a certain weight, mice don't get any taller, just more obese. Given this data, we would like to predict mouse height given its weight. For example, if you told me your mouse weighed this much, then we might predict that the mouse is this tall. Ideally, we would know the exact mathematical formula that describes the relationship between weight and height."
86,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,69.0,100.0,31.0," But, in this case, we don't know the formula, so we're going to use two machine learning methods to approximate this relationship. However, I'll leave the true relationship curve in the figure for reference. The first thing we do is split the data into two sets, one for training the machine learning algorithms and one for testing them. The blue dots are the training set, and the green dots are the testing set. Here's just the training set."
87,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,100.0,136.0,36.0," The first machine learning algorithm that we will use is linear regression, aka least squares. Linear regression fits a straight line to the training set. Note, the straight line doesn't have the flexibility to accurately replicate the arc in the true relationship. No matter how we try to fit the line, it will never curve. Thus, the straight line will never capture the true relationship between weight and height, no matter how well we fit it to the training set."
88,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,136.0,173.0,37.0," The inability for a machine learning method, like linear regression, to capture the true relationship is called bias. Because the straight line can't be curved like the true relationship, it has a relatively larger amount of bias. Another machine learning method might fit a squiggly line to the training set. The squiggly line is super flexible, and hugs the training set along the arc of the true relationship. Because the squiggly line can handle the arc in the true relationship between weight and height, it has very little bias."
89,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,173.0,214.0,41.0," We can compare how well the straight line and the squiggly line fit the training set by calculating their sums of squares. In other words, we measure the distances from the fit lines to the data, square them, and add them up. They are squared so that negative distances do not cancel out positive distances. Notice how the squiggly line fits the data so well that the distances between the line and the data are all zero. In the contest to see whether the straight line fits the training set better than the squiggly line, the squiggly line wins."
90,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,214.0,246.0,32.0," But remember, so far we've only calculated the sums of squares for the training set. We also have a testing set. Now let's calculate the sums of squares for the testing set. In the contest to see whether the straight line fits the testing set better than the squiggly line, the straight line wins. Even though the squiggly line did a great job fitting the training set, it did a terrible job fitting the testing set."
91,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,246.0,281.0,35.0," In machine learning lingo, the difference in fits between data sets is called variance. The squiggly line has low bias since it is flexible and can adapt to the curve in the relationship between weight and height. But the squiggly line has high variability because it results in vastly different sums of squares for different data sets. In other words, it's hard to predict how well the squiggly line will perform with future data sets. It might do well sometimes, and other times, it might do terribly."
92,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,281.0,313.0,32.0," In contrast, the straight line has relatively high bias since it cannot capture the curve in the relationship between weight and height. But the straight line has relatively low variance because the sums of squares are very similar for different data sets. In other words, the straight line might only give good predictions and not great predictions, but they will be consistently good predictions. Bam! Oh no, terminology alert."
93,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,314.0,348.0,34.0," Because the squiggly line fits the training set really well, but not the testing set, we say that the squiggly line is over fit. In machine learning, the ideal algorithm has low bias and can accurately model the true relationship. And it has low variability by producing consistent predictions across different data sets. This is done by finding the sweet spot between a simple model and a complex model. Oh no, another terminology alert."
94,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,348.0,376.0,28.0," Three commonly used methods for finding the sweet spot between simple and complicated models are regularization, boosting, and bagging. The stat quest on a random forest show an example of bagging in action. And we'll talk about regularization and boosting in future stat quests. Double Bam! Hooray, we've made it to the end of another exciting stat quest."
95,Machine Learning Fundamentals: Bias and Variance,https://www.youtube.com/watch?v=EuBBz3bI-aA,EuBBz3bI-aA,376.0,401.0,25.0," If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, please consider buying one or two of my original songs. All right, until next time, quest on. Thanks for watching."
96,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,0.0,30.6,30.6," Wait, Tacy, the ROC, and AUC, that cool, yeah, stack-west. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about ROC and AUC and they're going to be clearly explained. Note, this stack-west builds on the confusion matrix and sensitivity and specificity stack-wests."
97,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,30.6,51.52,20.92," So if you're not already down with those, check out the quests. Also, the example I give in this stack-west is based on logistic regression. So, even though ROC and AUC apply to more than just logistic regression, make sure you understand those basics. Let's start with some data."
98,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,51.52,74.44,22.92," The Y-axis has two categories, obese and not obese. The blue dots represent obese mice. And the red dots represent mice that are not obese. Along the X-axis, we have weight. This mouse is not obese, even though it weighs a lot."
99,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,74.44,97.2,22.760000000000005," It must be mighty mouse and just full of muscles. This mouse doesn't weigh that much, but it is still considered obese for its size. Now let's fiddle logistic regression curve to the data. When we're doing logistic regression, the Y-axis is converted to the probability that a mouse is obese."
100,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,97.2,121.72,24.52," Now let's just look at the curve. If someone told us that they had a heavy mouse that weighs this much, then the curve would tell us that there is a high probability that the mouse is obese. If someone told us that they had a light mouse that weighs this much, then the curve would tell us that there is a low probability that the mouse is obese."
101,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,121.72,144.2,22.47999999999999," So this logistic regression tells us the probability that a mouse is obese based on its weight. However, if we want to classify the mice as obese or not obese, then we need a way to turn probabilities into classifications. One way to classify mice is to set a threshold at 0.5."
102,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,144.2,180.92,36.72," In classify all mice with the probability of being obese greater than 0.5 as obese. In classify all mice with the probability of being obese less than or equal to 0.5 as not obese. Using 0.5 as the cutoff, we would call this mouse obese. If another mouse weighed this much, then we would classify it as obese. And if another mouse weighed this much, then we would classify it as not obese."
103,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,180.92,209.42,28.5," To evaluate the effectiveness of this logistic regression with the classification threshold set to 0.5, we can test it with mice that we know are obese and not obese. Here are the weights of 4 new mice that we know are not obese. And here are the weights of 4 new mice that we know are obese. We know that this mouse is not obese."
104,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,209.42,232.12,22.700000000000017," And the logistic regression with the classification threshold set to 0.5 correctly classifies it as not obese. This mouse is also correctly classified. But this mouse is incorrectly classified. We know that it is obese, but it is classified as not obese."
105,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,232.12,252.92,20.799999999999983, The next mouse is correctly classified. But this mouse is incorrectly classified. The last three mice are correctly classified. Now we create a confusion matrix to summarize the classifications. These three samples were correctly classified as obese.
106,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,252.92,281.92,29.00000000000003," And this sample was predicted to be obese, but was not obese. These three samples were correctly classified as not obese. And this sample was predicted to be not obese, even though it was obese. Once the confusion matrix is filled in, we can calculate sensitivity and specificity to evaluate this logistic regression when 0.5 is the threshold for obesity."
107,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,281.92,309.92,28.0," Little BAM, because so far this is all review. Now let's talk about what happens when we use a different threshold for deciding if a sample is obese or not. For example, if it was super important to correctly classify every obese sample, we could set the threshold to 0.1. This would result in correct classifications for all four obese mice."
108,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,309.92,333.92,24.0," But it would also increase the number of false positives. The lower threshold would also reduce the number of false negatives, because all of the obese mice were correctly classified. Note, if the idea of using a threshold other than 0.5 is blowing your mind, imagine that instead of classifying samples as obese or not obese,"
109,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,333.92,361.92,28.0," we were classifying samples as infected with Ebola and not infected with Ebola. In this case, it's absolutely essential to correctly classify every sample infected with Ebola in order to minimize the risk of an outbreak. And that means lowering the threshold even if that results in more false positives. On the other hand, we could set the threshold to 0.9."
110,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,361.92,383.92,22.0," In this case, we would correctly classify the same number of obese samples as when the threshold was set to 0.5. But we wouldn't have any false positives. And we would correctly classify one more sample that was not obese. And have the same number of false negatives as before."
111,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,384.92,405.92,21.0," With this data, the higher threshold does a better job classifying samples as obese or not obese. But the threshold could be set to anything between 0 and 1. How do we determine which threshold is the best? For starters, we don't need to test every single option."
112,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,405.92,432.92,27.0," For example, these thresholds result in the exact same confusion matrix. But even if we made one confusion matrix for each threshold that mattered, it would result in a confusingly large number of confusion matrices. So, instead of being overwhelmed with confusion matrices, receiver operator characteristic, ROC graphs, provide a simple way"
113,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,432.92,457.92,25.0," to summarize all of the information. The y-axis shows the true positive rate, which is the same thing as sensitivity. The true positive rate is the true positives divided by the sum of the true positives and the false negatives. In this example, the true positives are the samples that were correctly classified as obese."
114,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,457.92,495.92,38.0," And the false negatives are the obese samples that were incorrectly classified as not obese. The true positive rate tells you what proportion of obese samples were correctly classified. The x-axis shows the false positive rate, which is the same thing as one minus specificity. The false positive rate is the false positives divided by the sum of the false positives and true negatives. The false positives are the non-ob-samples that were incorrectly classified as obese."
115,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,495.92,528.9200000000001,33.00000000000006," And the true negatives are the samples correctly classified as not obese. The false positive rate tells you the proportion of not obese samples that were incorrectly classified at our false positives. To get a better sense of how the ROC works, let's draw one from start to finish using our example data. We'll start by using a threshold that classifies all of the samples as obese. And that gives us this confusion matrix."
116,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,528.9200000000001,557.92,28.999999999999886," First, let's calculate the true positive rate. There are four true positives, and there were zero false negatives. Doing the math gives us one. The true positive rate, when the threshold is so low that every single sample is classified as obese, is one. This means that every single obese sample was correctly classified."
117,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,557.92,588.92,31.0," Now let's calculate the false positive rate. There were four false positives in the confusion matrix, and there were zero true negatives. Doing the math gives us one. The false positive rate, when the threshold is so low that every single sample is classified as obese, is also one. This means that every single sample that was not obese was incorrectly classified as obese."
118,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,588.92,630.92,42.0," Now plot a point at one comma one. Means that even though we correctly classified all of the obese samples, we incorrectly classified all of the samples that were not obese. This green diagonal line shows where the true positive rate equals the false positive rate. Any point on this line means that the proportion of correctly classified obese samples is the same as the proportion of incorrectly classified samples that are not obese. Going back to the logistic regression, let's increase the threshold so that all but the lightest sample are called obese."
119,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,630.92,675.92,45.0," The new threshold gives us this confusion matrix. We then calculate the true positive rate and the false positive rate, and plot a point at zero point seven five comma one. Since the new point is to the left of the dotted green line, we know that the proportion of correctly classified samples that were obese is greater than the proportion of samples that were incorrectly classified as obese. In other words, the new threshold for deciding if a sample as obese or not is better than the first one. Now let's increase the threshold so that all but the two lightest samples are called obese."
120,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,675.92,711.92,36.0," The new threshold gives us this confusion matrix. We then calculate the true positive rate and the false positive rate, and plot a point at zero point five comma one. The new point is even further to the left of the dotted green line, showing that the new threshold further decreases the proportion of samples that were incorrectly classified as obese. In other words, the new threshold is the best one so far. Now we increase the threshold again."
121,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,711.92,734.92,23.0, Create a confusion matrix. Calculate the true positive rate and the false positive rate and plot the point. Now we increase the threshold again. Create a confusion matrix. Calculate the true positive rate and the false positive rate and plot the point.
122,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,734.92,771.92,37.0," The threshold represented by the new point correctly classifies 75% of the obese samples and 100% of the samples that were not obese. In other words, this threshold resulted in no false positives. Now we increase the threshold again and plot the point. Now we increase the threshold again and plot the point. Finally, we choose a threshold that classifies all of the samples as not obese and plot the point."
123,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,771.92,812.92,41.0," The point at zero comma zero represents a threshold that results in zero true positives and zero false positives. If we want, we can connect the dots and that gives us an ROC graph. The ROC graph summarizes all of the confusion matrices that each threshold produced. Without having to sort through the confusion matrices, I can tell that this threshold is better than this threshold. And depending on how many false positives I'm willing to accept, the optimal threshold is either this one or this one."
124,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,812.92,834.92,22.0," Bam! Now that we know what an ROC graph is, let's talk about the area under the curve or AUC. The AUC is 0.9. Bam! The AUC makes it easy to compare one ROC curve to another."
125,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,834.92,870.92,36.0," The AUC for the red ROC curve is greater than the AUC for the blue ROC curve, suggesting that the red curve is better. So if the red ROC curve represented logistic regression and the blue ROC curve represented a random forest, you would use the logistic regression. Double bam! Now, one last thing before we're all done. Although ROC graphs are drawn using true positive rates and false positive rates to summarize confusion matrices,"
126,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,870.92,905.92,35.0," there are other metrics that attempt to do the same thing. For example, people often replace the false positive rate with precision. Precision is the true positives divided by the sum of the true positives and false positives. Precision is the proportion of positive results that were correctly classified. If there were lots of samples that were not obese relative to the number of obese samples, then precision might be more useful than the false positive rate."
127,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,905.92,936.92,31.0," This is because precision does not include the number of true negatives in its calculation and is not affected by the imbalance. In practice, this sort of imbalance occurs when studying a rare disease. In this case, the study will contain many more people without the disease than with the disease. Bam! In summary, ROC curves make it easy to identify the best threshold for making a decision."
128,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,936.92,959.92,23.0," This threshold is better than this one. And the AUC can help you decide which categorization method is better. The red method is better than the blue method. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
129,"ROC and AUC, Clearly Explained!",https://www.youtube.com/watch?v=4jRBRDbJemM,4jRBRDbJemM,959.92,972.92,13.0," And if you want to support stat quest, well, consider getting a t-shirt or a hoodie or buying one or two of my original songs. The links for doing that are below. Alright, until next time, quest on."
130,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,0.0,18.0,18.0," Let's make a graph. Let's make it look cool. Thank goodness stat quests here because stat quest rules. Stat Quest. Hello, I'm Josh Starmer and welcome to stat quest."
131,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,18.0,36.0,18.0," Today we're going to talk about drawing ROC graphs and calculating the AUC in R. If you're interested in doing this at home, there's a link to the example code in the description below. In this stat quest, we'll draw a simple ROC graph,"
132,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,36.0,60.0,24.0," extract thresholds for a specific region in the ROC, draw and compute a partial area under the curve, and end by layering two ROC graphs so that they can be easily compared. Note, this stat quest builds on the example used in the ROC and AUC stat quest, so you might want to watch the first few minutes of that one"
133,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,60.0,78.0,18.0," if you haven't already. The first thing we need to do is load in PROC, the library that will draw ROC graphs for us. If you don't have PROC installed, just use installed-out packages, PROC to install it."
134,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,78.0,105.0,27.0," We're also going to use the random forest package as part of the example. For the purposes of this stat quest, you just need to know that a random forest is a way to classify samples, and we can change the threshold that we use to make those decisions. However, if you want more details, check out the quests. If you don't have random forest installed, just use installed-out packages,"
135,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,105.0,126.0,21.0," random forest to install it. Since we are going to generate an example dataset, let's set the seed for the random number generator so that we can reproduce our results. Note, I usually set the random number seed to 42, but 420 made a nicer looking set of random data."
136,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,126.0,154.0,28.0, This example dataset will be just like the one we used in the ROC and AUC stat quest. Only this one will have 100 samples instead of just 8. So let's start by setting num.samples to 100. Now we'll create 100 measurements and store them in a variable called weight. We do this by using the R-Norm function to generate 100 random values from a normal distribution
137,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,154.0,183.0,29.0," with the mean set to 172 and the standard deviation set to 29. Psst, just in case you're interested, the internet told me that the average man weighs 172 pounds with a standard deviation of 29. And then we use the sort function to sort the numbers from low to high. This next line of code classifies an individual as obese or not obese."
138,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,184.0,217.0,33.0," The way we are going to classify as sample as obese is to start by using the rank function to rank the weights from light as to heaviest. The light as sample will have rank equals 1 and the heaviest sample will have rank equals 100. Then we scale the ranks by 100. This means that the light as sample will equal 1 divided by 100, which equals 0.01. And the heaviest sample will equal 100 divided by 100, which equals 1."
139,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,217.0,252.0,35.0," Then we compare the scaled ranks to random numbers between 0 and 1. And if the random number is smaller than the scaled rank, then the individual is classified as obese. Otherwise it is classified as not obese. The if smaller than obese otherwise not obese is performed by the if-else function and the results are stored in a variable called obese. To see what that fancy line of code just did, we can print out the contents of obese."
140,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,252.0,273.0,21.0," The zeros stand for not obese and the ones stand for obese. The lighter samples are mostly zeros, not obese. And the heavier samples are mostly ones, obese. Now let's plot the data to see what it looks like. Bam."
141,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,273.0,295.0,22.0," These samples are obese. And these samples are not obese. Now we will use the GLM function to fiddle a gestic regression curve to the data. If you want to learn more about how the GLM function works, there's a stack-west that can fill you in on the details."
142,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,295.0,338.0,43.0," Otherwise just know that we can store the results of the GLM function in a variable called GLM.fit. And pass weight and the fitted dot values stored in GLM.fit into the lines function. To draw a curve that tells us the predicted probability that an individual is obese or not obese. GLM.fit, dollar sine fitted dot values contains the y-axis coordinates along the curve for each sample. In other words, GLM.fit, dollar sine fitted dot values contains estimated probabilities that each sample is obese."
143,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,338.0,372.0,34.0," We will use the known classifications and the estimated probabilities to draw an ROC curve. We use the ROC function from the PROC library to draw the ROC graph. We pass in the known classifications, obese or not obese for each sample. And the estimated probabilities that each sample is obese. And we tell the ROC function to draw the graph, not just calculate all of the numbers used to draw the graph."
144,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,373.0,392.0,19.0," When you use the ROC function, it prints out a bunch of stuff. The first part just echoes what you typed in and isn't very interesting. The second part is a little more interesting. It tells us how many samples were not obese. Remember, not obese equals zero."
145,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,392.0,409.0,17.0, And how many samples were obese. Obies equals one. The third part is the most interesting of all. It tells us the area under the curve or the AUC. Ta-da! Here's the graph.
146,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,409.0,442.0,33.0," Here's the ROC curve. And here's the diagonal line that shows where the true positive rate is the same as the false positive rate. But if you draw this graph in our studio, then you'll also get this ugly padding on each side. To get rid of the ugly padding, we have to use the power function and muck around with the graphics parameters. In this case, we set PTY, A-K-A, the plot type to S, which is short for square."
147,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,442.0,475.0,33.0," Then we use the up arrow key to bring back the call to the ROC function. And we get a much nicer ROC graph. By default, the ROC function plots specificity on the X-axis instead of one minus specificity. As a result, the X-axis goes from one on the left side to zero on the right side. If using a backwards X-axis gives you a headache, don't freak out."
148,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,475.0,507.0,32.0," Instead, set Legacy.Axis to True. And the ROC function will use one minus specificity on the X-axis and all will be right in the world again. Now that we're on the subject of changing the axes, I have a confession to make. I have a very hard time remembering what sensitivity and specificity mean. So I set percent to True so that the axes are in percentages rather than values between zero and one."
149,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,507.0,535.0,28.0, And I label the X-axis false positive percentage in label the Y-axis true positive percentage. Bam! Now we have a nice looking X-axis and a nice looking Y-axis. We can also change the color of the ROC curve and make it thicker. You can change the color to anything you want by setting the call parameter.
150,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,535.0,569.0,34.0," I set it to RGB values that I found on the color brewer website. And you can change the line thickness to whatever you want by setting the LWD parameter. Now, imagine we're interested in the range of thresholds that resulted in this part of the ROC curve. We can access those thresholds by saving the calculations that the ROC function does in a variable. And then make a data frame that contains all of the true positive percentages by multiplying the sensitivities by 100."
151,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,569.0,604.0,35.0," And the false positive percentages by multiplying one minus specificities by 100. And last but not least, the thresholds. We can then use the head function to look at the first six rows of the new data frame. And we see that when the threshold is set to negative infinity so that every single sample is called obese, then the TPP, the true positive percentage, is 100 because all of the obese samples were correctly classified."
152,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,604.0,649.0,45.0," And the FPP, the false positive percentage, is also 100 because all of the samples that were not obese were incorrectly classified. So the first row in ROC.df corresponds to the upper right hand corner of the ROC curve. We can use the tail function to look at the last six rows of the data frame. And we see that when the threshold is set to positive infinity so that every single sample is classified not obese, then the TPP and FPP are both zero because none of the samples were classified either correctly or incorrectly obese."
153,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,649.0,693.0,44.0," So the last row in ROC.df corresponds to the bottom left hand corner of the ROC curve. Now we can isolate the TPP, the FPP, and the thresholds used when the true positive rate is between 60 and 80. If we were interested in choosing a threshold in this range, we could pick one that had the optimal balance of true positives and false positives. Now let's go back to talking about customizing what the ROC function draws. If we want to print the AUC directly on the graph, then we set the print.auc parameter to true."
154,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,693.0,729.0,36.0," You can also draw and calculate a partial area under the curve. These are useful when you want to focus on the part of the ROC curve that only allows for a small number of false positives. To print and draw the partial AUC, we start by setting the print.auc parameter to true. We then specify where along the x-axis you want the AUC to be printed, otherwise the text might overlap something important. Note, I set along 45 after trying a bunch of different locations."
155,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,729.0,766.0,37.0," Then we set partial.auc to the range of specificity values that we want to focus on. Note, the range of values is in terms of specificity, not one minus specificity. So 100% specificity corresponds to 0% on our one minus specificity axis. And 90% specificity corresponds to 10% on our one minus specificity axis. Then we draw the partial area under the curve by setting AUC.auc to true."
156,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,766.0,793.0,27.0," And we set AUC.auc.auc to specify the polygon's color. Note, I use the same RGB numbers that I used to make the line blue. However, I added two digits to the end to make the color semi-transparent. Bam! For those of you keeping track, we're up to three exclamation points on the BAM."
157,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,793.0,820.0,27.0," Lastly, let's talk about how to overlap two ROC curves so that they are easy to compare. We'll start by making a random force classifier with the same dataset. Now we draw the original ROC curve for the logistic regression. Then we add the ROC curve for the random forest. We do this with the plot.auc function."
158,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,820.0,852.0,32.0," Almost everything in the call to plot.auc is the same as in the call to the ROC function. However, since we are using a random forest for the second ROC, we pass in the number of trees in the forest that voted correctly. We also set the color to green instead of blue. Again, I got these RGB values from the color brew website. We also set add to true so that this ROC curve is added to an existing graph."
159,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,852.0,887.0,35.0," And we set print.auc.y to 40 so that the AUC for the random forest is printed below the AUC for the logistic regression. Lastly, we draw legend in the bottom right hand corner. BAM. Once we're all done drawing ROC graphs, we need to reset the PTY graphical parameter back to its default value, M, which is short for maximum. As in, use the maximum amount of space provided to draw graphs."
160,ROC and AUC in R,https://www.youtube.com/watch?v=qcvAqAH60Yw,qcvAqAH60Yw,888.0,908.0,20.0," Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, consider getting a T-shirt or a hoodie or buying one or two of my original songs. The links to do that are all below. Alright, until next time, quest on."
161,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,0.0,26.0,26.0," Yes, you can understand entropy, the RAY StatQuest. Hello, I'm Josh Starman. Welcome to StatQuest. Today we're going to talk about entropy for data science, and it's going to be clearly explained. Note, this StatQuest assumes that you are already familiar with the main ideas of expected values."
162,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,26.0,57.0,31.0," If not, check out the quest. Entropy is used for a lot of things in data science. For example, entropy can be used to build classification trees, which are used to classify things. Entropy is also the basis of something called mutual information, which quantifies the relationship between two things. Entropy is the basis of a relative entropy, aka the Colback Leibler distance, and cross entropy,"
163,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,57.0,89.0,32.0," which show up all over the place, including fancy dimension reduction algorithms like T-Sney and U-MAP. What these three things have in common is that they all use entropy, or something derived from it, to quantify similarities and differences. So let's learn how entropy quantifies similarities and differences. However, in order to talk about entropy, first we have to understand surprise. So let's talk about chickens."
164,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,89.0,128.0,39.0," Imagine we had two types of chickens, orange and blue, and instead of just letting them randomly roam all over the screen, our friend statsquatch chased them around until they were organized into three separate areas, A, B, and C. Now, if statsquatch just randomly picked up a chicken and area A, then, because there are six orange chickens and only one blue chicken, there is a higher probability that they will pick up an orange chicken. And since there is a higher probability of picking up an orange chicken, it would not be very surprising if they did."
165,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,128.0,167.0,39.0," In contrast, if statsquatch picked up the blue chicken from area A, we would be relatively surprised. Area B has a lot more blue chickens than orange, and because there is now a higher probability of picking up a blue chicken, we would not be very surprised if it happened. And because there is a relatively low probability of picking the orange chicken, that would be relatively surprising. Lastly, area C has an equal number of orange and blue chickens. Thus, regardless of what color chicken we pick up, we would be equally surprised."
166,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,167.0,198.0,31.0," Combined, these areas tell us that surprise is, in some way, inversely related to probability. In other words, when the probability of picking up a blue chicken is low, this surprise is high. And when the probability of picking up a blue chicken is high, this surprise is low. Bam, now we have a general intuition of how probability is related to surprise. Now let's talk about how to calculate surprise."
167,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,199.0,239.0,40.0," Because we know there is a type of inverse relationship between probability and surprise, it's tempting to just use the inverse of probability to calculate surprise. Because when we plot the inverse, we see that the closer the probability is to zero, the larger the y-axis value. However, there is at least one problem with just using the inverse of the probability to calculate surprise. To get a better sense of this problem, let's talk about the surprise associated with flipping a coin. Imagine we had a terrible coin, in every time we flipped it, we got heads."
168,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,239.0,262.0,23.0," Blah, blah, blah, blah. Ugg, flipping this coin is super boring. Hey statsquatch, how surprised would you be if the next flip gave us heads? I would not be surprised at all. So when the probability of getting heads is one, then we want the surprise for getting heads to be zero."
169,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,262.0,298.0,36.0," However, when we take the inverse of the probability of getting heads, we get one instead of what we want, zero. And this is one reason why we can't just use the inverse of the probability to calculate surprise. So instead of just using the inverse of the probability to calculate surprise, we use the log of the inverse of the probability. Now, since the probability of getting heads is one, and thus, we will always get heads and it will never surprise us. The surprise for heads is zero."
170,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,299.0,342.0,43.0," In contrast, since the probability for getting tails is zero, and thus we'll never get tails, it doesn't make sense to quantify the surprise of something that will never happen. So when we plug in zero for the probability, and use the properties of logs to turn the division into subtraction, the second term is the log of zero. And because the log of zero is undefined, the whole thing is undefined. And this result is okay, because we're talking about the surprise associated with something that never happens. Like the inverse of the probability, the log of the inverse of the probability gives us a nice curve."
171,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,342.0,370.0,28.0," And the closer the probability gets to zero, the more surprise we get. But now the curve says there is no surprise when the probability is one. So surprise is the log of the inverse of the probability. Damn. Note, when calculating surprise for two outputs, in this case, the two outputs are heads and tails, then it is customary to use the log base two for the calculations."
172,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,370.0,412.0,42.0," Now that we know what surprise is, let's imagine that our coin gets heads 90% of the time, and it gets tails 10% of the time. Now let's calculate the surprise for getting heads and tails. As expected, because getting tails is much rarer than getting heads, the surprise for tails is much larger. Now let's flip this coin three times, and we get heads, heads, and tails. The probability of getting two heads and one tail is 0.9 times 0.9 for the heads, times 0.1 for the tails."
173,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,412.0,454.0,42.0," And if we want to know exactly how surprising it is to get two heads and one tail, then we can plug this probability into the equation for surprise. And use the properties of logs to convert the division into subtraction, and use the properties of logs to convert the multiplication into addition, and then plug in shug. And we get 3.62. But more importantly, we see that the total surprise for a sequence of coin tosses is just the sum of the surprises for each individual toss. In other words, the surprise for getting one heads is 0.15."
174,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,454.0,486.0,32.0," And since we got two heads, we add 0.15 to times, plus 3.32 for the one tail, to get the total surprise for getting two heads and one tail. Medium bam. Now, because this diagram takes up a lot of space, let's summarize the information in a table. The first row in the table tells us the probability of getting heads or tails. And the second row tells us the associated surprise."
175,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,486.0,533.0,47.0," Now, if we wanted to estimate the total surprise after flipping the coin 100 times, we approximate how many times we will get heads by multiplying the probability we will get heads, 0.9 by 100. And we estimate the total surprise from getting heads by multiplying by 0.15. So this term represents how much surprise we expect from getting heads in 100 coin flips. Likewise, we can approximate how many times we will get tails by multiplying the probability we will get tails, 0.1 by 100. And we estimate the total surprise from getting tails by multiplying by 3.32."
176,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,533.0,561.0,28.0," So the second term represents how much surprise we expect from getting tails in 100 coin flips. Now we can add the two terms together to find out the total surprise, and we get 46.7. Hey, statsquatch is back. Okay, I see that we just estimated the surprise for 100 coin flips, but aren't we supposed to be talking about entropy? Funny you should ask."
177,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,561.0,591.0,30.0," If we divide everything by the number of coin tosses 100, then we get the average amount of surprise per coin toss 0.47. So, on average, we expect the surprise to be 0.47 every time we flip the coin. And that is the entropy of the coin. The expected surprise every time we flip the coin. Double bound."
178,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,591.0,638.0,47.0," In fancy statistics notation, we say that entropy is the expected value of the surprise. Anyway, since we are multiplying each probability by the number of coin tosses 100, and also dividing by the number of coin tosses 100, then all of the values that represent the number of coin tosses 100 cancel out. And we are left with the probability that a surprise for heads will occur times its surprise. Thus, the entropy 0.47 represents the surprise we would expect per coin toss if we flipped this coin a bunch of times."
179,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,638.0,678.0,40.0," And yes, expecting surprise sounds silly, but it's not the silliest thing I've heard. Note, we can rewrite entropy just like an expected value using fancy sigma notation. The x represents a specific value for surprise times the probability of observing that specific value for surprise. So, for the first term getting heads, the specific value for surprise is 0.15, and the probability of observing that surprise is 0.9. So, we multiply those values together."
180,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,678.0,711.0,33.0," Then the sigma tells us to add that term to the term for tails. Either way we do the math, we get 0.47. Now, personally, once I saw that entropy was just the average surprise that we could expect, entropy went from something that I had to memorize to something I could derive. Because now, we can plug the equation for surprise in for x, the specific value, and we can plug in the probability."
181,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,711.0,741.0,30.0," And we end up with the equation for entropy. Bam! Unfortunately, even though this equation is made from two relatively easy to interpret terms, the surprise times the probability of the surprise, this isn't the standard form of the equation for entropy that you'll see out in the wild. First, we have to swap the order of the two terms."
182,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,741.0,773.0,32.0," Then we use the properties of logs to convert the fraction into subtraction. And the log of 1 is 0, then we multiply both terms in the difference by the probability. Then, lastly, we pull the minus sign out of the summation, and we end up with the equation for entropy that Claude Shannon first published in 1948. Small bam! That said, even though this is the original version, and the one you'll usually see,"
183,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,773.0,802.0,29.0," I prefer this version, since it is easily derived from surprise, and it is easier to see what is going on. Now, going back to the original example, we can calculate the entropy of the chickens. So let's calculate the entropy for area A. Because 6 of the 7 chickens are orange, we plug in 6 divided by 7 for the probability. Then we add a term for the 1 blue chicken."
184,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,802.0,836.0,34.0," By plugging in 1 divided by 7 for the probability. Now we just do the math and get 0.59. Note, even though the surprise associated with picking up an orange chicken is much smaller than picking up a blue chicken, there is a much higher probability that we will pick up an orange chicken than pick up a blue chicken. Thus, the total entropy 0.59 is much closer to the surprise associated with orange chickens than blue chickens."
185,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,836.0,861.0,25.0," Likewise, we can calculate the entropy for area B. Only this time, the probability of randomly picking up an orange chicken is 1 divided by 11, and the probability of picking up a blue chicken is 10 divided by 11, and the entropy is 0.44. In this case, the surprise for picking up an orange chicken is relatively high,"
186,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,861.0,906.0,45.0," but the probability of it happening is so low that the total entropy is much closer to the surprise associated with picking up a blue chicken. We also see that the entropy value, the expected surprise, is less for area B than area A. This makes sense because area B has a higher probability of picking a chicken with a lower surprise. Lastly, the entropy for area C is 1, and that makes the entropy for area C the highest we have calculated so far. In this case, even though the surprise for orange and blue chickens is relatively moderate 1,"
187,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,907.0,943.0,36.0," we always get the same, relatively moderate, surprise every time we pick up a chicken. And it is never outweighed by a smaller value for surprise, like we saw earlier, for area A and B. As a result, we can use entropy to quantify the similarity or difference in the number of orange and blue chickens in each area. Entropy is highest when we have the same number of both types of chickens. And as we increase the difference in the number of orange and blue chickens, we lower the entropy."
188,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,943.0,967.0,24.0," Triple B. P.S. the next time you want to surprise someone, just whisper the lock of the inverse of the probability. Bam. Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StacQuest study guides at StacQuest.org."
189,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,967.0,988.0,21.0," There's something for everyone. Hooray, we've made it to the end of another exciting StacQuest. If you like this StacQuest and want to see more, please subscribe. And if you want to support StacQuest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate."
190,Entropy (for data science) Clearly Explained!!!,https://www.youtube.com/watch?v=YtebGVx-Fxw,YtebGVx-Fxw,988.0,994.0,6.0," The links are in the description below. All right, until next time, Quest ON."
191,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,0.0,21.64,21.64," MUTU will live for me, sure. It's really cool gonna check it out now. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're gonna talk about mutual information and it's gonna be clearly explained."
192,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,21.64,49.0,27.36," I don't want to spend a lot of time scaling up my stuff to work in the cloud. I would rather spend my time working on my stuff because that's the fun part, lightning. This Stack Quest is also sponsored by the letters A, B and C. A always, B, B, C, curious. Always B, curious."
193,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,49.0,77.36,28.36," Imagine we had a data set with a lot of variables, which are also called features. Because we want to simplify the amount of timely spend collecting data, we wanted to remove some of them. Thus, we want to know how much each variable can tell us about the thing we want to predict. In this case, we want to predict if someone loves the movie Troll 2. And we want to know which of these variables, likes popcorn, height, etc."
194,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,77.36,112.36,35.0," play the biggest role in making good predictions. In theory, we could use something like R squared to see if a specific variable is related to Love's Troll 2. Except R squared only works with continuous data, and Love's Troll 2 is yes or no. And so is likes popcorn. By the way, if you're not familiar with R squared, you probably should be, so check out the quest. So, when we have a mixture of continuous and discrete variables, how do we quantify their relationship to the thing we want to predict?"
195,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,112.36,141.36,29.00000000000001," Well, one way to quantify how each variable is related to Love's Troll 2 is to use mutual information. Like R squared, mutual information is a numeric value that gives us a sense of how closely related to variables are. BAM! The equation for calculating mutual information looks kind of nasty. But don't worry, we'll go through it one step at a time."
196,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,141.36,178.36,37.0," In a nutshell, these two sigmas, Greek characters that stand for summation, tell us that we're going to do a lot of addition. And we're going to be adding up joint probabilities, and dividing some of those joint probabilities by marginal probabilities. What are joint and marginal probabilities? Joint probabilities are just the probability of two things occurring at the same time. For example, given our dataset, we can calculate the probability that someone likes popcorn and loves Troll 2."
197,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,178.36,226.36,48.0," In this case, the probability that someone likes popcorn and loves Troll 2 is three divided by five, because three of the five people in the dataset like popcorn and Love Troll 2. In contrast, marginal probabilities are just the probability of one thing occurring. For example, if we just focus on likes popcorn, we can calculate the probability that someone does not like popcorn. In this case, the probability that someone does not like popcorn is two divided by five, because two of the five people in the dataset do not like popcorn. Likewise, we can calculate the marginal probability that someone does not love Troll 2."
198,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,226.36,270.36,44.0," The probability that someone does not love Troll 2 is one divided by five, because only one of the five people in the dataset does not love Troll 2. Now, for any pair of variables like likes popcorn and loves Troll 2, we can keep track of their joint and marginal probabilities in its table. In this table, the first two columns represent whether or not someone likes popcorn. In the first two rows represent whether or not someone loves Troll 2. The joint probability that someone likes popcorn and loves Troll 2, which we calculated earlier, goes in the upper left hand corner of the table."
199,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,270.36,320.36,50.0," Likewise, we can calculate the joint probability that someone does not like popcorn and loves Troll 2. And put that in the top of the second column. Now, if we want to know the marginal probability that someone loves Troll 2, we can either calculate it like we did earlier by dividing the number of people that love Troll 2 by the total number of people, or we can simply add the joint probability that someone likes popcorn and loves Troll 2 to the joint probability that someone does not like popcorn and loves Troll 2. And when we do the math, we get 4 divided by 5. So, either way, we get the same marginal probability for divided by 5."
200,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,320.36,357.36,37.0," Likewise, we can solve for the remaining joint probabilities and plug them into the table. And we can solve for the marginal probability that someone does not love Troll 2 either directly from the data, or we can add up the two joint probabilities in the second row in the table. Either way, we get the same result. Then we can calculate the marginal probability that someone likes popcorn, either directly, or by adding the values in the column together. Then we can calculate the marginal probability that someone does not like popcorn."
201,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,357.36,380.36,23.0," Now we have a table filled out with all of the joint and marginal probabilities associated with likes popcorn and loves Troll 2. Bam! Note, the marginal probabilities are all in the margins of the table. So that's where the name comes from. Small ban."
202,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,380.36,427.36,47.0," Anyway, now that we have this table of joint and marginal probabilities, for likes popcorn and loves Troll 2, we can calculate their mutual information by plugging the joint and marginal probabilities into this equation. The joint probabilities go here and here. And the marginal probabilities both go here. Note, the two summations ensure that we include all possible combinations of the variables, likes popcorn, yes and no, with loves Troll 2, yes and no. For example, we start by plugging in the joint and marginal probabilities for where both popcorn and loves Troll 2 equal yes."
203,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,427.36,468.36,41.0," Then we add a term where popcorn is yes and loves Troll 2 is no. Then we add a term where popcorn is no and loves Troll 2 is yes. And lastly, a term where popcorn is no and loves Troll 2 is no. Now that we have expanded this double summation by adding terms for all possible combinations of popcorn and loves Troll 2, we plug in the joint and marginal probabilities that we calculated earlier. For example, the joint probability that someone likes popcorn and Troll 2 is 3 divided by 5, so we plug that in."
204,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,468.36,501.36,33.0," And then we plug in the marginal probability that someone likes popcorn, 3 divided by 5. And the marginal probability that someone loves Troll 2 for divided by 5. Then we just plug in the joint and marginal probabilities for all of the other terms. Note, before we move on, I want to point out that the log function that we use in this equation can be any base. However, the default log function for most machine learning and most programming languages is the natural log."
205,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,502.36,529.36,27.0," So that's what we use here. Anyway, when we do the math, the whole thing is equal to 0.22. So the mutual information between likes popcorn and loves Troll 2 is 0.22. We can then compare this mutual information value 0.22 to the mutual information values for other variables to decide which ones are the most useful. Double bound."
206,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,530.36,565.36,35.0," Oh no, it's a super technical note. If you look carefully at this second term in the equation, you'll see that if we divide 0 by 5, then we'll get a 0 here and we'll get a 0 in this numerator. Which means the whole term is 0 times the log of 0. And, technically, the log of 0 is not defined. The good news is that even though the log of 0 is not defined, as x gets close to 0, x times the log of x equal 0."
207,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,565.36,587.36,22.0," So the second term is just equal to 0. Tiny bam. Now let's go back to the raw data. And see what happens when we change it so that likes popcorn is always yes. In other words, let's see what the mutual information is now that likes popcorn is always yes and never changes."
208,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,587.36,616.36,29.0," So just like before, let's fill out the table of the joint and marginal probabilities. And now let's use the table to calculate the mutual information. First, let's expand the summation so that we have a term for all possible combinations of likes popcorn and loves Troll 2. Then we plug numbers into each term. And when we do the math, we get 0."
209,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,616.36,648.36,32.0," In other words, when likes popcorn never changes, it can't tell us anything about what's happening in loves Troll 2. And, in general, when at least one of the two columns never changes, then the mutual information will be 0. Because something that never changes, can't tell us about something that does. Now let's go back to the raw data. And see what happens when we change it so that likes popcorn is yes when loves Troll 2 is yes."
210,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,648.36,677.36,29.0," And likes popcorn is no when loves Troll 2 is no. In other words, now both columns change, but they change in the exact same ways. So let's fill out the table of the joint and marginal probabilities. And use the table to calculate the mutual information. And when we do the math, we get something close to, but not exactly 0.5."
211,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,677.36,718.36,41.0," So, given this data set, where the two columns change and they change in the exact same ways, the mutual information is close to 0.5. And this value is larger than 0.22, which is what we got when we calculated the mutual information with the original data. And remember, both columns changed in the original data, but not in exactly the same ways. So, when both columns change and the changes in one tell us more about what's going on in the other, then the mutual information value is larger. Now let's go back to the raw data again."
212,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,718.36,751.36,33.0," And see what happens when we change it so that likes popcorn is yes when loves Troll 2 is no. And likes popcorn is no when loves Troll 2 is yes. In other words, let's see what the mutual information is when both columns change, but in the exact opposite ways. So, we fill out the table of the joint and marginal probabilities. And plug the numbers into the equation for mutual information."
213,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,751.36,780.36,29.0," And when we do the math, just like when we had both columns change in the exact same ways, we get something close to, but not exactly 0.5. In other words, when both columns change, it doesn't matter if they change in the exact same or exact opposite ways. They both give us the same mutual information. And the changes in one column can tell us exactly what is changing in the other. Double bound."
214,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,780.36,814.36,34.0," So far, we've seen how to calculate mutual information for two discrete variables, likes popcorn and loves Troll 2. How do we calculate the mutual information when we have a continuous variable like height? When we want to calculate the mutual information and one or more of the variables as continuous, then we simply create a histogram of the continuous values. And then we can use each bend in the histogram as a discrete category. And use the discrete categories to calculate the joint and marginal probabilities."
215,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,814.36,848.36,34.0," Note, because we have three bends in the histogram, we have three columns of joint probabilities in the table. And that means that when we expand the equation for mutual information, we end up with six terms because we have Troll 2 equals yes and Troll 2 equals no for each of the three bends. Then we just do the math and get 0.22. And since we got 0.22 for both likes popcorn and height, we can use either one, triple bound."
216,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,848.36,891.36,43.0," One last note. If you're familiar with how entropy is used in data science, and if not, check out the quest, then you may have noticed that the equation for mutual information has a lot in common with the equation for entropy. They are both sums of probabilities times logs. These equations are similar because mutual information can be derived from entropy. And in a sense, mutual information tells us how, on average, the surprise or change we see in one variable is related to the surprise or change in another."
217,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,891.36,927.36,36.0," For example, do you remember when we calculated the mutual information and likes popcorn never changed? Well, when we have something that never changes, then the surprise is always 0. So it's no surprise that the mutual information is also 0, because the changes in Love's Troll 2 have nothing to be related to. In contrast, when the surprise is greater than 0 for both variables and the changes are related, then the mutual information will also be greater than 0 and reflect how much the changes are related."
218,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,927.36,945.36,18.0," Bam! Now it's time for some. Shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat quest PDF study guides and my book. The stat quest illustrated guide to machine learning at statquest.org."
219,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,945.36,969.36,24.0," There's something for everyone. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate the links are in the description below."
220,"Mutual Information, Clearly Explained!!!",https://www.youtube.com/watch?v=eJIp_mgVLwE,eJIp_mgVLwE,969.36,973.36,4.0," Alright, until next time, quest on!"
221,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,0.0,21.0,21.0," We go on a quest and that quest is really awesome. It's a stat quest. Yeah, yeah, yeah. Hello, and welcome to stat quest. Stat quest is brought to you by the friendly folks in the genetics department at the University"
222,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,21.0,40.0,19.0," of North Carolina at Chapel Hill. Today we're going to talk about fitting a line to data. AKA least squares, AKA linear regression. Now let's get to it. Okay, you worked really hard, you did the experiment, and now you got some data."
223,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,40.0,58.0,18.0," Here it is, plotted on x, y, graph. We usually like to add a line to our data, so we can see what the trend is. But is this the best line we should use? Or does this new line fit the data even better? Or what about this line?"
224,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,58.0,87.0,29.0," Is it better or worse than the other options? A horizontal line that cuts through the average y value of our data is probably the worst fit of all. However, it gives us a good starting point for talking about how to find the optimal line to fit our data. So now let's focus on this horizontal line. It cuts through the average y value, which is 3.5."
225,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,87.0,122.0,35.0," Let's just call this point b, because different data sets will have different average values on the y axis. That is to say, the y value for this line is b. And for this particular data set, b equals 3.5. We can measure how well this line fits the data by seeing how close it is to the data points. We'll start with the point in the lower left-hand corner of the graph with coordinates x1, y1."
226,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,122.0,161.0,39.0," We can now draw a line from this point up to the line that cuts across the average y value for this data set. The distance between the line and the first data point equals b minus y1. The distance between the line and the second data point is b minus y2. So far, the total distance between the data points and the line is the sum of the two distances. And we can calculate the distance between the line and the third point, that equals b minus y3."
227,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,161.0,189.0,28.0," Now we've added the third distance to our total sum. The distance for the fourth point is b minus y4. Note, y4 is greater than b, because it's above the horizontal line. So this value will be negative. That's no good, since it will subtract from the total and make the overall fit up here better than it really is."
228,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,189.0,212.0,23.0," The fifth data point is even higher relative to the horizontal line. This distance is going to be very negative. Back in the day, when they were first working this out, they probably tried taking the absolute value of everything, and then discovered that it made the math pretty tricky. And it ups squaring each term."
229,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,212.0,244.0,32.0," Squaring ensures that each term is positive. Here's the equation that shows the total distance the data points have from the horizontal line. In this specific example, 24.62 is our measure of how well this line fits the data. It's called the sum of squared residuals, because the residuals are the differences between the real data and the value of the data. The real data and the line, and we are summing the square of these values."
230,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,244.0,266.0,22.0," Now let's see how good the fit is if we rotate the line a little bit. In this case, the sum of squared residuals equals 18.72. This is better than before. Does this fit improve if we rotate a little more? Yes."
231,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,266.0,288.0,22.0," The sum of squared residuals now equals 14.05. That value keeps going down the more we rotate the line. What if we rotate the line a whole lot? Well, as you can see, the fit gets worse. In this case, the sum of squared residuals is 31.71."
232,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,288.0,318.0,30.0," So there's a sweet spot in between horizontal and two vertical. To find that sweet spot, let's start with the generic line equation. This is y equals ax or a times x plus b. A is the slope of the line, and b is the y intercept of the line. That's the location on the y-axis that the line crosses when x equals zero."
233,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,318.0,347.0,29.0," We want to find the optimal values for a and b so that we minimize the sum of squared residuals. In more general math terms, the sum of squared residuals is this complicated mathematical equation. But it's actually not that complicated. This first part is the value of the line at x1. And this second part is the observed value at x1."
234,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,347.0,384.0,37.0," So really all we're doing in this part of the equation is calculating the distance between the line and the observed value. So this is no big deal. Since we want the line that will give us the smallest sum of squares, this method for finding the best values for a and b is called least squares. If we plotted the sum of squared residuals versus each rotation, we get something like this. We're on the y-axis, we have the sum of squared residuals, and on the x-axis, we've got each different rotation of the line."
235,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,384.0,414.0,30.0," We see that the sum of squared residuals goes down when we start rotating the line, but that it's possible to rotate the line too far in the sum of squared residuals starts going back up again. How do we find the optimal rotation for the line? Well, we take the derivative of this function. The derivative tells us the slope of the function at every point. The slope at the point on the far left side is pretty steep."
236,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,414.0,446.0,32.0," As we move to the right, we see that the slope isn't as steep. The slope at the best point where we have the least squares is zero. After that, the slope starts getting steep again. Let's go back to that middle point where we have the least squares value and the slope is zero. Remember, the different rotations are just different values for A, the slope, and B, the intercept."
237,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,446.0,489.0,43.0," We can use a 3D graph to show how different values for the slope and intercept result in different sums of squares. In this graph, the intercept is the z-axis, so it's going back sort of deep into your computer screen. And if we select one value for the intercept, for example, assume we set the intercept value to be 3. Then we can change values for the slope and see how an intercept of 3 plus different values for the slope would affect the sum of squared residuals. Anyways, we do that for bunches of different intercepts and slopes."
238,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,489.0,533.0,44.0," Taking the derivatives of both the slope and the intercepts tells us where the optimal values are for the best fit. Note, no one ever solves this problem by hand. This is done on a computer, so for most people, it's not essential to know how to take these derivatives. However, it's essential to understand the concepts. Big and important concept number 1. We want to minimize the square of the distance between the observed values and the line. Big and important concept number 2. We do this by taking the derivative and finding where it is equal to zero."
239,The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.),https://www.youtube.com/watch?v=PaFPbb66DxQ,PaFPbb66DxQ,533.0,561.0,28.0," The final line minimizes the sums of squares. It gives the least squares between it and the real data. In this case, the line is defined by the following equation, y equals 0.77 x plus 0.66. Hooray, we've made it to the end of another stat quest. Tune in next time for another exciting adventure in statistics land."
240,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,0.0,21.0,21.0, Say that on a boat headed toward StatQuest. Join me on this boat that's go to StatQuest. It's super cool. Hello and welcome to StatQuest. StatQuest is brought to you by the friendly folks in the genetics department at the University
241,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,21.0,35.0,14.0," of North Carolina at Chapel Hill. Today we're going to be talking about linear regression, aka General Linear models Part 1. There's a lot of parts to linear models, but it's a really cool and powerful concept."
242,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,35.0,53.0,18.0," So let's get right down to it. I promise you, I have lots and lots of slides that talk about all the nitty-gritty details behind linear regression. But first, let's talk about the main idea behind it. The first thing you do in linear regression is use least squares"
243,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,53.0,72.0,19.0," to fit a line to the data. The second thing you do is calculate r squared. Lastly, calculate a p-value for r squared. There are lots of other little things that come up along the way, but these are the three most important concepts behind linear regression."
244,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,72.0,92.0,20.0," In the StatQuest, fitting a line to data, we talked about, fitting a line to data. But let's do a quick review. I'm going to introduce some new terminology in this part of the video, so it's worth watching, even if you've already seen the earlier StatQuest."
245,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,92.0,112.0,20.0," That said, if you need more details, check that StatQuest out. For this review, we're going to be talking about a data set where we took a bunch of mice and we measured their size and we measured their weight. Our goal is to use mouse weight as a weight predict mouse size."
246,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,114.0,135.0,21.0," First, draw a line through the data. Second, measure the distance from the line to the data, square each distance and then add them up. Terminology alert, the distance from the line to the data point is called a residual. Third, rotate the line a little bit."
247,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,135.0,154.0,19.0," With the new line, measure the residuals, square them, and then sum up the squares. Now rotate the line a little bit more. Sum up the squared residuals, etc. etc. We rotate and then sum up the squared residuals."
248,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,154.0,171.0,17.0," Rotate, then sum up the squared residuals. Just keep doing that. After a bunch of rotations, you can plot the sum of squared residuals and corresponding rotation. So in this graph, we have the sum of squared residuals on the y-axis"
249,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,171.0,194.0,23.0," and the different rotations on the x-axis. Lastly, you find the rotation that has the least sum of squares. More details about how this is actually done in practice are provided in the stat quest on fitting a line to data. So, we see that this rotation is the one with the least squares."
250,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,194.0,218.0,24.0," So it will be the one to fit to the data. This is our least squares rotation, superimposed on the original data. Bam, now we know why the method for fitting a line is called least squares. Now we have fit a line to the data. This is awesome. Here's the equation for the line."
251,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,218.0,240.0,22.0," Least squares estimated two parameters. A y-axis intercept and a slope. Since the slope is not zero, it means that knowing a mouse is weight will help us make a guess about that mouse's size. How good is that guess?"
252,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,240.0,263.0,23.0," Calculating r squared is the first step in determining how good that guess will be. The stat quest r squared explained talks about, you got it r squared. Let's do a quick review. I'm also going to introduce some additional terminology, so it's worth watching this part of the video"
253,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,263.0,288.0,25.0," even if you've seen the original stat quest on r squared. First, calculate the average mouse size. Okay, I've just shifted all the data points to the y-axis to emphasize that at this point we are only interested in mouse size. Here, I've drawn a black line to show the average mouse size."
254,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,288.0,319.0,31.0," Bam, now some the squared residuals. Just like in least squares, we measure the distance from the mean to the data point and square it and then add those squares together. Terminology alert, we'll call this SS mean for some of squares around the mean. Note, the sum of squares around the mean equals the data minus the mean squared."
255,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,319.0,342.0,23.0," The variation around the mean equals the data minus the mean squared divided by n. n is the sample size. In this case, n equals 9. The shorthand notation is the variation around the mean equals the sum of squares around the mean divided by n, the sample size."
256,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,342.0,370.0,28.0, Another way to think about variance is as the average sum of squares per mouse. Now go back to the original plot and sum up the squared residuals around our least squares fit. We'll call this SS fit for the sum of squares around the least squares fit. The sum of squares around the least squares fit is the sum of the distances
257,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,370.0,395.0,25.0," between the data and the line squared. Just like with the mean, the variance around the fit is the distance between the line and the data squared divided by n, the sample size. The shorthand is the variation around the fitted line equals the sum of squares around the fitted line divided by n, the sample size."
258,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,395.0,417.0,22.0," Again, we can think of the variation around the fit as the average of the sum of squares around the fit for each mouse. In general, the variance of something equals the sum of squares divided by the number of those things. In other words, it's an average of sum of squares."
259,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,417.0,439.0,22.0," I mentioned this because it's going to come in handy in a little bit, so keep it in the back of your mind. Okay, let's step back a little bit. This is the raw variation in mouse size. And this is the variation around the least squares line. There's less variation around the line that we fit by least squares."
260,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,439.0,470.0,31.0," That is to say, the residuals are smaller. As a result, we say that some of the variation in mouse size is explained by taking mouse weight into account. In other words, heavier mice are bigger, lighter mice are smaller. Our squared tells us how much of the variation in mouse size can be explained by taking mouse weight into account. This is the formula for our squared."
261,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,470.0,502.0,32.0," It's the variation around the mean minus the variation around the fit divided by the variation around the mean. Let's look at an example. In this example, the variation around the mean equals 11.1, and the variation around the fit equals 4.4. So we plug those numbers into the equation. The result is that r squared equals 0.6, which is the same thing as saying 60%."
262,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,502.0,536.0,34.0," This means there is a 60% reduction in the variance when we take the mouse weight into account. Alternatively, we can say that mouse weight explains 60% of the variation in mouse size. We can also use the sum of squares to make the same calculation. This is because when we're talking about variation, everything's divided by n, the sample size. Since everything's scaled by n, we can pull that term out and just use the raw sum of squares."
263,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,536.0,570.0,34.0," In this case, the sum of squares around the mean equals 100, and the sum of squares around the fit equals 40. Plugging those numbers into the equation gives us the same value we had before. r squared equals 0.6, which equals 60%. 60% of the sums of squares of the mouse size can be explained by mouse weight. Here's another example. We're also going to go back to using variation in the calculation since that's more common."
264,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,570.0,609.0,39.0," In this case, knowing mouse weight means you can make a perfect prediction of mouse size. The variation around the mean is the same as it was before, 11.1, but now the variation around the fitted line equals 0, because there are no residuals. Plugging the numbers in gives us an r squared equal to 1, which equals 100%. In this case, mouse weight explains 100% of the variation in mouse size. Okay, one last example."
265,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,609.0,647.0,38.0," In this case, knowing mouse weight doesn't help us predict mouse size. If someone tells us they have a heavy mouse, well, that mouse could either be small or large with equal probability. Similarly, if someone said they had a light mouse, well, again, we wouldn't know if it was a big mouse or a small mouse, because each of those options is equally likely. Just like the other two examples, the variation around the mean is equal 11.1. However, in this case, the variation around the fit is also equal 11.1."
266,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,647.0,690.0,43.0," So we plug those numbers in and we get r squared equal 0, which equals 0%. In this case, mouse weight doesn't explain any of the variation around the mean. When calculating the sum of squares around the mean, we collapse the points onto the y-axis just to emphasize the fact that we were ignoring mouse weight. But we could just as easily draw a line, y equals the mean mouse size, and calculate the sum of squares around the mean around that. In this example, we applied r squared to a simple equation for a line."
267,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,690.0,729.0,39.0," y equals 0.1 plus 0.78 times x. This gave us an r squared of 60%, meaning 60% of the variation in mouse size could be explained by mouse weight. But the concept applies to any equation no matter how complicated. First, you measure square and sum the distance from the data to the mean, then measure square and sum the distance from the data to the complicated equation. Once you've got those two sums of squares, just plug them in and you've got r squared."
268,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,729.0,757.0,28.0," Let's look at a slightly more complicated example. Imagine we wanted to know if mouse weight and tail length did a good job predicting the length of the mouse's body. So we measure a bunch of mice. To plot this data, we need a three-dimensional graph. We want to know how well weight and tail length predict body length."
269,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,757.0,790.0,33.0," The first mouse we measured had weight equals 2.1, tail length equals 1.3, and body length equals 2.5. So that's how we plot this data on this 3D graph. Here's all the data in the graph. The larger circles are points that are closer to us and represent mice that have shorter tails. The smaller circles are points that are further from us and represent mice with longer tails."
270,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,790.0,813.0,23.0," Now we do a least squares fit. Since we have the extra term in the equation representing an extra dimension, we fit a plane instead of a line. Here's the equation for the plane. The y value represents body length. Least squares estimates three different parameters."
271,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,813.0,840.0,27.0," The first is the y intercept. That's when both tail length and mouse weight are equal to zero. The second parameter 0.7 is for the mouse weight. The last term, 0.5, is for the tail length. If we know a mouse's weight and tail length, we can use the equation to guess the body length."
272,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,840.0,879.0,39.0," For example, given the weight and tail length for this mouse, the equation predicts this body length. Just like before, we can measure the residuals, square them, and then add them up to calculate our squared. Now, if the tail length or the z axis is useless and doesn't make the sum of squares fit any smaller, then lea squares will ignore it by making that parameter equal to zero. In this case, plugging the tail length into the equation would have no effect on predicting the mouse's size."
273,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,879.0,911.0,32.0," This means equations with more parameters will never make the sum of squares around the fit, worse than equations with fewer parameters. In other words, this equation, mouse size equal zero.3 plus mouse weight plus flip of a coin, plus favored color plus astrological sign plus extra stuff will never perform worse than this equation. Mouse size equal zero.3 plus mouse weight."
274,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,911.0,947.0,36.0," This is because lea squares will cause any term that makes sum of squares around the fit worse to be multiplied by zero, and in a sense no longer exists. Now, due to random chance, there is a small probability that the small mice in the dataset might get heads more frequently than large mice. If this happened, then we'd get a smaller sum of squares fit and a better r squared. Waw, waw, here's the frowny face of sad times."
275,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,947.0,983.0,36.0," The more silly parameters we add to the equation, the more opportunities we have for random events to reduce sum of squares fit and result in a better r squared. Thus, people report and adjusted r squared value that, in essence, scales r squared by the number of parameters. r squared is awesome, but it's missing something. What if all we had were two measurements? We'd calculate the sum of squares around the mean, in this case, that would be 10,"
276,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,983.0,1024.0,41.0," then we'd calculate the sum of squares around the fit, which equals zero. The sum of squares around the fit equals zero, because you can always draw a straight line to connect any two points. What this means is when we calculate r squared by plugging the numbers in, we're going to get 100%. 100% is a great number. We've explained all the variation, but any two random points will give us the exact same thing. It doesn't actually mean anything. We need a way to determine if the r squared value is statistically significant."
277,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1024.0,1068.0,44.0," We need a p-value. Before we calculate the p-value, let's review the main concepts behind r squared one last time. The general equation for r squared is the variance around the mean minus the variance around the fit divided by the variance around the mean. In our example, this means the variation in the mouse size minus the variation after taking weight into account divided by the variation in mouse size. In other words, r squared equals the variation in mouse size explained by weight divided by the variation in mouse size without taking weight into account."
278,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1068.0,1112.0,44.0," In this particular example, r squared equals 0.6, meaning we saw a 60% reduction in variation once we took mouse weight into account. Now that we have a thorough understanding of the ideas behind r squared, let's talk about the main ideas behind calculating a p-value for it. The p-value for r squared comes from something called f. f is equal to the variation in mouse size explained by weight divided by the variation in mouse size not explained by weight. The numerators for r squared and for f are the same."
279,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1112.0,1145.0,33.0," That is to say it's the reduction in variance when we take the weight into account. The denominator is a little different. These dotted lines, the residuals, represent the variation that remains after fitting the line. This is the variation that is not explained by weight. So together, we have the variation in mouse size explained by weight divided by the variation in mouse size not explained by weight."
280,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1145.0,1170.0,25.0," Now let's look at the underlying mathematics. Just as a reminder, here's the equation for r squared. This is the general equation that will tell us if r squared is significant. The meat of these two equations are very similar and rely on the same sums of squares. Like we said before, the numerators are the same."
281,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1170.0,1199.0,29.0," In our mouse size and weight example, the numerator is the variation in mouse size explained by weight. In the sum of squares around the fit, it's just the residuals squared and summed up around the fitted line. So that's the variation that the fit does not explain. These numbers over here are the degrees of freedom. They turn the sums of squares into variances."
282,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1199.0,1226.0,27.0," I'm going to dedicate a whole stack quest to degrees of freedom, but for now, let's see if we can get an intuitive feel for what they're doing here. Let's start with these. P fit is the number of parameters in the fit line. Here's the equation for the fit line in a general format. We just have the y-intercept plus the slope times x."
283,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1226.0,1249.0,23.0," The y-intercept and the slope are two separate parameters. That means P fit equals two. P mean is the number of parameters in the mean line. In general, that equation is y equals the y-intercept. That's what gives us a horizontal line that cuts through the data."
284,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1249.0,1272.0,23.0," In this case, the y-intercept is the mean value. This equation just has one parameter. Thus, P mean equals one. Both equations have a parameter for the y-intercept. However, the fit line has one extra parameter, the slope."
285,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1272.0,1304.0,32.0," In our example, this slope is the relationship between weight and size. In this example, P fit minus P mean equals two minus one, which equals one. The fit has one extra parameter, mouse weight. Thus, the numerator is the variance explained by the extra parameter. In our example, that's the variance in mouse size explained by mouse weight."
286,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1304.0,1331.0,27.0," If we had used mouse weight and tail length to explain variation in size, then we would end up with an equation that had three parameters and P fit would equal three. Thus, P fit minus P mean would equal three minus P mean. So, we would equal three minus one, which equals two. Now the fit has two extra parameters, mouse weight and tail length."
287,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1331.0,1362.0,31.0," With the fancier equation for the fit, the numerator is the variance in mouse size explained by mouse weight and tail length. Now let's talk about the denominator for our equation for F. The denominator is the variation in mouse size not explained by the fit. That is to say, it's the sum of squares of the residuals that remain after we fit our new line to the data."
288,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1362.0,1398.0,36.0," Why divide sum of squares fit by n minus P fit instead of just n? Intuitively, the more parameters you have in your equation, the more data you need to estimate them. For example, you only need two points to estimate a line, but you need three points to estimate a plane. If the fit is good, then the variation explained by the extra parameters in the fit will be a large number, and the variation not explained by the extra parameters in the fit will be a small number."
289,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1398.0,1424.0,26.0," That makes F a really large number. Now, that question we've all been dying to know the answer to, how do we turn this number into a P value? Conceptually, generate a set of random data. Calculate the mean and the sum of squares around the mean. Calculate the fit and the sum of squares around the fit."
290,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1424.0,1448.0,24.0," Now, plug all those values into our equation for F, and that will give us a number, in this case, that number is 2. Now, plot that number in a histogram. Now, generate another set of random data. Calculate the mean and the sum of squares around the mean. Then, calculate the fit and the sum of squares around the fit."
291,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1448.0,1468.0,20.0," Plug those values into our equation for F. And in this case, we get F equals 3, so we then plug that value into our histogram. And then we repeat with yet another set of random data. In this case, we got F equals 1. That's plotted on our histogram."
292,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1468.0,1495.0,27.0," And we just keep generating more and more random data sets, calculating the sum of squares, plugging them into our equation for F, and plotting the results on our histogram. Now, imagine we did that, hundreds, if not millions of times. When we're all done with our random data sets, we return to our original data set. We then plug the numbers into our equation for F."
293,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1495.0,1523.0,28.0," In this case, we got F equals 6. The p-value is the number of more extreme values divided by all of the values. So, in this case, we have the value at F equals 6, and the value at F equals 7, divided by all the other randomizations that we created originally. If this concept is confusing to you, I have a stat quest that explains p-values."
294,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1523.0,1538.0,15.0," So check that one out. Bam! You can approximate the histogram with a line. In practice, rather than generating tons of random data sets, people use the line to calculate the p-value."
295,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1538.0,1565.0,27.0," Here's an example of one standard F distribution that people use to calculate p-values. The degrees of freedom determine the shape. The red line represents another standard F distribution that people use to calculate p-values. In this case, the sample size used to draw the red line is smaller than the sample size used to draw the blue line."
296,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1565.0,1587.0,22.0," Notice that when n minus p-fit equals 10, the distribution tapers off faster. This means that the p-value will be smaller when there are more samples relative to the number of parameters in the fit equation. Triple-bound! Hooray! We finally got our p-value."
297,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1587.0,1603.0,16.0," Now let's review the main ideas. Given some data that you think are related, linear regression quantifies the relationship in the data. This is our squared. This needs to be large."
298,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1603.0,1619.0,16.0, It also determines how reliable that relationship is. This is the p-value that we calculated with F. This needs to be small. You need both to have an interesting result. Hooray!
299,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1619.0,1632.0,13.0," We've made it to the end of another exciting stat quest. Wow, this was a long one. I hope you had a good time. If you like this and want to see more stat quests like it, want your subscribe to my channel."
300,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1632.0,1642.0,10.0," It's really easy. Just click the red button. And if you have any ideas of stat quests that you'd like me to create, just put them in the comments below. That's all there is to it."
301,"Linear Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=nk2CQITm_eo,nk2CQITm_eo,1642.0,1647.0,5.0," Alright. Toon in next time, for another really exciting stat quest."
302,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,0.0,28.0,28.0," Stat Quest, Stat Quest, Stat Quest, Stat Quest, Yeah, Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill. Today we're going to be talking about multiple regression, and it's going to be... Really explained!"
303,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,28.0,61.0,33.0," This Stat Quest builds on the one for linear regression, so if you haven't already seen that one yet, check it out. Alright, now let's get to it. People who don't understand linear regression tend to make a big deal out of the differences between simple and multiple regression. It's not a big deal, and a Stat Quest on simple linear regression already covered most of the concepts we're going to cover here. You might recall from the Stat Quest on the linear regression that simple regression is just fitting a line to data,"
304,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,61.0,85.0,24.0," or interested in the R squared and the P value to evaluate how well that line fits the data. In that same Stat Quest, I also showed you how to fit a plane to data. Well that's what multiple regression is. You fit a plane or some higher dimensional object to your data. A term like higher dimensional object sounds really fancy and complicated, but it's not."
305,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,85.0,117.0,32.0," All it means is that we're adding additional data to the model. In the previous example, all that meant was that instead of just modeling body length by mouse weight, we modeled body length using mouse weight and tail length. If we added additional factors like the amount of food eaten, or the amount of time spent running on a wheel, well those would be considered additional dimensions, but they're really just additional pieces of data that we can add to our fancy equation."
306,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,117.0,157.0,40.0," So from the Stat Quest on linear regression, you may remember the first thing we did was calculate R squared. Well the good news is calculating R squared is the exact same for both simple regression and multiple regression. There's absolutely no difference. Here's the equation for R squared and we plug in the values for the sums of squares around the fit, and then we plug in the sums of squares around the mean value for the body length. Regardless of how much additional data we add to our fancy equation, if we're using it to predict body length, then we use the sums of squares around the body length."
307,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,157.0,184.0,27.0," When caveat is for multiple regression, you adjust R squared to compensate for the additional parameters in the equation. We covered this in the Stat Quest for linear regression, so it's no big deal. Now we want to calculate a p value for our R squared. Calculating F and the p value is pretty much the same. You plug in the sums of squares around the fit, and then you plug in the sums of squares around the mean."
308,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,184.0,232.0,48.0," For simple regression, p fit equals two because we have two parameters in the equation that least squares has to estimate. And for this specific example, the multiple regression version of p fit equals three, because least squares had to estimate three different parameters. If we added additional data to the model, for example, the amount of time a mouse spends running on a wheel, then we have to change p fit to equal the number of parameters in our new equation. And for both simple regression and multiple regression, p main equals one because we only have to estimate the mean value of the body length. So far, we have compared this simple regression to the mean and this multiple regression to the mean."
309,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,232.0,269.0,37.0," But we can compare them to each other, and this is where multiple regression really starts to shine. This will tell us if it's worth the time and trouble to collect the tail length data, because we will compare a fit without it, the simple regression to a fit with it, the multiple regression. Calculating the F value is the exact same as before. Only this time, we replace the mean stuff with the simple regression stuff. So instead of plugging in the sums of squares around the mean, we plug in the sums of squares around the simple regression."
310,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,269.0,291.0,22.0," And instead of plugging in p main, we plug in p simple, which equals the number of parameters in the simple regression. That's two. And then we plug in the sums of squares for the multiple regression. And we plug in the number of parameters in our multiple regression equation. Bam!"
311,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,291.0,318.0,27.0," If the difference in r squared values between the simple and multiple regression is big, and the p value is small, then adjusting tail length to the model is worth the trouble. Hurray! We've made it to the end of another exciting stat quest. Now for this stat quest, I've made another one that shows you how to do multiple regression in r. It shows all the little details and sort of what's important and what's not important about the output that r gives you."
312,"Multiple Regression, Clearly Explained!!!",https://www.youtube.com/watch?v=zITIFTsivN8,zITIFTsivN8,318.0,324.0,6.0," So check that one out, and don't forget to subscribe. Okay, until next time, quest on."
313,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,0.0,21.0,21.0, St. Quest. St. Quest. Yeah. Hello and welcome to St. Quest. St. Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill.
314,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,21.0,42.56,21.56," Today we're doing part two of our series on general linear models. Last time we talked about how to do linear regression. This time we're going to talk about how to use those exact same techniques to do T tests and a nova. We'll do this using something called a design matrix, which is a cool concept that will"
315,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,42.56,67.0,24.44, expand upon in future stat quests on general linear models. Let's start with a super quick review of linear regression. Last time we measured mouse weight and mouse size and we wanted to learn two things from it. We wanted to learn how useful mouse weight was for predicting mouse size. Our squared told us this.
316,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,67.0,93.0,26.0," And we wanted to know if the relationship was due to chance. The p-value told us this. Now let's see if we can apply those concepts to a T test. In this specific example, we're going to be comparing gene expression between control mice and mutant mice. Mutant mice are just normal mice that have a specific gene that's been knocked out and is no longer functioning correctly."
317,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,93.0,123.0,30.0," The goal of a T test is to compare means and see if they are significantly different from each other. If the same method can calculate p-values for a linear regression and a T test, then we can easily calculate p-values for more complicated situations. So now I'm going to walk you through the steps for using the techniques from linear regression to do a T test. On the left side of the screen, I remind you how each step applies to linear regression."
318,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,123.0,152.0,29.0," On the right side of the screen, I'll show you how those steps apply to T tests. Step 1. Ignore the x-axis and find the overall main. To emphasize that we want to focus on the y-axis, I've removed the labels on the x-axis. Here are the overall means for the linear regression and the T test. The next step is to calculate the sum of squared residuals around the mean."
319,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,152.0,173.0,21.0," This is SS mean. These are the residuals. The distance from the data points to the lines. In this case, the lines are the overall means. Bam, calculating the sum of squared residuals around the mean was easy. Step 3. Fit a line to the data."
320,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,173.0,195.0,22.0," Note, this is when we start caring about the x-axis again. On the left side, we have the least squares fit to the data. However, how do we do a least squares fit to a T test? Let's start by just fitting a line to the control data. We start by finding a least squares fit to the control data."
321,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,195.0,220.0,25.0," It turns out that the mean is the least squares fit. The mean intercepts the y-axis at 2.2. This is the equation for horizontal line that intercepts the y-axis at 2.2. Thus, this is the line that we fit to the control data. Now let's fit a line to the mutant data."
322,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,220.0,244.0,24.0," The least squares fit is the mean of the mutant data. The mean intercepts the y-axis at 3.6. This is the equation for horizontal line that intercepts the y-axis at 3.6. Thus, this is the line that we fit to the mutant data. We have fit two lines to the data."
323,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,244.0,271.0,27.0," Originally, when we did the regression, we fit a single line to the data. However, there is a way to combine these two lines into a single equation. This will make the steps for computing F the exact same for the regression and the T test, which, in turn, means a computer can do it automatically. This is key because we don't want to do this by hand, ever."
324,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,271.0,301.0,30.0," This is going to look a little weird, but just bear with me. Keep in mind that the goal is to have a flexible way for a computer to solve this, and every other least squares-based problem without having to create a whole new method each time. This is the equation, which combines both lines for this point. We have one times the mean of the control data, zero times the mean of the mutant data,"
325,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,301.0,326.0,25.0," plus the residual. Yes, this is strange, especially multiplying the mutant mean by zero, but bear with me. If we multiply things out, the equation for this point would be y equals 2.2 plus the residual. And that sort of makes sense, but just bear with me. This is the equation for the next point."
326,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,326.0,351.0,25.0," The only difference is the residual, this one is smaller. This is the equation for the next point. Again, the only difference is the residual. This is the equation for the next point, and again, the only difference is the residual. This is the equation for the first point in the mutant data set."
327,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,351.0,378.0,27.0," Now we are multiplying the control mean by zero, and multiplying the mutant mean by one. These are the equations for the remaining points. Now let's focus on the ones and zeros. They function like on and off switches for the two means. A one turns the mean on, and a zero turns the mean off."
328,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,378.0,409.0,31.0," When we isolate the ones and zeros, they form a matrix called a design matrix. The design matrix can be combined with an abstract version of the equation to represent a fit to the data. Column 1 turns the control mean on or off. Column 2 turns the mutant mean on or off. In practice, the role of each column is assumed, and the equation is written out like this."
329,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,409.0,441.0,32.0," Y equals the mean of the control data plus the mean of the mutant data. Now that we have the fit for the control and mutant data down to a single equation, plus design matrix, we can move on to calculating F and the P value. So, step 4, calculate the sum of squares of the residuals around the fitted lines. With the linear regression, that means the sum of these squared residuals."
330,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,441.0,468.0,27.0," The sum of squares around the fit for the T test is the sum of these squared residuals. To review what we've done so far, we've calculated the sum of squared residuals around the mean, and then we calculated the sum of squared residuals around the fitted line. Now we can just plug these things in to our equation for F. F will lead to a P value."
331,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,468.0,495.0,27.0," For the linear regression, P mean refers to the number of parameters in the equation for the mean mouse size. That's one parameter. In the T test, P mean refers to the number of parameters in the equation for the mean of the gene expression. That's also just one parameter. For the linear regression, P fit refers to the number of parameters in the equation for the fitted line."
332,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,495.0,521.0,26.0," In this case, that's two. The parameters are the intercept and the slope. For the T test, P fit refers to the number of parameters in the line that we fit to the T test data. In this case, P fit equals two, because we had to estimate two parameters, one for the mean of the control data, and one for the mean of the mutant data."
333,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,521.0,535.0,14.0, Now we can calculate a P value for the T test. Bam! Let's review what we've done so far. Here's the original data. Gene expression for control mice and mutant mice.
334,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,535.0,562.0,27.0," The first thing we did is we calculated the sum of squares of the residuals around the overall mean. Then we calculated the sum of squares of the residuals around the fit. In order to do this with a single equation, we had to create a design matrix. Once we've calculated the sum of squares, all we have to do is plug the values into the equation for F, and then we'll get our P value."
335,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,562.0,585.0,23.0," Now let's do an ANOVA. ANOVA tests if all five categories are the same. Here we have control and mutant mice just like before, but we also have control and mutant mice on a funky diet, and we also have heterozyg of mice. The first thing we do is calculate the sum of squares around the mean."
336,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,585.0,608.0,23.0," We do this just like before. We calculate an overall mean value for all of the categories, and then square the residuals and sum them up. No big deal. The equation for the overall mean is just y equals mean expression. That equation only has a single parameter, the overall mean, so P mean equals 1."
337,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,608.0,628.0,20.0," Now we calculate the sum of squares around the fitted lines. The equation for the fitted lines has five parameters, one for each mean. Therefore, P fit equals 5. Here's what the design matrix looks like. One column per category."
338,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,628.0,658.0,30.0," Now that we've calculated the sum of squares around the mean and the sum of squares around the fit, along with P mean and P fit, we can plug those values in and calculate F. Triple B, if we can calculate F, then we've got ourselves a P value. One last important detail before we're done. The design matrices that I've shown you are not the standard design matrices used for doing T tests and a Nova."
339,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,658.0,678.0,20.0, This is what we used for the T test in this stat quest. But this is a more common design matrix for the same thing. Both design matrices will get the job done. It's just the one on the right is more commonly used. We'll talk about this one and other more elaborate designs in the next stat quest.
340,"Using Linear Models for t-tests and ANOVA, Clearly Explained!!!",https://www.youtube.com/watch?v=NF5_btOaCig,NF5_btOaCig,679.0,697.0,18.0," However, we've made it to the end of another exciting stat quest. If you'd like this and would like to see more stat quests like it, feel free to subscribe. And if you have any suggestions for future stat quests, put them in the comments below. Toon in next time for another exciting stat quest."
341,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,0.0,19.92,19.92," Stack Quest is getting bigger, watch out! Hello and welcome to Stack Quest. Stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill. Today we're going to be talking about general linear models and this is part 3 of a series"
342,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,19.92,39.36,19.44," that we're doing on this. This time we're going to focus on design matrices. Stack Quest picks up exactly where part 2 leaves off, so if you haven't seen that one yet, I'd recommend doing it right now. In part 2 of this series, we ended by saying that this was not the standard design matrix"
343,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,39.36,59.24,19.880000000000003, for a T-test. It was kind of a cliffhanger. And then I showed you that this is the standard design matrix for a T-test. It corresponds to a slightly different equation. Let's focus on what this new design matrix and equation are all about.
344,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,59.24,83.19999999999999,23.959999999999987," In this version, all measurements, both control and mutant turn on the term for the mean control value. But only the mutant measurements turn on the term for the difference between the mean of the mutant data and the mean of the control data. This term serves as an offset that we can use for the mutant data."
345,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,83.19999999999999,108.08,24.88000000000001," For example, this one turns the term for the mean of the control data on for this data point. And this zero turns the term for the difference between the mean of the mutant data and the mean of the control data off for this data point. This one turns the term for the mean of the control data on for this data point."
346,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,108.08,130.84,22.760000000000005," And this one turns the term for the difference between the mean of the mutant data and the control data on for this data point. The residuals are the same for both equations and design matrices. The equations also have the same number of parameters, too. So P-fit is the same."
347,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,130.88,151.84,20.960000000000008," So in our equation for F, we plug in the exact same value for the sum of squares around the fit. We also plug in the exact same value for the P-fit parameter. This result in the exact same value for F. And that means we're going to get the exact same P value."
348,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,151.84,173.28,21.44," If they both do the same thing, and the result is the same P value, why is the one on the right more common? I'll be honest, I don't know the answer for sure, but I think it has something to do with regression. So far, we've looked at design matrices in the context of using ones and zeros to turn"
349,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,173.28,195.56,22.28, parts of the equation on or off. So let's take a step back and remember how it works. Remember that the numbers in the first column are multiplied by the term for the mean of the control values. And the numbers in the second column are multiplied by the term that represents the difference
350,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,195.56,225.88,30.319999999999997," between the mean of the mutant values and the mean of the control values. Multiplying the mean of the control data by one turns it on by just letting it be. Multiplying the difference between the mean of the mutant data and the mean of the control data by zero makes it zero, and that turns it off. A design matrix full of ones and zeros is perfect for doing t-tests or anovas."
351,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,225.88,251.4,25.52000000000001," Anytime we have different categories of data, but we can use other numbers. For example, here's a design matrix for linear regression. It pairs with this equation. We've got a bunch of ones in the first column. But in the second column, we've got the x-axis position for each point."
352,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,251.4,279.64000000000004,28.24000000000004," Let's focus on the first row in the design matrix for now. It corresponds to this point. Just like before, the numbers in the first column multiply the first term in the formula. In this case, multiplying the y-intercept by one turns it on. And just like before, the numbers in the second column multiply the second term in the formula."
353,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,279.64000000000004,301.28000000000003,21.639999999999983," In this case, we're scaling the term for the slope. To make this more concrete, let's see what happens when we use real numbers for the y-intercept and slope. The y-intercept is super small and equals 0.01. So that's the number we plug in here."
354,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,301.28,320.4,19.120000000000005, The slope equals 0.8. And we plug that in right here. And now we do the math. And you get a point on the least squares-fit line that corresponds with the first data point. Now let's focus on the second row.
355,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,320.4,342.32,21.920000000000016, It corresponds to this point. The number in the first column multiplies the y-intercept. In the number in the second column scales the slope. Plug in the y-intercept and the slope and do the math. And you get a point on the line that corresponds to the second data point.
356,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,342.32,373.48,31.160000000000025," Plugging each row into the equation gives us a bunch of points on the least squares-fit line. Once we have all the points on the line, we can calculate the residuals. And that means we can calculate a p-value. This example shows that a design matrix isn't always just a bunch of zeros and ones, but can be any set of numbers that we want to plug into an equation one row at a time."
357,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,373.48,396.44000000000005,22.96000000000004," One note before we move on. Since this style of design matrix with ones all the way down the first column is more common. All of the examples from here on out will be consistent with this format. Now that we know we can put any number into the design matrix, let's do something fancy."
358,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,396.44000000000005,416.0,19.559999999999945," Let's combine a t-test and a regression. Holy smokes, that's totally crazy. Okay, we're back to the relationship between mouse weight and mouse size. However, we have two types of mice. These measurements are from normal control mice."
359,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,416.0,441.76,25.75999999999999," These measurements are from mutant mice that make them tall and skinny. By eye, we can see that mutant mice tend to be larger, even if they weigh the same. In other words, the mutant mice seem to follow this trend. And the control mice seem to follow this trend. Can we use statistics to test if there's a significant difference between the two types"
360,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,441.8,469.04,27.239999999999952," of mice? If we just did a regression, we'd get a nice looking line. But it wouldn't tell us if the mutant mice were significantly larger than the normal mice. On the other hand, a normal t-test would ignore the relationship between weight and size. And in this case, the p-value is greater than 0.05."
361,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,469.08,499.0,29.91999999999996," The humans mouse type and the relationship between weight and size are both important. We need to combine them into a single test. In other words, instead of comparing this mean to this mean, which is what a t-test would do, we want to compare this line to this line. To do this, we need an equation that has a term for the y-intercept for the normal mice."
362,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,499.0,529.8,30.799999999999955," We also need a term for the mutant mouse offset. And lastly, a term for the slope, which, in this case, is the same for both types of mice. This means we need to design matrix where the first column is ones. This means that both lines intercept the y-axis at some point. The second column indicates whether the mutant offset is on or off."
363,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,529.8,557.76,27.96000000000004," The mutant offset is off for the control mice. And on for the mutant mice, this allows the mutants to have their own y-intercept. And the last column has the weight data. The first four values are the X-coordinates for the control mice. And the last four values are the X-coordinates for the mutant mice."
364,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,557.76,579.6,21.840000000000032," Let's focus on the first row. Plug in the numbers and we get a value on the red line. Now let's focus on the second row. Again, we get a value on the red line. And from here, we just plug in the values and we get coordinates on either the red or the green"
365,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,579.6,601.0,21.399999999999977," line. We get coordinates on the red line when the mutant offset is off. And we get coordinates on the green line when the mutant offset is on. Once we have the locations on the lines, we can calculate the residuals, which are hard to see since they are so small in this example."
366,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,601.0,627.24,26.24000000000001," Now we can compare the fancy model to a simpler model. In this simpler model, we model mouse size by using just the average size of the mouse. We ignore mouse weight and we ignore mouse type. This is the default model that we use when we do the T-test. Now we plug in the sum of squares of the residuals for the fancy model."
367,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,627.24,648.48,21.24000000000001, And we plug in three for the P fancy term since there are three parameters in our fancy equation. And we plug in the sum of squares of the residuals for the simple model. And we plug in one for the P simple term since there is only one parameter in the simple equation.
368,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,648.48,676.16,27.67999999999995, This gives us 21.88 and that gives us a P value of 0.003. Bam. The small P value says that taking weight and mouse type into account is significantly better at predicting size than just using the average size. Note the simple model can be any simpler model.
369,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,676.16,707.08,30.920000000000076," If we did a super simple linear regression, we'd have a model that takes weight into account but ignores the fact that some mice are normal and others are mutants. Then we plug in the sum of squares of the residuals just like before. The simple regression equation has two parameters, so P simple equals two. We then plug in the numbers and we get a P value equal to 0.0023."
370,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,707.08,723.24,16.159999999999968, Double bam. This small P value suggests that using both weight and mouse type is better at predicting mouse size than weight alone. Here's another simple model. It's just a normal T test.
371,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,723.24,746.56,23.319999999999936," This model ignores mouse weight. Again, plug in the sum of squares of the residuals and the equation has two parameters, so P simple equals two. And that gives us a P value equal to 0.0025. Oh my gosh, it's the coveted triple bam."
372,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,746.56,771.16,24.600000000000023," This small P value suggests that using mouse weight and type is better at predicting mouse size than mouse type alone. So you can see that the question you want to ask determines what type of simple model you want to use to compare your fancy model to. Okay, one last example of a design matrix."
373,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,771.16,791.0400000000001,19.88000000000011," Lab A does an experiment. One lab B replicates it. However, their measurements tended to be smaller overall. This is a batch effect. We would like to combine these two data sets to see if mutants are different from controls,"
374,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,791.0400000000001,810.64,19.59999999999991," but we need to compensate for the batch effect. Here's how to do it. First, add a term for the mean control value from lab A. Second, add a term for the lab B offset. This takes care of the batch effect."
375,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,810.64,838.16,27.519999999999985," Third, add a term for the differences between the mutant and the control measurements. Here's the design matrix. Essentially, we want to know if this last term in the equation is important or not. Alternatively, is this last column important? So we compare the fit of this fancy equation to this simpler one that ignores the control"
376,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,838.16,863.4,25.24000000000001," mutant difference. A small p-value will tell us that the equation that keeps track of the mutant control differences predicts the gene expression better than one that does not. This will mean that the difference between controls and mutants is significant. Hooray, we've made it to the end of another exciting stat quest."
377,"Design Matrices For Linear Models, Clearly Explained!!!",https://www.youtube.com/watch?v=CqLGvwi-5Pc,CqLGvwi-5Pc,863.4,879.0400000000001,15.6400000000001," If you like this stat quest and would like to see more, please subscribe. And if you have any other ideas of things you'd like me to do stat quest on, or you've got a certain design matrix you'd like to see an example of, just let me know in the comments below. Until then, quest on."
378,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,0.0,20.0,20.0," This stat quest is odd. So I'm going to talk about the odds and the log of the odds. Stat Quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to be talking about odds and log odds,"
379,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,20.0,42.0,22.0," and they're going to be clearly explained. The odds are in favor that you're already familiar with odds. For example, you might say that the odds and favor of my team winning the game are 1 to 4. Visually, we have 5 games total, 1 of which my team will win,"
380,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,42.0,61.0,19.0," and 4 of which my team will lose. So the odds are 1 to 4. Alternatively, we can write this as a fraction. Visually, we have 1 game my team wins, divided by the 4 games that my team loses."
381,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,61.0,82.0,21.0," If we do the math, we will see that the odds are 0.25 that my team will win the game. Here's another example. You might say that the odds and favor of my team winning the game are 5 to 3. Visually, we have 8 games total,"
382,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,82.0,98.0,16.0," 5 of which my team will win, and 3 of which my team will lose. So the odds are 5 to 3. Alternatively, we can write this as a fraction, 5 divided by 3."
383,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,98.0,119.0,21.0," Visually, we have the 5 games my team wins, divided by the 3 games my team loses. If we do the math, we see that the odds are 1.7 that my team will win the game. Note, odds are not probabilities."
384,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,119.0,138.0,19.0," The odds are the ratio of something happening, i.e. my team winning, to something not happening, i.e. my team not winning. Probability is the ratio of something happening, i.e. my team winning,"
385,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,138.0,158.0,20.0," to everything that could happen, i.e. my team winning and losing. In the previous example, the odds and favor of my team winning the game are 5 to 3. However, the probability of my team winning is the number of games they win,"
386,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,158.0,178.0,20.0," 5 divided by the total number of games they play, 8. Here's the math. For the odds, we have the ratio of 5 to 3, and for the probability, we have the ratio of 5 to 8. Thus, we see that the odds and favor of my team winning"
387,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,178.0,205.0,27.0," are different from the probability of my team winning. Now that we know that odds are different from probabilities, let's talk about how odds can be calculated from probabilities. In the last example, we saw that the odds of winning are 1.7. In the probability of winning is 0.625."
388,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,205.0,228.0,23.0," We can also calculate the probability of losing. The probability of losing is 0.375. Note, we could also calculate the probability of losing as 1 minus the probability of winning. That equals 1 minus 5 divided by 8."
389,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,228.0,249.0,21.0," And that gives us 8 divided by 8 minus 5 divided by 8. And ultimately, we get 3 divided by 8. This is equal to 0.375. So either way, we get the same probability. Now let's take the ratio of the probability of winning"
390,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,249.0,268.0,19.0," to the probability of losing. Alternatively, we can put 1 minus the probability of winning into the denominator. Either way, we get the same ratio. 5 divided by 8 divided by 3 divided by 8."
391,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,268.0,288.0,20.0," The 8's cancel out, since they scale the numerator and the denominator by the exact same amount. Thus, the ratio of the probability ends up being the same thing as the ratio of the raw counts. And so either way, we get the same odds."
392,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,288.0,306.0,18.0," 1.7. I mentioned this because about 50% of the time, you see odds calculated from counts. In the other 50% of the time, you will see odds calculated from probabilities. Either way, you get the same results."
393,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,306.0,326.0,20.0," Note, out there in the wild world of statistics, you will often see this formula simplified to this, where p is the probability of winning. Bam! Now that we know what the odds are,"
394,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,326.0,346.0,20.0," let's talk about the log of the odds. Let's go back to the original example. In this example, we calculated the odds of winning as 1 to 4 or 0.25. If my team was worse, the odds of winning could be 1 to 8,"
395,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,346.0,369.0,23.0," or 0.125. And if my team was terrible, the odds of winning could be 1 to 16, or 0.063. And, lastly, if my team was the worst, the odds of winning could be 1 to 32, or 0.031."
396,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,369.0,387.0,18.0," We can see that the worst my team is, the odds of winning get closer and closer to 0. In other words, if the odds are against my team winning, then they will be between 0 and 1. Now, if my team was good,"
397,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,387.0,404.0,17.0," then the odds might be 4 to 3, or 1.3, in favor of my team winning. And if my team was better, the odds might be 8 to 3, or 2.7 in favor of winning. And if my team was really good,"
398,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,404.0,427.0,23.0," the odds might be 32 to 3, or 10.7 in favor of winning. We can see that the better my team is, the odds of winning start at 1 and just go up and up. In other words, if the odds are 4 or my team winning, then they will be between 1 and infinity."
399,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,427.0,454.0,27.0," Another way to look at this is with a number line. The odds of my team losing go from 0 to 1. And the odds of my team winning go from 1 to infinity and beyond. The asymmetry makes it difficult to compare the odds 4 or against my team winning. For example, if the odds are against 1 to 6,"
400,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,454.0,482.0,28.0," then the odds are 1 divided by 6, which equals 0.17. But if the odds are in favor 6 to 1, then the odds are 6 divided by 1, which equals 6. The magnitude of these odds looks way smaller than these odds. Taking the log of the odds solves this problem by making everything symmetrical."
401,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,482.0,507.0,25.0," For example, if the odds are against 1 to 6, then the log of the odds are the log of 1 divided by 6, which equals the log of 0.17, which equals negative 1.79. And if the odds are in favor 6 to 1, then the log of the odds are the log of 6 divided by 1,"
402,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,507.0,530.0,23.0," which equals the log of 6, which equals 1.79. Using the log function, the distance from the origin, is the same for 1 to 6 or 6 to 1 odds. Double bound. Okay. Now that we know the main idea about the log of the odds,"
403,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,530.0,553.0,23.0," let's get into some details. Earlier, we saw that odds can be calculated from counts. And we saw that the same odds could be calculated from probabilities. And that means we can calculate the log of the odds with counts or probabilities, either way, will get the same value."
404,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,553.0,573.0,20.0," Note, the log of the ratio of the probabilities is called the legit function, and it forms the basis for logistic regression. I mentioned it because if you do logistic regression, you'll see it a whole lot. It's no big deal. Okay. I get it."
405,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,573.0,595.0,22.0," odds are just the ratio of something happening to something not happening. And the log of the odds is just the log of the odds. What's the big deal? To show you what the big deal is all about, if I pick pairs of random numbers that add up to 100, for example,"
406,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,595.0,621.0,26.0," and use them to calculate the log of the odds and draw a histogram, the histogram is in the shape of a normal distribution. This makes the log of the odds useful for solving certain statistics problems. Specifically, ones where we are trying to determine probabilities about when or lose or yes or no or true or false type situations."
407,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,621.0,642.0,21.0," Triple BAM. In summary, the odds are just the ratio of something happening. I eat my team winning to something not happening. I eat my team not winning. And the log of the odds is just the log of the odds."
408,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,642.0,658.0,16.0," It's no big deal. The log of the odds makes things symmetrical, easier to interpret, and easier for fancy statistics. One last thing before we go. Even though the odds is a ratio,"
409,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,658.0,678.0,20.0," it's different from an odds ratio. But don't panic. We'll talk about the odds ratio in the next stat quest. Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
410,"Odds and Log(Odds), Clearly Explained!!!",https://www.youtube.com/watch?v=ARfXDSkQf1Y,ARfXDSkQf1Y,678.0,688.0,10.0," And if you'd like to support stat quest, well, like the video, and consider buying one or two of my original songs. All right. Until next time, quest on."
411,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,0.0,20.0,20.0," Stack quest is on my mind. All night long. Stack quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about odds ratios and log odds ratios,"
412,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,20.0,45.0,25.0," and they're going to be clearly explained. The Stack Quest on odds and the log of the odds ended in a cliffhanger. We talked about how the odds are just the ratio of something happening, I.E. My team winning, to something not happening, I.E. My team not winning. We illustrated this with circles."
413,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,45.0,75.0,30.0," Blue circles represented my team winning, red circles represented my team losing. And the odds of my team winning were just the blue circles over the red circles. Alternatively, we could just use numbers to represent the odds. And when we do the math, we see the odds of winning are 0.5. The cliffhanger came when I said that even though the odds are ratio,"
414,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,75.0,102.0,27.0," it's not what people mean when they say odds ratio. So let's clear this up once and for all. When people say odds ratio, they're talking about a ratio of odds. So we've got a ratio of these odds to these odds. Doing the math gives us 2 divided by 4 over 3 divided by 1."
415,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,102.0,119.0,17.0," And that gives us 0.17. Just like when we calculate the odds of something, if the denominator is larger than the numerator, the odds ratio will go from 0 to 1. And if the numerator is larger than the denominator,"
416,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,119.0,145.0,26.0," then the odds ratio will go from 1 to infinity and beyond. And just like the odds, taking the log of the odds ratio, makes things nice and symmetrical. For example, if the odds ratio is 2 divided by 4 over 3 divided by 1, then the log of the odds ratio equals negative 1.79."
417,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,145.0,167.0,22.0," And if the odds ratio is the opposite, it's 3 to 1 over 2 to 4, then the log of the odds ratio is the positive version, 8 equals 1.79. Great. Now that we've got that cleared up, what can we do with odds ratios? Here's an example of the odds ratio in action."
418,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,167.0,195.0,28.0," We've got a bunch of people, 356 to be exact. 29 of these people have cancer, and 327 do not. We also know that 140 of these people have the mutated gene. I'm just going to let you imagine which gene I'm talking about here. And 216 people do not have the mutated gene."
419,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,195.0,221.0,26.0," We can use an odds ratio to determine if there's a relationship between the mutated gene and cancer. If someone has the mutated gene, or the odds higher that they will get cancer, given that a person has the mutated gene, the odds that they have cancer are 23 to 117. So we'll put that on top of the odds ratio."
420,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,221.0,239.0,18.0," And given that a person does not have the mutated gene, the odds that they have cancer are 6 to 210. So we'll put that on the bottom of the odds ratio. Here's our odds ratio. We do the math,"
421,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,240.0,260.0,20.0, and the odds ratio tells us that the odds are 6.88 times greater that someone with the mutated gene will also have cancer. And the log of the odds ratio is 1.93. Small ban. What does all this mean?
422,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,260.0,285.0,25.0," The odds ratio and the log of the odds ratio are like our squared. The indicate a relationship between two things. In this case, a relationship between the mutated gene and cancer. And just like our squared, the values correspond to effect size. Larger values mean the mutated gene is a good predictor of cancer."
423,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,285.0,305.0,20.0," Smaller values mean that the mutated gene is not a good predictor of cancer. Bam! However, just like our squared, we need to know if this relationship is statistically significant. So let's do it."
424,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,305.0,334.0,29.0," There are three ways to determine if an odds ratio or log of an odds ratio is statistically significant. 1. Fischer's exact test. 2. A chi-squared test. And 3. The walled test. One super annoying thing is that there is no general consensus on which method is best, and people often mix and match."
425,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,334.0,353.0,19.0," Some people who use fischer's exact test, or the chi-squared test to calculate a p-value, and use the walled test to calculate a confidence interval. And some people are happy to let wall do all the work, calculate the p-value, and the confidence interval."
426,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,353.0,379.0,26.0," The last method ensures that the p-value and confidence interval will always be consistent, but check and see what other folks do in your field to find out what is most acceptable. So let's start with fissures exact test. Step 1. Watch the stack west on enrichment analysis using fissures exact test and the hyper geometric distribution. Bam."
427,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,379.0,414.0,35.0," Now think of the people as a bag of tasty m&ms. People with cancer are represented by 23 plus 6 equals 29 red m&ms. People with out cancer are represented by 117 plus 210 equals 327 blue m&ms. Now, just like in that stack west, we work out the p-value for grabbing a handful of 23 red m&ms and 117 blue m&ms."
428,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,414.0,438.0,24.0," And just like in that stack west, we use a computer, and it says the p-value equals 0.00001. Another small bam. Now let's talk about how to calculate the p-value using a chi-squared test. The chi-squared test compares the observed values to expected values"
429,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,438.0,463.0,25.0," that assume there is no relationship between the mutated gene and cancer. To do this, we calculate the probability of having cancer as the total number of people with cancer. That's 29. Divided by the total number of people. That's 356. So the probability of having cancer is 0.08."
430,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,463.0,508.0,45.0," So if the gene is not associated with the 23 plus 117 equals 140 people with the mutated gene, then the probability of having cancer times the number of people with the mutated gene equals 11.2. Thus, the expected number of people with the mutated gene and cancer is 11.2. And the remaining 128.8 people with the mutated gene are expected not to have cancer. Likewise, if the gene is not associated with the 6 plus 210 equals 216 people without the mutated gene,"
431,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,508.0,557.0,49.0," then the probability of having cancer times the number of people without the mutated gene equals 17.3. Thus, the expected number of people with cancer that do not have the mutated gene is 17.3. And the remaining 198.7 people with the mutated gene are expected not to have cancer. Now we do a chi-square test to compare the observed and expected values. In the p-value is 0.001 with the continuity correction and 0.000404 without the continuity correction."
432,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,557.0,588.0,31.0," If you're not familiar with the chi-square test, don't panic. We'll do a stack quest on it. One last small bound. Lastly, let's talk about the walled test. This test is commonly used to determine the significance of odds ratios in logistic regression and to calculate confidence intervals. The walled test takes advantage of the fact that the log of the odds ratios, just like the log of the odds, are normally distributed."
433,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,588.0,631.0,43.0," This is a histogram of 10,000 randomly generated log of the odds ratios that tells us what to expect if there is no relationship between the mutated gene and cancer. If you want to draw this histogram at home, randomly pick a total number of people between 300 and 400. We do this to simulate the fact that if we repeated this experiment, we might not get the exact same sample size each time. Then, for each sample, select a random number between 0 and 1. If the number was less than 0.08, the proportion of people with cancer, then the sample has cancer."
434,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,631.0,661.0,30.0," Then, pick another random number between 0 and 1. If the number was less than 0.39, the proportion of people with the mutated gene, then it has the mutated gene. This gives you a matrix of random values that did not depend on a relationship between the mutated gene and cancer. Lastly, calculate the log of the odds ratio. Do this 10,000 times and draw a histogram."
435,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,661.0,690.0,29.0," A normal curve fits pretty well. Notice that the histogram and curve are centered on 0. When there is no difference in the odds, the log of the odds ratio equals 0. The standard deviation of the 10,000 log of the odds ratios is 0.43. However, it is more common to estimate the standard deviation from the observed values."
436,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,690.0,724.0,34.0," You do this by taking the square root of the sum of 1 over each of the observed values. If we do the math, we get 0.47. The two different standard deviations that we calculated are very similar. All that the wall test does is look to see how many standard deviations the observed log of the odds ratio is from 0. And since the wall test typically uses the estimated standard deviation,"
437,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,724.0,752.0,28.0," we will replace the histogram with a normal curve centered on 0 that has a standard deviation of 0.47. The log of the odds ratio is the same one that we already calculated. Here's where the log of the odds ratio goes on the curve. To find out how many standard deviations the log of the odds ratio is away from 0, we simply divide by the standard deviation."
438,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,753.0,780.0,27.0," Thus, we divide 1.93 by 0.47. And that gives us 4.11. So our log of the odds ratio is 4.11 standard deviations away from the mean of the distribution. A general rule of thumb with normal distributions is that anything further than two standard deviations from the mean will have a p value less than 0.05."
439,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,780.0,812.0,32.0," So we know our log of the odds ratio is statistically significant. However, to get a precise two-sided p value, we can add up the areas under the curve for points greater than 1.93 and for points less than negative 1.93. However, this is traditionally done using a standard normal curve. IE, a normal curve with mean equal 0 and standard deviation equal to 1."
440,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,812.0,843.0,31.0," And that means adding up the areas under the curve for points that are greater than 4.11 and for points that are less than negative 4.11. Where 4.11 is the number of standard deviations that the log of the odds ratio is away from the mean. Ultimately, the p value that the mutated gene does not have a relationship with cancer is 0.005. Double bound."
441,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,843.0,877.0,34.0," Before we go, here are some final thoughts about the three different statistical tests we can use with the log of the odds ratio. When I generated the 10,000 random log of the odds ratios, I performed all three tests on them. If the test worked as expected, 5% should have p values less than 0.5. Here's what I got. For fishers' exact test, 4% of the p values were less than 0.05."
442,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,877.0,917.0,40.0," For the chi-square test, with the continuity correction, 3% of the p values were less than 0.05. Without the continuity correction, 5% of the p values were less than 0.05. For the wall test, 5% of the p values were less than 0.05. So, all of the tests did a good job limiting significant p values, so just find out what method is most commonly used in your field. Personally, I'd be more comfortable with a borderline p value if I knew it passed all of the tests."
443,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,917.0,975.0,58.0," In summary, an odds ratio is just a ratio of odds, and a log of the odds ratio is just wait for it, the log of an odds ratio. The odds ratio, and the log of the odds ratio, tells us if there is a strong or weak relationship between two things, like whether or not having a mutated gene increases the odds of having cancer. And, depending on the field you work in, people use fishers' exact test, chi-squareed, or the walled test to determine p values for the significance of that relationship. Hooray! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe. And if you want to support stat quest, click the like button below and consider buying one or two of my original songs."
444,"Odds Ratios and Log(Odds Ratios), Clearly Explained!!!",https://www.youtube.com/watch?v=8nm0G-1uJzA,8nm0G-1uJzA,975.0,978.0,3.0," All right, until next time, quest on!"
445,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,0.0,16.6,16.6," If you can fit a line, you can fit a squiggle. If you can make me laugh, you can make me giggle. Statquist. Hello, I'm Josh Starmer and welcome to Statquist. Today we're going to talk about logistic regression."
446,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,16.6,29.8,13.2," This is a technique that can be used for traditional statistics, as well as machine learning. So let's get right to it. Before we dive into logistic regression, let's take a step back and review linear regression."
447,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,30.8,53.8,23.0," In another Statquist, we talked about linear regression. We had some data, weight and size. Then we fit a line to it. And with that line, we could do a lot of things. First, we could calculate our squared and determine if weight and size are correlated."
448,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,53.8,83.8,30.0," Large values imply a large effect. And second, calculate a p-value to determine if the r-squared value is statistically significant. And third, we could use the line to predict size given weight. If a new mouse has this weight, then this is the size that we predict from the weight. Although we didn't mention it at the time, using data to predict something,"
449,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,83.8,110.8,27.0," falls under the category of machine learning. So plain old linear regression is a form of machine learning. We also talked a little bit about multiple regression. Now we are trying to predict size using weight and blood volume. Alternatively, we could say that we are trying to model size using weight and blood volume."
450,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,110.8,139.8,29.00000000000001, Multiple regression did the same things that normal regression did. We calculated r-squared and we calculated the p-value. And we could predict size using weight and blood volume. And this makes multiple regression a slightly fancier machine learning method. We also talked about how we can use discrete measurements like genotype to predict size.
451,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,139.8,163.8,24.0," If you are not familiar with the term genotype, don't freak out. It's no big deal. Just know that it refers to different types of mice. Lastly, we could compare models. So on the left side, we've got normal regression using weight to predict size. And we can compare those predictions to the ones we get from multiple regression,"
452,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,163.8,194.8,31.0," where we're using weight and blood volume to predict size. Comparing the simple model to the complicated one tells us if we need to measure weight and blood volume to accurately predict size, or if we can get away with just weight. Now that we remember all the cool things we can do with linear regression, let's talk about logistic regression. Logistic regression is similar to linear regression except"
453,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,194.8,223.8,29.0," logistic regression predicts whether something is true or false, instead of predicting something continuous like size. These mice are obese and these mice are not. Also, instead of fitting a line to the data, logistic regression fits an S shaped logistic function. The curve goes from zero to one."
454,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,223.8,261.8,38.0," And that means that the curve tells you the probability that a mouse is obese based on its weight. If we weighed a very heavy mouse, there is a high probability that the new mouse is obese. If we weighed an intermediate mouse, then there is only a 50% chance that the mouse is obese. Lastly, there is only a small probability that a light mouse is obese. Although logistic regression tells the probability that a mouse is obese or not, it's usually used for classification."
455,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,261.8,285.8,24.0," For example, if the probability a mouse is obese is greater than 50%, then we'll classify it as obese. Otherwise, we'll classify it as not obese. Just like with linear regression, we can make simple models. In this case, we can have obesity predicted by weight. Or more complicated models."
456,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,285.8,323.8,38.0," In this case, obesity is predicted by weight and genotype. In this case, obesity is predicted by weight and genotype and age. And lastly, obesity is predicted by weight, genotype age and astrological sign. In other words, just like linear regression, logistic regression can work with continuous data, like weight and age, and discrete data, like genotype and astrological sign. We can also test to see if each variable is useful for predicting obesity."
457,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,323.8,352.8,29.0," However, unlike normal regression, we can't easily compare the complicated model to the simple model, and we'll talk more about why in a bit. Instead, we just test to see if a variable is effect on a prediction is significantly different from zero. If not, it means that the variable is not helping to prediction. We use walled test to figure this out."
458,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,352.8,382.8,30.0," We'll talk about that in another stat quest. In this case, the astrological sign is totes useless. That's statistical jargon for not helping. That means we can save time and space in our study by leaving it out. Logistic regressions ability to provide probabilities in classifying new samples using continuous and discrete measurements makes it a popular machine learning method."
459,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,382.8,420.8,38.0," One big difference between linear regression and logistic regression is how the line is fit to the data. With linear regression, we fit the line using least squares. In other words, we find the line that minimizes the sum of the squares of these residuals. We also use the residuals to calculate r squared and to compare simple models to complicated models. Logistic regression doesn't have the same concept of a residual, so it can't use least squares and it can't calculate r squared."
460,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,420.8,455.8,35.0," Instead, it uses something called maximum likelihood. There's a whole stat quest on maximum likelihood, so see that for details, but in a nutshell, you pick a probability scaled by weight of observing in a beast mouse, just like this curve. And you use that to calculate the likelihood of observing a non-obesim mouse that weighs this much. And then you calculate the likelihood of observing this mouse. And you do that for all of the mice."
461,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,455.8,478.8,23.0," And lastly, you multiply all of those likelihoods together. That's the likelihood of the data given this line. Then you ship the line and calculate a new likelihood of the data. And then ship the line and calculate the likelihood again. And again."
462,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,478.8,505.8,27.0," Finally, the curve with the maximum value for the likelihood is selected. Bam. In summary, logistic regression can be used to classify samples, and it can use different types of data, like size and originate type, to do that classification. And it can also be used to assess what variables are useful for classifying samples."
463,StatQuest: Logistic Regression,https://www.youtube.com/watch?v=yIYKR4sgzI8,yIYKR4sgzI8,505.8,526.8,20.999999999999943," I.e. astrological sign is Toats Useless. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you have suggestions for future stat quests, well, put them in the comments below. Until next time, quest on."
464,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,0.0,21.0,21.0," If I were in Hawaii, I'd be sitting on the beach, in the shade of the tree. Watch and stack quest. Hello, I'm Josh Starman, welcome to Stack Quest. Today we're going to cover logistic regression,"
465,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,21.0,41.0,20.0," and we're going to dive deep into the details. This is part one of a series of videos I'm going to do on logistic regression. This time we're talking about coefficients. This Stack Quest follows up on logistic regression clearly explained, which provides the big picture of what logistic regression is and how it works."
466,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,41.0,63.0,22.0," In this video, I want to dive deeper into how logistic regression works. Specifically, we'll talk about the coefficients that are the result of any logistic regression. We'll talk about how they are determined and interpreted. We'll talk about the coefficients in the context of using a continuous variable, like weight, to predict obesity."
467,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,63.0,89.0,26.0," And we'll talk about the coefficients in the context of testing if a discrete variable, like whether or not a mutated gene is related to obesity. Before we dive into the details, let's do a quick review of some of logistic regression's main ideas to make sure we're all on the same page. In this example, the y-axis is the probability of mouse is obese."
468,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,89.0,121.0,32.0," It goes from zero, the mouse is not obese, to one, the mouse is obese. The dotted line is fit to the data to predict the probability of mouse's obese given its weight. If we wait a heavyweight mouse, then the corresponding point on the line indicates that there is a high probability close to one that it is obese. And if we wait a middleweight mouse, then there's an intermediate probability,"
469,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,121.0,147.0,26.0," close to zero point five that it is obese. Lastly, a lightweight mouse has a low probability, close to zero, of being obese. Okay, those are all the basics we need for this stat quest. One last thing before we get started. I want to mention that logistic regression is a specific type of generalized linear model,"
470,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,147.0,176.0,29.0," often abbreviated GLM. Generalized linear models are a generalization of the concepts and abilities of regular linear models, which we've already talked about in many stat quests. That means that if you're familiar with linear models, then you're well on your way to understanding logistic regression. We'll start by talking about logistic regression when we use a continuous variable like weight to predict obesity."
471,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,176.0,205.0,29.0," This type of logistic regression is closely related to linear regression, a type of linear model. So let's do a super quick review of linear regression. Shameless self-promotion. If you're not already familiar with linear regression, check out the stat quest on linear regression and linear models part one. Okay, we start with some data, and we fit a line to it."
472,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,205.0,251.0,46.0," And this is the equation for the line. It has a Y-axis intercept and a slope, and we plug in values for weight to get predicted value. So, even though we didn't measure a mouse that weighed 1.5, we can plug 1.5 into the equation, and predict that mouse to have size 1.91. And a mouse with weight 1.5 and size 1.91 would end up on the line at this point. Now, even though it's silly, the equation can predict the size of mice with weight equal zero."
473,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,251.0,281.0,30.0," That's just the Y-axis intercept, 0.86. And we can even predict the size of mice with negative weights. I'm pointing this out because the fact that we are not limiting the equation to a specific domain and range makes it easier to solve. And this has a big effect on how logistic regression is done. So, over on the left side, we have the linear regression."
474,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,281.0,319.0,38.0," And on the right side, we have the logistic regression. With linear regression, the values on the Y-axis can, in theory, be any number. Unfortunately, with logistic regression, the Y-axis is confined to probability values between zero and one. To solve this problem, the Y-axis in logistic regression is transformed from the probability of obesity to the log odds of obesity. So, just like the Y-axis in linear regression, it can go from negative infinity to positive infinity."
475,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,320.0,359.0,39.0," So, we can see what we're doing, let's move the logistic regression to the left side. Now, let's transform the Y-axis from a probability of obesity scale to a log odds of obesity scale. We do that with the legit function that we talked about in the odds slash log odds stat quest. P, in this case, is the probability of a mouse being obese and corresponds to a value on the old Y-axis between zero and one. The midpoint on the old Y-axis corresponds to P equals zero point five."
476,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,359.0,407.0,48.0," And when we plug P equals zero point five into the legit formula and do the math, we get zero, the center of the new Y-axis. Here is P equals zero point seven three one on the old Y-axis. If we plug P equals zero point seven three one into the legit function and do the math, we get one on the new Y-axis. If we plug P equals zero point eight eight into the legit function and do the math, we get two on the new Y-axis. If we plug P equals zero point nine five into the legit function and do the math, we get three on the new Y-axis."
477,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,407.0,451.0,44.0," Lastly, these blue points from the original data are at P equals one. If we plug P equals one into the legit function and do the math, well, technically you can't divide by zero. However, the log of one divided by zero equals the log of one minus the log of zero and the log of zero is defined as negative infinity. And since something minus negative infinity equals positive infinity, this whole thing is equal to positive infinity. This means the original samples that were labeled obese are at positive infinity on the new Y-axis."
478,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,451.0,502.0,51.0," As a result, the original Y-axis from zero point five to one is stretched out from zero to positive infinity on the new Y-axis. Similarly zero point five to zero on the old Y-axis is stretched out from zero to negative infinity on the new Y-axis. Ultimately, we end up with the log of the odds of obesity on the new Y-axis. And the new Y-axis transforms the squiggly line into a straight line. The important thing to know is that even though the graph with the squiggly line is what we associate with logistic regression, the coefficients are presented in terms of the log odds graph."
479,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,503.0,537.0,34.0," In the stack quest, logistic regression details part two, fitting a line with maximum likelihood, will talk more about how this line is fit to the data. But for now, just take my word for it that this is the best fitting line. Just like with linear regression, the best fitting line has a Y-axis intercept and a slope. The coefficients for the line are what you get when you do logistic regression. The first coefficient is the Y-axis intercept when weight equals zero."
480,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,537.0,570.0,33.0," It means that when weight equals zero, the log of the odds of obesity are negative 3.476. In other words, if you don't weigh anything, the odds are against you being obese. Here's the standard error for the estimated intercept. And the Z value is the estimated intercept divided by the standard error. In other words, it's the number of standard deviations the estimated intercept is away from zero on the standard normal curve."
481,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,570.0,598.0,28.0," That means that this is the walled test that we talked about in the odds ratio stack quest. Since the estimate is less than two standard deviations away from zero, we know it is not statistically significant. And this is confirmed by the large p-value. The area under the standard normal curve that is further than 1.47 standard deviations away from zero. The second coefficient is the slope."
482,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,599.0,630.0,31.0," It means that for every one unit of weight gained, the log of the odds of obesity increases by 1.825. Here's the standard error for the slope. Again, the Z value is the number of standard deviations the estimate is from zero on the standard normal curve. And again, the estimate is less than two standard deviations from zero, so it is not statistically significant. This is no surprise with such a small sample size."
483,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,630.0,658.0,28.0, And this is confirmed with the large p-value. Double-band. Now we know all about the logistic regression coefficients when we use a continuous variable like weight to predict obesity. Now let's talk about logistic regression coefficients in the context of testing if a discrete variable like whether a mouse has a mutated gene is related to obesity.
484,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,658.0,684.0,26.0," On the left side, we have mice that have a normal gene. And on the right side, we have mice with a mutated gene. Just like before, some of the mice are obese. And some of the mice are not obese. This type of logistic regression is very similar to how a t-test is done using linear models."
485,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,684.0,707.0,23.0," So let's do a quick review of how a t-test is done using linear models. Shainless self-promotion. If you're not already familiar with how a t-test can be performed using a linear model, then check out the stat quest. Linear models part two, t-tests and a nova. Okay, we start with some data."
486,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,707.0,733.0,26.0," In this case, we've measured the size of mice that have a normal gene. And the size of mice with a mutated version of that gene. Then we fit two lines to the data. The first line represents the mean size for the mice with the normal copy of the gene. The second line represents the mean size of the mice with the mutated copy of the gene."
487,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,733.0,768.0,35.0, These two lines come together to form the coefficients in this equation. The mean value for size for the mice with the normal copy of the gene goes here. And the difference between the mean size of the mice with the mutated gene and the mean size of the mice with the normal gene goes here. We then pair this equation with the design matrix to predict the size of a mouse given that it has the normal or mutated version of the gene. This is the design matrix for the observed data.
488,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,768.0,812.0,44.0," The first column corresponds to values for B1 and it turns on the first coefficient, the mean of the normal mice. The second column corresponds to values for B2 and turns the second coefficient, the mean of the mutant mice, minus the mean of the normal mice, off or on, depending on whether it is a zero or a one. For example, the first row in the design matrix corresponds to a mouse with a normal copy of the gene. We predicted size by replacing B1 with 1 and replacing B2 with zero. And then we just do the math."
489,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,812.0,845.0,33.0, And we see that this mouse is associated with the mean of the normal mice. This row in the design matrix corresponds to a mouse with the mutated gene. We plug in one for B1 and plug in one for B2. And we see that the mean of the mice with the normal gene plus the difference between the two means. Associates this mouse with the mean for the mice with the mutated gene.
490,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,845.0,880.0,35.0," When we do a T test this way, we are basically testing to see if this coefficient, the mean of the mutant mice, minus the mean of the normal mice, is equal to zero. But you already know all this T test stuff. What you really want to know is how it applies to logistic regression. The first thing we do is transform the y-axis from the probability of being obese to the log of the odds of obesity. Now we fit two lines to the data."
491,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,880.0,915.0,35.0," For the first line, we take the normal gene data and use it to calculate the log of the odds of obesity for mice with the normal gene. Thus, the first line represents the log of the odds of obesity for the mice with the normal gene. Let's call this the log of the odds for gene normal. We then calculate the log of the odds of obesity for the mice with the mutated gene. Thus, the second line represents the log of the odds of obesity for a mouse with the mutant gene."
492,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,915.0,946.0,31.0," Let's call this the log of the odds gene mutated. These two lines come together to form the coefficients in this equation. The log of the odds gene normal goes here. In the difference between the log of the odds gene mutated and the log of the odds gene normal goes here. And since subtracting one log from another can be converted into division,"
493,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,946.0,982.0,36.0," this term is a log of the odds ratio. It tells us on a log scale how much having the mutated gene increases or decreases the odds of a mouse being obese. Okay, now that we know what the equation is all about, let's substitute in the numbers. The log of the odds for gene normal is just the log of two divided by nine. And the log of the odds for gene mutated is just the log of seven divided by three."
494,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,982.0,1016.0,34.0, Now we just do the math and that gives us these coefficients. And those are what you get when you do logistic regression. The intercept is the log of the odds for gene normal and the gene mutant term is the log of the odds ratio that tells you on a log scale. On a log scale how much having the mutated gene increases or decreases the odds of being obese. And here are the standard errors for those estimated coefficients.
495,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,1016.0,1065.0,49.0," And here are the Z values, aka the wall's test values, that tell you how many standard deviations the estimated coefficients are away from zero on a standard normal curve. The Z value for the intercept, negative 1.9 tells us that the estimated value for the intercept, negative 1.5, is less than two standard deviations from zero. And thus, not significantly different from zero. And this is confirmed by a p-value greater than 0.05. The Z value for gene mutant, the log odds ratio that describes how having the mutated gene increases the odds of being obese is greater than two, suggesting it is statistically significant."
496,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,1065.0,1099.0,34.0," And this is confirmed by a p-value less than 0.05. Double bound. Now we have seen how some of the linear model concepts for regression apply to logistic regression. And we have seen how some of the linear model concepts for T tests apply to logistic regression. In short, in terms of the coefficients logistic regression is the exact same as good old linear models except the coefficients are in terms of the log odds."
497,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,1100.0,1129.0,29.0," This means that all those fancy things we can do with linear models like multiple regression and a nova can be done using logistic regression. All we have to remember is that the scale for the coefficients is log odds. Triple bound. Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe."
498,Logistic Regression Details Pt1: Coefficients,https://www.youtube.com/watch?v=vN5cNN2-HWE,vN5cNN2-HWE,1129.0,1140.0,11.0," And if you want to support stat quest, well, click the like button below and consider buying one or two of my original songs. Alright, until next time, quest on."
499,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,0.0,28.92,28.92," maximum, likely hood, in your neighborhood. I think that you should learn about it. Stequest. Hello, I'm Josh Starman. Welcome to Stequest. Today we're going to follow up on our series of videos on logistic regression. This time we're going to talk about fitting a line using maximum likelihood. That is to say, we're going to talk about how this squiggle is optimized"
500,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,28.92,65.12,36.2," to fit the data the best. In the first video in this series, logistic regression details part 1, coefficients, we saw that logistic regression is very similar to regular old linear models. Like linear regression, t-tests, and fancy stuff like multiple regression and a nova. The big difference is that logistic regression uses the log odds on the y-axis. However, you may recall that in that stat quest, I said, just take my word for it that this"
501,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,65.12,103.12,38.0," is the best fitting line. Well, the time for blindly trusting me is over. Let's see how this line is fit to the data. However, before we talk about how lines are fit in logistic regression, let's do a super quick review of how lines are fit in linear regression. We start with some data. And we fit along to it using least squares. In other words, we measure the residuals, the distances between the data and the line, then square them so that negative values do not cancel"
502,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,103.12,139.12,36.0," out positive values and then add them all up. Then we rotate the line a little bit and do the same thing. Measure the residuals, square them, and add them up. And the line with the smallest sum of squared residuals, the least squares, is the line chosen to fit best. Okay, now that we remember how to fit a line in linear regression, let's talk about logistic regression. In this example, we are using logistic regression to determine the effect of weight on a"
503,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,139.12,180.12,41.0," density. These mice are obese, and these mice are not obese. Our goal is to draw the best fitting squiggle for this data. As we know, in logistic regression, we transform the y-axis from the probability of obesity to the log odds of obesity. We can draw a candidate best fitting line on the graph. The only problem is that the transformation pushes the raw data to positive and negative infinity. And this means that the residuals, the distance from the"
504,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,181.12,223.12,42.0," data points to the line, are also equal to positive and negative infinity. And this means we can't use these squares to find the best fitting line. Instead, we use maximum likelihood. The first thing we do is project the original data points onto the candidate line. This gives each sample a candidate log odds value. In other words, the log odds of this point is 2.1. And the log odds of this point is 1.4. Then we transform the candidate log odds to candidate probabilities"
505,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,224.12,266.12,42.0," using this fancy looking formula, which is just a reordering of the transformation from probability to log odds. For those at home keeping score, here's how to convert the equation that takes the probability as input and outputs log odds into an equation that takes log odds as input and outputs probability. First, we exponentiate both sides. Then we multiply both sides by 1 minus p and then multiply 1 minus p and e to the log odds. Then we add p times e to the log odds"
506,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,266.12,320.12,54.0," to both sides. Then we pull p out on the left side of the equation. Lastly, we divide both sides by 1 plus e to the log odds. Bam! Now let's see this fancy equation in action. For example, for this point, we substitute negative 2.1 for the log odds. And that gives us p equals 0.1. And that gives us a y coordinate on the squiggle. And we do the same thing for all of the points. Now we use the observed status, obese or not obese, to calculate their likelihood given the shape of the squiggly line. We'll start by calculating the likelihood of the obese"
507,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,321.12,363.12,42.0," mice given the shape of the squiggle. The likelihood that this mouse's obese given the shape of the squiggle is the value on the y-axis where the point intersects the squiggle 0.49. In other words, the likelihood that this mouse's obese given the shape of the squiggle is the same as the predicted probability. In this case, the probability is not calculated as the area under a curve, but instead is the y-axis value. And that's why it's the same as the likelihood. The likelihood that this mouse's obese is 0.9."
508,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,363.12,427.12,64.0," The likelihood that these mice are obese are 0.91, 0.91, and 0.92. The likelihood for all of the obese mice is just the product of the individual likelihoods. Now we'll figure out the likelihoods for the mice that are not obese. Note, the lower the probability of being obese, the higher the probability of not being obese. Thus, for these mice, the likelihood equals 1 minus the probability the mouse's obese. The probability that this mouse's obese is 0.9, so the probability and likelihood that it is not obese is 1 minus 0.9. The probability that this mouse's obese is 0.3, so the probability and likelihood that it is not obese is 1 minus 0.3."
509,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,428.12,483.12,55.0," The probabilities that these mice are obese are both 0.01, so the probability and the likelihood that they are not obese is 1 minus 0.01. Now we can include the individual likelihoods for the mice that are not obese to the equation for the overall likelihood. Note, although it is possible to calculate the likelihood as the product of the individual likelihoods, statisticians prefer to calculate the log of the likelihood instead. Either way works, because the squiggle that maximizes the likelihood is the same one that maximizes the log of the likelihood. With the log of the likelihood or log likelihood to those in the know, we add the logs of the individual likelihoods instead of multiplying the individual likelihoods."
510,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,484.12,521.12,37.0," Thus, the log likelihood of the data given the squiggle is negative 3.77. And this means that the log likelihood of the original line is negative 3.77. Now we rotate the line. In calculate its log likelihood by projecting the data onto it, transforming the log odds to probabilities, and then calculating the log likelihood. And the final value for the log likelihood is negative 4.15."
511,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,522.12,559.12,37.0," So this one is not as good as the first line. And we just keep rotating the log odds line and projecting the data onto it. And then transforming it to probabilities and calculating the log likelihood. Note, the algorithm that finds the line with the maximum likelihood is pretty smart. Each time it rotates the line, it does so in a way that increases the log likelihood. Thus, the algorithm can find the optimal fit after a few rotations."
512,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,560.12,593.12,33.0," Ultimately, we get a line that maximizes the likelihood and that's the one chosen to have the best fit. Bam, that's all there is to it. But just like with linear regression, there's more to logistic regression than just fitting a line. We want to know if that line represents a useful model, and that means we need an r squared value and a p value. However, in logistic regression, we have to do that without the usual residuals."
513,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,594.12,617.12,23.0," So we'll learn about r squared and p values for logistic regression in the next step quest. Cool. Hurray, we've made it to the end of another exciting step quest. If you like this step quest and want to see more of them, please subscribe. And if you want to support step quest, well, click the like button below and consider buying one or two of my original songs."
514,Logistic Regression Details Pt 2: Maximum Likelihood,https://www.youtube.com/watch?v=BfKanl1aSG0,BfKanl1aSG0,618.12,621.12,3.0," All right, until next time, quest on."
515,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,0.0,21.0,21.0," Oskwe, P values, effects on significance, let's talk about it. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. In this video, we're going to continue our series on Logistic Regression, and we're going to talk about R-squared and P values."
516,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,21.0,46.0,25.0," In the Stack Quest, Logistic Regression Details Part 2, fitting a line with maximum likelihood, we had weight measurements for obese mice, and weight measurements for some mice that were not obese. And we converted the y-axis from probability to the log odds of obesity, and then fit a line to that data using maximum likelihood."
517,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,46.0,70.0,24.0," Well, technically, we maximize the log likelihood, but either way you do it, you get the same best fitting line. However, we ended with a bit of a cliffhanger. We know that the line is best fit, but how do we know if it is useful? In other words, how do we calculate R-squared and a P value for the relationship between weight"
518,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,70.0,89.0,19.0," and obesity? The answer is, it's complicated. Although the idea behind generalized linear models, a common framework for solving all kinds of problems, is nice. The result is a bit of a hot mass."
519,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,89.0,114.0,25.0," Even though pretty much everyone agrees on how to calculate R-squared and the associated P value for linear models, there is no consensus on how to calculate R-squared for logistic regression. There are more than 10 different ways to do it. So, before you settle on a way to calculate R-squared for logistic regression, look and see what other people are already doing in your field."
520,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,114.0,145.0,31.0," That will give you a good starting point. For this stat quest, rather than describe every single R-squared for logistic regression, I'm focusing on one that is commonly used and is easily calculated from the output that R gives you. Just so you know, this R-squared is called McFadden's pseudo R-squared. Another bonus is that this method is very similar to how R-squared is calculated for regular old linear models."
521,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,145.0,173.0,28.0," So let's do a super quick review of R-squared for regular old linear models using size and weight measurements as an example so that the concepts are fresh in your mind. Wow, that was a long sentence. In linear regression and other linear models, R-squared and the related p-value are calculated using the residuals. In brief, we square the residuals and then add them up."
522,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,173.0,209.0,36.0," I call this SS-fit for sum of squares of the residuals around the best fitting line. And we compare that to the sum of squared residuals around the worst fitting line, the mean of the y-axis values. This is called SS-mean. R-squared compares a measure of a good fit SS-fit to a measure of a bad fit SS-mean. R-squared is the percentage of variation around the mean that goes away when you fit a line to the data."
523,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,209.0,234.0,25.0," Also, because I want to refer to this later, I'm going to point out another thing you already know. R-squared goes from 0 to 1. If there wasn't a relationship between weight and size, the data might look like this. And the best fitting line might look like this. In this case, SS-fit equals SS-mean."
524,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,234.0,266.0,32.0," And when we plug in the values for SS-fit and SS-mean, we get 0 in the numerator, and then R-squared equals 0. On the other hand, when the line fits the data perfectly, SS-fit equals 0. And that means when we plug in the values for SS-fit and SS-mean, we get SS-mean minus 0 in the numerator. And then R-squared equals 1."
525,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,266.0,301.0,35.0," I told you this was something you already knew. Now let's talk about R-squared in terms of logistic regression. Like linear regression, we need to find a measure of a good fit to compare to a measure of a bad fit. Unfortunately, the residuals for logistic regression are all infinite, so we can't use them. But we can project the data onto the best fitting line, and then we translate the log odds back to probabilities."
526,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,301.0,336.0,35.0," And lastly, calculate the log likelihood of the data given the best fitting squiggle. In this case, that gives us negative 3.77. We can call this L-L-fit for the log likelihood of the fitted line, and use it as a substitute for SS-fit. Now we need a measure of a poorly fitted line that is analogous to SS-mean. We do this by calculating the log odds of obesity without taking weight into account."
527,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,336.0,373.0,37.0," The overall log odds of obesity is just the total number of obese mice. Divided by the total number of mice that are not obese, then we just take the log of the whole thing and do the math. In this case, we get a horizontal line at 0.22. Then project the data onto this line, and then we translate the log odds back to probabilities. That gives us a horizontal line at p equals 0.56."
528,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,373.0,412.0,39.0," Note, the overall log odds 0.22 translates to the overall probability of being obese 0.56. In other words, we can arrive at the same solution by calculating the overall probability of obesity. Hooray, they are the same, so we have two different ways to calculate the exact same number. Now calculate the log likelihood of the data given the overall probability of obesity. This gives us negative 6.18."
529,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,412.0,450.0,38.0," We'll call this L-L overall probability and use it as a substitute for SS-mean. So we have L-L overall probability, a measure of a bad fit, and L-L-fit, hopefully a measure of a good fit. And it makes intuitive sense that we could combine them just like we combined SS-mean and SS-fit to calculate our squared. Plugging in the numbers gives us an R squared value equals 0.39. Bam!"
530,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,450.0,500.0,50.0," However, before we get too excited, let's verify that calculating R squared with these log likelihoods were result in a value between 0 when the fit is bad and 1 when the fit is good. Let's start by looking at the R squared we'll get when weight is not a good predictor of obesity. There are light mice that are obese and light mice that are not obese, and there are heavy mice that are obese and heavy mice that are not obese. Intuitively, we can see that with this data weight makes a poor predictor of obesity. The maximum likelihood best fit in, lying for this data, has an intercept of negative 0.22 and a slope of practically 0."
531,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,500.0,530.0,30.0, 5.10 to the negative 17 to be exact on the log odds scale. And this translates to a horizontal line at 0.45 on the probability scale. L-L-fit is the log likelihood of the data projected onto the best fitting line. And that value is negative 6.18. Now let's calculate L-L overall probability.
532,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,530.0,561.0,31.0," The first step is simply to calculate the overall probability of obesity. That gives us a line at 0.44. L-L overall probability is the log likelihood of the data projected onto the overall probability. In this case, L-L overall probability equals negative 6.18. Now let's just plug in the values for L-L overall probability and L-L-fit."
533,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,561.0,594.0,33.0," When we do the math, we get R squared equals 0. Bam! Now let's look at the R squared world get when weight is an awesome predictor of obesity. The maximum likelihood best fitting line for this data has an intercept of negative 63.72 and a slope of 22.42. And this translates to a squiggly line on the probability scale."
534,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,594.0,622.0,28.0," L-L-fit is the log likelihood of the data projected onto the best fitting line. In this case, L-L-fit equals 0. That's because the log of 1 equals 0, so L-L-fit is just a sum of a bunch of 0s. Now let's calculate the overall probability of obesity. And that gives us a line at 0.44."
535,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,622.0,656.0,34.0," L-L overall probability is the log likelihood of the data projected onto the overall probability. In this case, that value is negative 6.18. At this point, you may have noticed something. When the model is a poor fit, the log likelihood for logistic regression is a relatively large negative value. And when the model is a good fit, the log likelihood for logistic regression is a value close to 0."
536,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,656.0,689.0,33.0," Log likelihood for logistic regression will always be between 0 and negative infinity because we are taking logs of values between 0 and 1. And good fits result in log likelihoods close to 0, and bad fits result in larger negative log likelihoods. Okay, back to calculating r squared. Let's plug in the values for L-L overall probability and L-L-fit. This gives us an r squared equals 1."
537,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,689.0,719.0,30.0," Double bound. So we see that, at least on an intuitive level, the r squared calculated with log likelihoods behaves like the r squared calculated from sums of squares. The log likelihood r squared values go from 0 for poor models to 1 for good models. Now we need a p-value. The good news is that calculating the p-value is pretty straightforward."
538,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,719.0,763.0,44.0," Two times the difference between L-L-fit and L-L overall probability equals a chi squared value with degrees freedom equal to the difference in the number of parameters in the two models. L-L-fit has two parameters since it needs estimates for a y-axis intercept and a slope. L-L overall probability has one parameter since it only needs an estimate for a y-axis intercept. So, in this case, the degrees of freedom equals 1. Here's a graph of a chi squared distribution with 1 degree of freedom."
539,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,763.0,807.0,44.0," In the worst case scenario, L-L-fit equals L-L overall probability and the whole thing equals 0. In this case, the p-value equals 1 since the area under the curve from 0 to infinity equals 1. However, most of the time, L-L-fit will be closer to 0 than L-L overall probability. And since the log likelihoods are negative, this will be a positive value on the x-axis and the p-value will get smaller. In the example we've worked out in this stat quest, we end up with 4.82 for the chi squared distribution."
540,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,807.0,842.0,35.0," And the p-value equals 0.03. Thus, the relationship between weight and obesity is not due to chance, and the r-squared value, 0.39, tells us the effect size of this relationship. Triple bound. Before we leave, there are a few things I need to point out. When you see these formulas for r-squared and the associated p-value out in the wild, they will look more like this."
541,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,842.0,881.0,39.0," This is because the formulas usually include terms for the saturated model. I'll talk more about the saturated model in another stat quest. For now, however, just know that when doing logistic regression, the log likelihood of the saturated model equals 0, so we can omit it. And when we omit the term for the saturated model, we get the simple equations I've presented in this stat quest. However, the log likelihood of the saturated model isn't always 0 when it is used with other generalized linear models."
542,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,881.0,917.0,36.0," So people included when talking about the log likelihood r-squared and associated p-value, so that it will work in other situations. But don't worry, we'll talk about these topics in a stat quest on the saturated model and deviant statistics. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, please click the like button below and consider buying one or two of my original songs."
543,Logistic Regression Details Pt 3: R-squared and p-value,https://www.youtube.com/watch?v=xxFYro8QuXA,xxFYro8QuXA,917.0,920.0,3.0," All right, until next time, quest on."
544,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,0.0,29.8,29.8," Step Quest in an elevator, step Quest out in the long, step Quest in a wheelbarrow. Step Quest is everywhere. Hello, I'm Josh Stormer and welcome to Step Quest. Today we're going to be talking about saturated models and deviant statistics. Note there's a Jackhammer blasting away at Concrete right across the street from me."
545,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,29.8,53.72,23.92," So if you hear a low rumbling noise, that's not my stomach, that's a Jackhammer. If you're watching this video, chances are it is because you've already stumbled over something called the saturated model. You might have seen it in the context of the log likelihood-based R-squared formula, aka McFadden's pseudo-R-squared."
546,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,53.72,80.92,27.200000000000003," Or maybe you saw the saturated model in the formula for residual deviance. Or maybe you're just here for the song. Regardless, in this video, I'll demystify the saturated model and a related concept, deviant statistics. Lastly, I'll explain why you can ignore the saturated model when you're doing logistic"
547,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,80.92,104.6,23.679999999999996," regression. Note, the saturated model and deviant statistics are part of generalized linear models, and are intended to be used in a wide variety of situations. Including, but not limited to logistic regression, linear models, and other methods. So let's dive in."
548,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,104.6,124.2,19.60000000000001," Imagine we weighed some mice. Now, just for the sake of this example, assume that we already know the standard deviation of the data. That means all we need to do to fit a normal curve to the data is estimated to mean. Bam!"
549,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,124.2,147.2,22.999999999999982," Here's our normal curve centered on the estimated mean value. The normal curve is a model of the data. We can use it instead of the original data to estimate probabilities and do statistical tests. Since we only had to estimate the mean for this model, we can say that the model has one parameter."
550,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,147.2,181.88,34.68000000000001," And since a one parameter model is as simple as it gets, we'll call it the null model. The likelihood of the data given the null model equals 0.03. And the log likelihood of the data given the null model equals the log of 0.03, which equals negative 3.51. We could also create a slightly fancier model by fitting two normal curves to the data."
551,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,181.88,213.28,31.40000000000001," This model would have two parameters, one for each mean that we estimated. Next, we're still assuming we do not have to estimate standard deviations. The likelihood of the data given the fancier model equals 3.57. And the log likelihood of the data given the fancier model equals the log of 3.57, which equals 1.27."
552,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,213.28,241.6,28.319999999999997," We could also create a super fancy model for our data. The super fancy model has one parameter per data point. The super fancy model is called the saturated model because it maxes out the number of parameters we can estimate. The likelihood of the data given the saturated model equals 1,291.5."
553,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,241.6,271.88,30.28," And the log likelihood of the data given the saturated model is the log of 1,291.5, which equals 7.16. So far, we've looked at the likelihood of the data using three different models. We looked at the likelihood of the data given the null model, which has only one parameter. We looked at the likelihood of the data given a model with two parameters."
554,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,271.88,294.96000000000004,23.08000000000004," Let's call this the proposed model. Imagine it's the one we're really interested in using. And we looked at the likelihood of the data given the saturated model, which has one parameter per data point. By calculating the likelihood of the null model, we have a sense of the worst case scenario."
555,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,295.0,318.28,23.279999999999973," The likelihood of the saturated model is as high as it can be. Ideally, we want the likelihood of the data given our proposed model to be larger than the null model and close to the saturated model. When using likelihoods, we use the null and saturated models to determine whether the proposed model is the probability of the null model."
556,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,318.32,343.48,25.159999999999968," Using likelihoods, we use the null and saturated models to determine whether the proposed model does a good job with the data. In other words, we use the null and saturated models to calculate R squared and its p-value for the proposed model. Let's start by explaining the role that the saturated model plays in the log likelihood"
557,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,343.48,367.04,23.56000000000006," based R squared. But before we get started, let me review one concept from the normal R squared, the one from linear regression calculated with the sums of squares of the residuals. With the normal R squared, the sums of squares for the null model determines the boundary for a bad fit."
558,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,367.04,393.16,26.119999999999948," And when the model fits the data perfectly, the residuals are all zero and the sums of squares for the proposed model equal zero. In other words, there is a fixed value zero that represents a boundary for the best a model can do. And when we plug in zero, we get R squared equals one."
559,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,393.16,421.44,28.28000000000003," To summarize, the normal R squared has a boundary for how poorly a model can perform, the sums of squares for the null model, and a fixed value zero that represents the best possible model. Now let's go back to talking about the log likelihood based R squared. Just like before, the null model provides a boundary for how poor a model can perform."
560,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,421.44,449.2,27.75999999999999," However, unlike the traditional R squared, there is no fixed value for an ideal fit that works in every situation. And that's where the saturated model comes in. The saturated model provides an upper bound for what an ideal fit is. Thus, if the proposed model is just as good as the saturated model, then the log likelihood"
561,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,449.2,472.24,23.04000000000002," of the proposed model will equal the log likelihood of the saturated model. And that makes the numerator the same as the denominator and R squared equals one. Bam. But not a big bam, just a little bam for now. To see the problem that the saturated model solves in action, let's see what happens when"
562,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,472.24,496.16,23.920000000000016," we exclude it from the equation and plug in the log likelihood values that we've calculated so far. First, we plug in the log likelihood for the null model that's negative 3.51. Then we plug in the log likelihood for the proposed model that equals 1.27. Now just do the math."
563,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,496.16,518.6,22.44," When we leave out the saturated model, the R squared equals 1.36. Whoa. R squared is only supposed to go from 0 to 1. We must be missing a scaling factor. Now let's redo the calculation, but this time let's plug in the value for the log likelihood"
564,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,518.6,540.08,21.480000000000015," of the saturated model. The other values are the same as before. And now we just do the math. This time we got 0.45, a reasonable value that is between 0 and 1. This deserves a big bam."
565,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,540.08,565.2800000000001,25.200000000000045," Bam. In summary, the log likelihood of the saturated model ensures that the R squared value ranges between 0 and 1. Now let's talk about deviants, a concept related to the saturated model that will, ultimately, lead us to a p value for the R squared."
566,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,565.28,598.72,33.440000000000055," We're going to talk about two types of deviants, residual deviants, and no deviants. Residual deviants is defined as two times the difference between the log likelihood of the saturated model and the log likelihood of the proposed model. The two times part makes the difference in these log likelihoods have a chi-squared distribution with degrees freedom equal to the difference in the number of parameters."
567,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,598.72,628.8000000000001,30.08000000000004," In this case, the degrees of freedom is 4. The number of parameters in the saturated model, 6, minus the number of parameters in the proposed model, 2. To see the residual deviants in action, let's plug in the numbers. Now we just do the math, and we get 11.78."
568,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,628.8000000000001,664.0,35.19999999999993," Here's a chi-squared distribution with 4 degrees of freedom. And here's where the residual deviants, 11.78, is on the x-axis. The area under the curve from 11.78 to infinity is 0.02. So the p value equals 0.02. And this means that the saturated model is significantly different from the proposed model."
569,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,664.0,694.52,30.519999999999985," To be honest, I'm not a huge fan of this statistic in and of itself, but it is very common and, as we'll see in a bit, can be used as a stepping stone for calculating the p value for R-squared. Note, for this chi-squared test to work correctly, the proposed model and the saturated model have to be nested. In other words, the proposed model has to be a simpler version of the saturated model and not"
570,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,694.52,719.8,25.279999999999973," just any old model. Now let's talk about the null deviants. The null deviants is defined as two times the difference between the log likelihood of the saturated model and the log likelihood of the null model. Again, the two times part makes the difference in these log likelihoods have a chi-squared distribution"
571,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,719.8,744.1600000000001,24.360000000000127," with degrees freedom equal to the difference in the number of parameters. In this case, the degrees of freedom is five. The number of parameters in the saturated model, six, minus the number of parameters in the null model, one. To see the null deviants in action, let's plug in the numbers."
572,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,744.1600000000001,776.16,31.999999999999886, Now we just do the math. And that gives us a value of 21.34. Here's a chi-squared distribution with five degrees of freedom. And here's where the null deviants 21.34 is on the x-axis. And the area under the curve from 21.34 to infinity is close to zero.
573,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,776.16,806.88,30.720000000000027," So the p-value equals close to zero. Thus, the null and saturated models are significantly different. Just like for the residual deviants, I'm not a huge fan of this statistic in and of itself. However, it's very common and you can calculate the p-value for the log likelihood are squared from the difference in the deviants."
574,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,806.88,831.04,24.159999999999968," This difference is a chi-squared value with degrees freedom equal to the difference in the number of parameters for the proposed model and the null model. In this case, the degrees of freedom is one. The number of parameters in the proposed model, two, minus the number of parameters in the null model, one."
575,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,831.04,862.32,31.280000000000086," Digging in the values for the null and residual deviants gives us, 9.56. Here's a chi-squared distribution with one degree of freedom. And here's where 9.56 is on the x-axis. And the area under the curve from 9.56 to infinity is 0.002."
576,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,862.32,891.68,29.3599999999999," Thus, the p-value for the log likelihood are squared value we calculated before is 0.002. This means that not only does our model have a reasonable effect size, the r-squared value, we know that that value is not by chance. Double-bound. Note, there is more than one way to calculate the chi-squared value for the r-squared."
577,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,891.68,925.0,33.32000000000005," Alternatively, we can calculate the significance of the r-squared directly by comparing the proposed model to the null model, since the null model is nested within the proposed model. The formula is simply two times the difference between the log likelihood of the proposed model and the log likelihood of the null model. Again, the two times part makes the difference in these log likelihoods have a chi-squared distribution"
578,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,925.0,950.28,25.279999999999973," with degrees freedom equal to the difference in the number of parameters. However, I mentioned the method that uses deviances because the null and residual deviances are very frequently reported. In contrast, the log likelihood of the proposed model and the log likelihood of the null model are not always reported."
579,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,950.28,973.16,22.879999999999995," Little Bingam OK, we talked about the saturated model, residual deviants, and null deviants. Now let's talk about why you can ignore the saturated model entirely when you do logistic regression. When you do logistic regression, you're fitting a squiggle to your data and calculating"
580,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,973.16,1004.12,30.96000000000004," the log likelihood of the data given that squiggle. However, with the saturated model, the squiggle fits the data perfectly. In the log likelihood of the data is zero because the log of one equals zero. So the log likelihood is just a big sum of zero. This means the equation for the likelihood based r squared is this, which boils down to"
581,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,1004.12,1034.76,30.639999999999983," this simpler equation. Similarly, the equations for residual deviants and null deviants reduce to just two times the negative log likelihoods of the proposed model and the null model. Note, these deviances will ultimately be positive numbers because the likelihoods will be numbers between zero and one and the log of a number between zero and one is a negative"
582,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,1034.76,1055.2,20.44000000000005," number. And multiplying a negative number by negative one results in a positive number. Double BAM In summary, we talked about how the saturated model provides an upper bound on the log likelihood for a model."
583,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,1055.2,1080.68,25.480000000000015," And that upper bound is useful for calculating a likelihood based r squared. However, with logistic regression, the upper bound is always going to be zero so we can ignore it. We also talked about how residual and null deviants can be used to calculate the p value for a likelihood based r squared."
584,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,1080.68,1105.36,24.680000000000064," The difference between the null deviants and the residual deviants equals a chi-squared value with degrees of freedom equal to the number of parameters in the proposed model minus the number of parameters in the null model. Her A, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe."
585,Saturated Models and Deviance,https://www.youtube.com/watch?v=9T0wlKdew6I,9T0wlKdew6I,1105.36,1116.36,11.0," And if you want to support stat quest, please click the like button below and consider buying one or two of my original songs. Alright, until next time, quest on."
586,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,0.0,11.12,11.12," Step Quest. Get them free, get them. Step Quest. Kinda sneaky. Step Quest."
587,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,11.12,29.92,18.8," Hello, I'm Josh Starman. Welcome to Step Quest. Today, at long last, we're going to cover logistic regression in our note. A link to the code, which is chock full of comments and should be easy to follow, is in the description below."
588,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,29.92,60.74,30.82," For this example, we're going to get a real data set from the UCI Machine Learning Repository. Specifically, we want the heart disease data set. Note, this is the same data set we used when we made random forests in R. If you're familiar with that data, you can skip ahead to about three minutes and 44 seconds in this video. We start by making a variable called URL, and set it to the location of the data we"
589,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,60.74,85.2,24.46," want. This is how we read the data set into R from the URL. The head function shows us the first six rows of data. Unfortunately, none of the columns are labeled. So we named the columns after the names that were listed on the UCI website."
590,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,86.2,112.6,26.40000000000001," Hey! Now, when we look at the first six rows with the head function, things look a lot better. However, the stir function, which describes the structure of the data, tells us that some of the columns are messed up. Right now, sex is a number, but it's supposed to be a factor where zero represents female"
591,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,112.6,137.92,25.319999999999997," and one represents male. CP, aka chest pain, is also supposed to be a factor where levels 1 through 3 represent different types of pain and four represents no chest pain. CA and Thaw are correctly called factors, but one of the levels is question mark when we need it to be in A."
592,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,137.92,164.56,26.64000000000001," So we've got some cleaning up to do. The first thing we do is change the question marks to NA's. Then, just to make the data easier on the eyes, we convert the zeros and sex to F for female and the ones to M for male. Lastly, we convert the column into a factor."
593,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,164.56,185.16000000000005,20.600000000000023," Then we convert a bunch of other columns into factors, since that's what they're supposed to be. See the UCI website or the sample code on the stat quest blog for more details. Since the CA column originally had a question mark in it, rather than NA, are things it's a column of strings."
594,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,185.16000000000005,206.48,21.319999999999965," We correct that assumption by telling R that it's a column of integers. And then we convert it to a factor. Then we do the same thing for Thaw. The last thing we need to do to the data is make HD, aka heart disease, a factor that is easy on the eyes."
595,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,206.48,228.12,21.64000000000001," Here I'm using a fancy trick with the if else function to convert the zeros to healthy and the ones to unhealthy. We could have done a similar trick for sex, but I wanted to show you both ways to convert numbers towards. Once we're done fixing up the data, we check that we have made the appropriate changes"
596,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,228.12,248.88,20.75999999999999," with the stir function. Hooray, it worked. Now we see how many samples rows of data have NA values. Later we will decide if we can just toss these samples out, or if we should impute values for the NA's."
597,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,248.88,268.92,20.039999999999964, Six samples rows of data have NA's and them. We can view the samples within NA's by selecting those rows from the data frame. And there they are. Here are the NA values. Five of the six samples are male.
598,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,268.92,294.76000000000005,25.84000000000009," And two of the six have heart disease. If we wanted to, we could impute values for the NA's using a random forest or some other method. However, for this example, we'll just remove these samples. Including the six samples within NA's, there are 303 samples. Then we remove the six samples that have NA's."
599,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,294.76000000000005,322.44000000000005,27.680000000000007," And after removing those samples, there are 297 samples remaining. Small bow. Now we need to make sure that healthy and disease samples come from each gender, female and male. If only male samples have heart disease, we should probably remove all females from the model. We do this with the X tab's function."
600,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,322.44000000000005,349.16,26.719999999999917," We pass X tab's the data and use model syntax to select the columns in the data we want to build a table from. In this case, we want a table with heart disease and sex. And bam. Healthy and unhealthy patients are both represented by a lot of female and male samples."
601,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,349.16,369.36,20.200000000000045," Now let's verify that all four levels of chest pain, CP for short, were reported by a bunch of patients. Yes. And then we do the same thing for all of the Boolean and categorical variables that we are using to predict heart disease."
602,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,369.36,394.28,24.91999999999996," Here's something that could cause trouble for the resting electrocardiographic results. Only four patients represent level one. This could, potentially, get in the way of finding the best fitting line. However, for now we'll just leave it in and see what happens. And then we just keep looking at the remaining variables to make sure that they're all"
603,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,394.28,413.4,19.120000000000005," represented by a number of patients. OK, we've done all the boring stuff. Now let's do logistic regression. Let's start with a super simple model. We'll try to predict heart disease using only the gender of each patient."
604,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,413.4,440.84,27.44000000000005," Here's our call to the GLM function. The function that performs generalized linear models. First, we use formula syntax to specify that we want to use sex to predict heart disease. Then we specified the data that we are using for the model. Lastly, we specify that we want the binomial family of generalized linear models."
605,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,440.88,464.84,23.95999999999998," This makes the GLM function do logistic regression as opposed to some other type of generalized linear model. Oh, I almost forgot to mention that we are storing the output from the GLM function in a variable called logistic. We then use the summary function to get details about the logistic regression."
606,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,464.84,489.04,24.200000000000045," Bam! The first line has the original call to the GLM function. Then it gives you a summary of the deviance residuals. They look good since they are close to being centered on zero and are roughly symmetrical. If you want to know more about deviance residuals, check all the stack quest."
607,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,489.04,519.04,29.999999999999943," Deviance residuals clearly explained. Then we have the coefficients. They correspond to the following model. Heart disease equals negative 1.0438 plus 1.2737 times the patient is male. The variable, the patient is male, is equal to zero when the patient is female and one when the patient is male."
608,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,519.04,556.04,37.0," Thus, if we are predicting heart disease for a female patient, we get the following equation. Heart disease equals negative 1.0438 plus 1.2737 times zero. This reduces two heart disease equals negative 1.0438. Thus, the log odds that a female has heart disease equals negative 1.0438. If we were predicting heart disease for a male patient, we get the following equation."
609,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,556.04,609.04,53.0," Heart disease equals negative 1.0438 plus 1.2737 times 1. That reduces two heart disease equals negative 1.0438 plus 1.2737. Since this first term is the log odds of a female having heart disease, the second term indicates the increase in the log of the odds that a male has of having heart disease. In other words, this second term is the log of the odds ratio of the odds that a male will have heart disease over the odds that a female will have heart disease. This part of the Logistic Regression Output shows how the wall's test was computed for both coefficients."
610,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,609.04,644.04,35.0," And here are the P values. Both P values are well below 0.05 and thus, the log of the odds and the log of the odds ratios are both statistically significant. But remember, a small P value alone isn't interesting. We also want large effect sizes, and that's what the log of the odds and the log of the odds ratio tells us. If you want to know more details on the coefficients and the wall test, check out the following stat quests."
611,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,644.04,691.04,47.0," odds and the log odds clearly explained odds ratios and log odds ratios clearly explained in logistic regression details part 1 coefficients. Next, we see the default dispersion parameter used for this logistic regression. When we do normal linear regression, we estimate both the mean and the variance from the data. In contrast, with logistic regression, we estimate the mean of the data and the variance is derived from the mean. Since we are not estimating the variance from the data, and instead just deriving it from the mean, it is possible that the variance is underestimated."
612,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,691.04,721.04,30.0," If so, you can adjust the dispersion parameter in the summary command. Then we have the null deviants and the residual deviants. These can be used to compare models, compute our squared, and an overall P value. For more details, check out the stat quests. Logistic regression details part 3 are squared and its P value, and saturated models and deviants statistics clearly explained."
613,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,722.04,763.04,41.0," Then we have the AIC, the KIKI information criterion, which in this context is just the residual deviants adjusted for the number of parameters in the model. The AIC can be used to compare one model to another. Lastly, we have the number of Fisher scoring iterations, which just tells us how quickly the GLM function converged on the maximum likelihood estimates for the coefficient. If you want more details on how the coefficients were estimated, check out the stat quest. Logistic regression details part 2, fitting aligned with maximum likelihood."
614,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,763.04,805.04,42.0," Double bound. Now that we have done a simple logistic regression using just one of the variables, sex, to predict heart disease, we can create a fancy model that uses all of the variables to predict heart disease. This formula syntax, HD, tilde, dot means that we want to model heart disease, HD, using all of the remaining variables in our data frame called data. We can then see what our model looks like with the summary function. Dang! The summary goes off the screen."
615,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,806.04,846.04,40.0," No worries, we'll just talk about a few of the coefficients. We see that age isn't a useful predictor because it has a large p-value. However, the median age in our data set was 56, so most of the folks were pretty old and that explains why it wasn't very useful. Gender is still a good predictor though. If we scroll down to the bottom of the output, we see that the residual deviants and the AIC are both much smaller for this fancy model than they were for the simple model, when we only use gender to predict heart disease."
616,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,846.04,884.04,38.0," If we want to calculate McFadden's pseudo-R squared, we can pull the log likelihood of the null model out of the logistic variable by getting the value for the null deviants and dividing by negative 2. Then we just do the math, and we end up with a pseudo-R squared equal 0.55. This can be interpreted as the overall effect size. And we can use those same log likelihoods to calculate a p-value for that R squared using a chi-squared distribution. In this case, the p-value is tiny, so the R squared value isn't due to dumb luck."
617,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,884.04,913.04,29.0," One last shameless self-promotion, more details in the R squared and p-value can be found in the following stat quest. Logically, we can use the same log likelihoods to calculate a p-value for that R squared using a chi-squared distribution. In this case, the p-value is tiny, so the R squared value isn't due to dumb luck. More details in the R squared and p-value can be found in the following stat quest. Logistic regression details part 3, R squared and its p-value."
618,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,913.04,948.04,35.0," Lastly, we can draw graph that shows the predicted probabilities that each patient has heart disease along with their actual heart disease status. I'll show you the code in a bit. Most of the patients with heart disease, the ones in turquoise, are predicted to have a high probability of having heart disease. And most of the patients without heart disease, the ones in salmon, are predicted to have a low probability of having heart disease. Thus, our logistic regression has done a pretty good job."
619,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,948.04,989.04,41.0," However, we could use cross-validation to get a better idea of how well it might perform with new data, but we'll say that for another day. To draw the graph, we start by creating a new data frame that contains the probabilities of having heart disease along with the actual heart disease status. Then we sort the data frame from low probabilities to high probabilities. Then we add a new column to the data frame that has the rank of each sample from low probability to high probability. Then we load the GG plot 2 library so we can draw fancy graph."
620,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,989.04,1012.04,23.0," Then we load the cow plot library so that GG plot has nice looking to faults. Then we call GG plot and use G on point to draw the data. And lastly, we call GG save to save the graph as a PDF file. Triple BAM! Hooray!"
621,"Logistic Regression in R, Clearly Explained!!!!",https://www.youtube.com/watch?v=C4N3_XJJ-jU,C4N3_XJJ-jU,1012.04,1031.04,19.0," We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe. And if you want to support stat quest, well, please click the like button below and consider buying one or two of my original songs. Alright, until next time, quest on!"
622,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,0.0,25.16,25.16," If I was a cat, I'd be sleeping on the couch, but I'm not a cat. So I'm watching Stack Quest, Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about Deviant's residuals and they're going to be clearly explained."
623,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,25.16,55.48,30.32, This Stack Quest follows up on the Stack Quest on saturated models and Deviant's statistics. So watch that video first if you're not familiar with those topics. We already know that the residual deviants is defined as two times the difference between the log likelihood of the saturated model and the log likelihood of the proposed model. But start by reviewing how the residual deviants is calculated.
624,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,55.48,77.72,22.24," The likelihood of the data given the saturated model is equal to the likelihood of the first data point times the likelihood of the second data point times the likelihood of the third data point. Note we're just using three data points in this example. However, if you've got more data, you just keep adding them to the multiplication."
625,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,77.72,102.56,24.840000000000003," And the log likelihood is just, duh, the log of the likelihood. And that's what we plug into the first part of the equation. The likelihood of the data given the proposed model equals the likelihood of the first data point times the likelihood of the second data point times the likelihood of the third data point."
626,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,102.56,129.72,27.16, Let me just take the log of the likelihood and plug that into the last part of the equation. Putting the log likelihood together gets the difference in the likelihoods. These terms represent the difference in likelihood between the saturated and proposed model for the first data point. These terms represent the difference for the second data point.
627,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,129.72,155.48,25.75999999999999," And these terms represent the difference for the third data point. We can rearrange the equation for residual deviants to reflect the difference in likelihoods for the individual data points. Again, the first term represents the difference for the first data point. The second term represents the difference for the second data point."
628,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,155.48,186.04,30.56," And the third term represents the difference for the third data point. Deviants residuals are just the square roots of the individual terms. In other words, the deviants residuals represent the square root of the contribution that each data point has to the overall residual deviants. The first deviants residual represents the square root of the difference in log likelihoods"
629,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,186.04,207.12,21.079999999999984, for the first Dations point. The second deviants residual represents the square root of the difference in log likelihoods for the second data point. This last Deviants residual represents the square route of the difference in log likelihoods for the 3rd Data point.
630,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,207.12,234.56,27.44," If we then square the individual deviants residuals and then add them up, we will end up with the residual deviants. Thus, the deviants residuals are analogous to the residuals and ordinarily squares. When we square these residuals, we get the sums of squares that we use to assess how well the model fits the data."
631,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,234.56,259.2,24.639999999999983," Similarly, when we square the deviants residuals, we get the residual deviants that we use to assess how well a model fits the data. Bam! When doing logistic regression, the equation for deviants residuals can be rewritten to be based on the distances between the data and the best fitting line."
632,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,259.2,280.92,21.71999999999997, The blue deviants residuals calculated with the blue distances are positive because they are above the squiggly line. And the deviants residuals calculated with the red distances are negative because they are below the squiggly line. Making the residuals above the squiggle positive and the residuals below the squiggle
633,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,280.92,303.04,22.12000000000006," negative helps us identify outliers. If you plot them on an xy graph, they should be centered and relatively close to zero. Here's the first deviants residual. It's close to zero because the squiggly line is very close to it. Here's the second deviants residual."
634,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,303.04,323.52,20.47999999999996, It's a lot further from zero since the squiggly line doesn't fit it well. Here's the third deviants residual. Here's the fourth deviants residual. Here's the fifth deviants residual. And here's the last deviants residual.
635,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,323.52,350.44,26.920000000000016," The second and fifth deviants residuals are relatively far from zero and may be outliers. We should make sure we didn't make a mistake when we originally labeled them. Double-band. In summary, deviants residuals represent the square root of the contribution that each data point has to the overall residual deviants."
636,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,350.44,372.0,21.56," And we use them to identify outliers. Here we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, please click the like button below and consider buying one or two of my original songs."
637,Deviance Residuals,https://www.youtube.com/watch?v=JC56jS2gVUE,JC56jS2gVUE,372.0,375.0,3.0," All right. Until next time, quest on."
638,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,0.0,20.2,20.2," regularization. It's just another way to say Dixin's a Tazation. Let's check it out with Wichord regression, Stacquest. Hello, I'm Josh Starmer and welcome to Stacquest. Today we're going to do part one of a series of video on regularization techniques."
639,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,20.2,38.4,18.2," In this video we're going to cover Regretion and it's going to be clearly explained. Note, this Stacquest assumes you understand the concepts of bias and variance in the context of machine learning. If not, check out machine learning fundamentals, bias and variance."
640,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,39.4,57.4,18.0," It also assumes that you are familiar with linear models. If not, check out the following Stacquest. The links are in the description below. Lastly, if you're not already familiar with the concept of cross validation, check out the Stacquest on cross validation."
641,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,57.4,82.4,25.000000000000007," In this Stacquest, we will, one, look at a simple example that shows the main idea is behind Regretion. Two, go into details about how Regretion works. Three, show how Regretion works in a variety of situations. And four, lastly we'll talk about how Regretion can solve the unsolvable"
642,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,82.4,105.4,23.0," BAM. Let's start by collecting weight and size measurements from a bunch of mice. Since these data look relatively linear, we will use linear regression, aka least squares, to model the relationship between weight and size. So we'll fit a line to the data using least squares."
643,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,105.4,134.4,29.0," In other words, we find the line that results in the minimum sum of squared residuals. Ultimately, we end up with this equation for the line. The line has two parameters, a y-axis intercept and a slope. We can plug and evaluate for weight, for example, 2.5, and do the math, and get a value for size."
644,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,134.4,156.4,22.0," Together, the value for weight, 2.5, and the value for size, 2.8, give us a point on the line. When we have a lot of measurements, we can be fairly confident that the least squares line accurately reflects the relationship between size and weight. But what if we only have two measurements?"
645,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,156.4,186.4,30.0," We fit a new line with least squares. Since the new line overlaps to two data points, the minimum sum of squared residuals equals zero. Ultimately, we end up with this equation for the new line. Note, here are the original data and the original line for comparison. Let's call the two red dots the training data in the remaining green dots the testing data."
646,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,186.4,212.4,26.0," The sum of the squared residuals for just the two red points, the training data, is small. In this case, it is zero. But the sum of the squared residuals for the green points, the testing data, is large. And that means that the new line has high variance. In machine learning lingo, we'd say that the new line is over fit to the training data."
647,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,212.4,244.4,32.0," Now let's go back to just the training data. We just saw that least squares results in a line that is over fit and has high variance. The main idea behind the re-church regression is to find a new line that doesn't fit the training data as well. In other words, we introduce a small amount of bias into how the new line is fit to the data. But in return for that small amount of bias, we get a significant drop in variance."
648,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,244.4,274.4,29.99999999999997," In other words, by starting with a slightly worse fit, re-church regression can provide better long-term predictions. Bam! Now let's dive into the nitty-gritty and learn how re-church regression works. Let's go back to just the training data. When least squares determines values for the parameters in this equation, it minimizes the sum of the squared residuals."
649,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,274.4,308.4,34.0," In contrast, when re-church regression determines the values for the parameters in this equation, it minimizes the sum of the squared residuals, plus lambda times the slope squared. Note, I usually try to avoid using Greek characters as much as possible, but if you are ever going to do re-church regression in practice, you have to know that this term is called lambda. This part of the equation adds a penalty to the traditional least squares method."
650,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,308.4,335.4,27.0," And lambda determines how severe that penalty is. To get a better idea of what's going on, let's plug in some numbers. Let's start by plugging in the numbers that correspond to the least squares fit. The sum of the squared residuals for the least squares fit is zero, because the line overlaps the data points. And the slope is 1.3."
651,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,336.4,366.4,30.0," We'll talk more about lambda later, but for now, let lambda equal 1. All together, we have zero plus 1 times 1.3 squared. And when we do the math, we get 1.69. Now let's see what happens when we plug in numbers for the rigid regression line. The sum of the squared residuals is zero point 3 squared for this residual,"
652,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,366.4,395.4,29.0," plus zero point 1 squared for this residual. The slope is zero point 8. And just like before, we'll let lambda equal 1. All together, we have zero point 3 squared, plus zero point 1 squared, plus 1 times zero point 8 squared. And when we do the math, we get 0.74."
653,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,395.4,433.4,38.0," For the least squares line, the sum of squared residuals, plus the rigid regression penalty, is 1.69. For the rigid regression line, the sum of squared residuals, plus the rigid regression penalty, is 0.74. Thus, if we wanted to minimize the sum of the squared residuals, plus the rigid regression penalty, we would choose the rigid regression line over the least squares line. Without the small amount of bias at the penalty creates, the least squares fit has a large amount of variance."
654,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,433.4,478.4,45.0," In contrast, the rigid regression line, which has a small amount of bias due to the penalty, has less variance. Now, before we talk about lambda, let's talk a little bit more about the effect that the rigid regression penalty has on how the line is fit to the data. To keep things simple, imagine we only have one line. This line suggests that for every one unit increase in weight, there is a one unit increase in predicted size. If the slope of the line is steeper, then for every one unit increase in weight, the prediction for size increases by over two units."
655,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,478.4,527.4,49.0," In other words, when the slope of the line is steep, then the prediction for size is very sensitive to relatively small changes in weight. When the slope is small, then for every one unit increase in weight, the prediction for size barely increases. In other words, when the slope of the line is small, then predictions for size are much less sensitive to changes in weight. Now let's go back to the least squares and rigid regression lines fit to the two data points. The rigid regression penalty resulted in a line that has a smaller slope, which means that predictions made with the rigid regression line are less sensitive to weight than the least squares line."
656,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,527.4,554.4,27.0," Bam! Now let's go back to the equation that rigid regression tries to minimize and talk about lambda. Lambda can be any value from zero to positive infinity. When lambda equals zero, then the rigid regression penalty is also zero. And that means that the rigid regression line will only minimize the sum of squared residuals."
657,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,554.4,588.4,34.0," And the rigid regression line will be the same as the least squares line because they are both minimizing the exact same thing. Now let's see what happens as we increase the value for lambda. In the example we just looked at, we set lambda equals one and the rigid regression line ended up with a smaller slope than the least squares line. When we set lambda equals two, the slope gets even smaller. And when we set lambda equals three, the slope is even smaller."
658,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,588.4,618.4,30.0," And the larger we make lambda, the slope gets asymptotically close to zero. So the larger lambda gets, our prediction for size become less and less sensitive to weight. So how do we decide what value to give lambda? We just try a bunch of values for lambda and use cross validation. Typically, 10 fold cross validation to determine which one results in the lowest variance."
659,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,618.4,648.4,30.0," Double bound. In the previous example, we showed how rigid regression would work when we want to predict size, which is a continuous variable, using weight, which is also a continuous variable. However, rigid regression also works when we use a discrete variable, like normal diet versus high fat diet, to predict size. In this case, the data might look like this."
660,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,648.4,688.4,40.0," In the least squares fitted equation might look like this. Where 1.5, the equivalent of a y-intercept corresponds to the average size of the mice on the normal diet. In 0.7, the equivalent of a slope corresponds to the difference between the average size for the mice on the normal diet compared to the mice on the high fat diet. Note, from here on out, we'll refer to this distance as diet difference. High fat diet is either zero for mice on the normal diet or one for mice on the high fat diet."
661,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,689.4,728.4,39.0," In other words, this term alone predicts the size of mice on the normal diet. In the sum of these two terms, is the prediction for the size of mice on the high fat diet. For the mice on the normal diet, the residuals are the distances between the mice and the normal diet mean. And for mice on the high fat diet, the residuals are the distances between the mice and the high fat diet mean. When these squares determines the values for the parameters in this equation, it minimizes the sum of the squared residuals."
662,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,728.4,765.4,37.0," In other words, these distances between the data and the means are minimized. When rich regression determines values for the parameters in this equation, it minimizes the sum of the squared residuals, plus lambda times diet difference squared. Remember, diet difference simply refers to the distance between the mice on the normal diet and the mice on the high fat diet. When lambda equals zero, this whole term ends up being zero. And we get the same equation that we got with least squares."
663,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,765.4,808.4,43.0," But when lambda gets large, the only way to minimize the whole equation is to shrink diet distance down. In other words, as lambda gets larger, our prediction for the size of mice on the high fat diet becomes less sensitive to the difference between the normal diet and the high fat diet. And remember, the whole point of doing rich regression is because small sample sizes like these can lead to poorly squares estimates that result in terrible machine learning predictions. Bam! Rich regression can also be applied to logistic regression."
664,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,808.4,852.4,44.0," In this example, we are using weight to predict if a mouse's obese or not. This is the equation for this logistic regression. And, rich regression would shrink the estimate for the slope, making our prediction about whether or not of mouse's obese less sensitive to weight. Note, when applied to logistic regression, rich regression optimizes the sum of the likelihoods instead of the squared residuals because logistic regression is solved using maximum likelihood. So far, we've seen simple examples of how rich regression helps reduce variance by shrinking parameters and making our predictions less sensitive to them."
665,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,852.4,880.4,28.0," But we can apply rich regression to complicated models as well. In this model, we've combined the weight measurement data from the first example with the two diets from the second example. Combining these two datasets gives us this equation. And, rich regression tries to minimize this. Now, the rich regression penalty contains the parameters for the slope and the difference between diets."
666,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,880.4,920.4,40.0," In general, the rich regression penalty contains all of the parameters except for the y intercept. If we had a big, huge, crazy equation with terms for astrological sign, the airspeed of a swallow and other stuff, then the rich regression penalty would have all those parameters squared except for the y intercept. Every parameter except for the y intercept is scaled by the measurements. And that's why the y intercept is not included in the rich regression penalty. Double bam."
667,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,920.4,946.4,26.0," Okay. Now the next thing we're going to talk about is going to sound totally random, but trust me, it will lead to the coolest thing about rich regression. It's so cool, it's almost like magic. We all know that this is the equation for a line. And in order for least squares to solve for the parameters, the y intercept and slope, we need at least two data points."
668,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,946.4,984.4,38.0," These data points result in these parameters, and this specific line. If we only have one data point, then we wouldn't be able to solve for these parameters, because there would be no way to tell if this line is better than this line or this line, or any old line that goes through the one data point. All of these lines have zero residuals, and thus all minimize the sum of the squared residuals. It's not until we have two data points that it becomes clear that this is the least squares solution."
669,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,984.4,1020.4,36.0," Now let's look at an equation that has three parameters to estimate. We need to estimate a y intercept, a slope that reflects how weight contributes to the prediction of size, and a slope that reflects how age contributes to the prediction of size. When we have three parameters to estimate, then just two data points isn't going to cut it. That's because, in three dimensions, which is what we get when we add another access to our graph for age, we have to fit a plane to the data instead of just a line."
670,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,1020.4,1061.4,41.000000000000114," And with only two data points, there's no reason why this plane fits the data any better than this plane or this plane. But as soon as we have three data points, we can solve for these parameters. If we have an equation with four parameters, then least squares needs at least four data points to estimate all four parameters. And if we have an equation with 10,000 and one parameters, then we need at least 10,000 and one data points to estimate all of the parameters. An equation with 10,000 and one parameters might sound bonkers, but it's more common than you might expect."
671,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,1061.4,1097.4,36.0," For example, we might use gene expression measurements from 10,000 genes to predict size. And that would mean we would need gene expression measurements from 10,000 and one mice. Unfortunately, collecting gene expression measurements from 10,000 and one mice is crazy expensive and time consuming right now. In practice, a huge data set might have measurements from 500 mice. So what do we do if we have an equation with 10,000 and one parameters and only 500 data points?"
672,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,1097.4,1134.4,37.0," We use Ridge regression. It turns out that by adding the Ridge regression penalty, we can solve for all 10,000 one parameters with only 500 or even fewer samples. One way to think about how Ridge regression can solve for parameters when there isn't enough data is to go back to our original size versus weight example. Only this time, there's only one data point in the training set. Least squares can't find a single optimal solution since any line that goes through the dot will minimize the sum of the squared residuals."
673,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,1134.4,1176.4,42.0," But Ridge regression can find a solution with cross validation and the Ridge regression penalty that favors smaller parameter values. Since this stat quest is already super long, we'll save a more thorough discussion of how this works for a future stat quest. Triple BAM. In summary, when the sample sizes are relatively small, then Ridge regression can improve predictions made from new data, i.e. reduced variance by making the predictions less sensitive to the training data. This is done by adding the Ridge regression penalty to the thing that must be minimized."
674,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,1176.4,1210.4,34.0," The Ridge regression penalty itself is lambda times the sum of all squared parameters except for the y intercept. And lambda is determined using cross validation. Lastly, even when there isn't enough data to find the least squares parameter estimates, Ridge regression can still find a solution using cross validation and the Ridge regression penalty. Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
675,Regularization Part 1: Ridge (L2) Regression,https://www.youtube.com/watch?v=Q81RR3yKn30,Q81RR3yKn30,1210.4,1218.4,8.0," And if you want to support stat quest, well, consider buying one or two of my original songs. Alright, until next time, quest on."
676,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,0.0,19.0,19.0," LASO and Rich Recreation are similar, but there's a big, important difference. We'll talk about it. StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest."
677,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,19.0,38.0,19.0," Today we're going to do part two of our series on regularization. We're going to talk about LASO regression and it's going to be clearly explained. This StatQuest follows up on the one on Ridge regression, so if you aren't already familiar with that, check it out. Even if you are familiar with Ridge regression,"
678,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,38.0,59.0,21.0," you should seriously consider watching or at least skimming that StatQuest, because the examples in this video are based on the ones in that video. LASO regression is very, very similar to Ridge regression, but it has some very, very important differences. To understand those similarities and differences,"
679,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,59.0,77.0,18.0," let's first do a super quick review of Ridge regression. In the StatQuest on Ridge regression, we started out with weight and size measurements from a bunch of mice, and we split the data into two sets. The red dots were training data,"
680,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,77.0,100.0,23.0," and the green dots were testing data. Then we fit a line to the training data using least squares. In other words, we minimize the sum of the squared residuals. When we did this, we saw that even though the line fit the training data really well, that is to say it had low bias."
681,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,100.0,118.0,18.0," It did not fit the testing data very well at all. That is to say it had high variance. Then we fit a line to the data using Ridge regression. We minimized the sum of the squared residuals, plus lambda times the slope squared."
682,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,118.0,138.0,20.0," Ridge regression is just least squares, plus the Ridge regression penalty. The blue Ridge regression line did not fit the training data as well as the red least squares line. In other words, Ridge regression had more bias than least squares."
683,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,138.0,157.0,19.0," But in return for that small amount of bias, the Ridge regression line had a significant drop in variance. The main idea was that by starting with a slightly worse fit, Ridge regression provided better long term predictions. Bam."
684,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,157.0,177.0,20.0," Now let's go back to the equation that Ridge regression minimizes, and focus on the Ridge regression penalty. If instead of squaring the slope, we take the absolute value, then we have LASO regression. Note, just like with Ridge regression,"
685,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,177.0,195.0,18.0," lambda can be any value from zero to positive infinity, and is determined using cross validation. Like Ridge regression, LASO regression, the orange line results in a line with a little bit of bias. But less variance than least squares."
686,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,195.0,206.0,11.0, Bam. Ridge regression. And LASO regression. Look very similar. And they do similar things.
687,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,206.0,228.0,22.0," In this case, they make our predictions of size less sensitive to this tiny training data set. Both Ridge and LASO regression can be applied in the same context, which is a situation where we are using two different diets to predict size. Or an allogistic regression setting where we use weight to predict obesity."
688,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,228.0,252.0,24.0," And both Ridge and LASO regression can be applied to complicated models that combine different types of data. In this case, we've combined the data from the first two examples, weight, which is continuous, and high-fat diet, which is discrete. Just like the Ridge regression penalty, the LASO regression penalty contains all of the estimated parameters"
689,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,252.0,276.0,24.0," except for the Y intercept. It's also worth mentioning that when Ridge and LASO regression shrink parameters, they don't have to shrink them all equally. For example, if these were the training data, and these were the testing data, then when lambda equals zero, we would start with these least-squares estimates for the slope"
690,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,276.0,299.0,23.0," and the offset for diet difference. But as we increase the value for lambda, Ridge and LASO regression may shrink diet difference a lot more than they shrink the slope. Okay, we've seen how Ridge and LASO regression are similar. Now let's talk about the big difference between them."
691,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,300.0,328.0,28.0," To see what makes LASO regression different from Ridge regression, let's go back to the two sample training data. And let's focus on what happens when we increase the value for lambda. When lambda equals zero, then the LASO regression line will be the same as the least-squares line. As lambda increases in value, the slope gets smaller, until the slope equals zero."
692,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,329.0,353.0,24.0," Bam! The big difference between Ridge and LASO regression is that Ridge regression can only shrink the slope asymptotically close to zero, while LASO regression can shrink the slope all the way to zero. To appreciate this difference, let's look at a big, huge, crazy equation."
693,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,353.0,388.0,35.0," The goal of this equation is to predict size. The terms for weight and high-fat diet are both reasonable things to use to predict size. But the astrological sign and the air speed of a swallow, African or European, are terrible ways to predict size. When we apply Ridge regression to this equation, we find the minimal sum of the squared residuals plus the Ridge regression penalty. In the larger we make lambda, these parameters might shrink a little bit,"
694,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,388.0,419.0,31.0," and these parameters might shrink a lot, but they will never be equal to zero. In contrast, with LASO regression, when we increase the value for lambda, then these parameters will shrink a little bit, and these parameters will go all the way to zero. And these terms go away. And we're left with a way to predict size that only includes weight and diet."
695,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,419.0,445.0,26.0," And excludes all of the silly stuff. Since LASO regression can exclude useless variables from equations, it is a little better than Ridge regression at reducing the variance in models that contain a lot of useless variables. In contrast, Ridge regression tends to do a little better when most variables are useful. Double-band."
696,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,445.0,474.0,29.0," In summary, Ridge regression is very similar to LASO regression. And the superficial difference is that Ridge regression squares the variables, and LASO regression takes the absolute value. But the big difference is that LASO regression can exclude useless variables from equations. This makes the final equations simpler and easier to interpret."
697,Regularization Part 2: Lasso (L1) Regression,https://www.youtube.com/watch?v=NGf0voTMlcs,NGf0voTMlcs,474.0,492.0,18.0," Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, please consider buying one or two of my original songs. All right, until next time, quest on."
698,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,0.0,23.36,23.36," Rich, regression versus LASO regression, which one will survive? StatQuest. Hello, I'm Josh Stormer and welcome to StatQuest. Today we're going to talk about Rage versus LASO regression and the differences are going to be visualized."
699,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,23.36,47.76,24.4," Note, this StatQuest assumes that you are already familiar with Rage and LASO regression. If not, check out the quests. To show you the difference between Rage and LASO regression, we're going to use a very simple dataset that consists of weight and height measurements. And we'll start by fitting this horizontal line to the data."
700,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,47.76,71.84,24.080000000000005," This horizontal line represents a terrible fit. And we can measure how bad that fit is by calculating residuals, the difference between the observed and predicted values. In order to compare the horizontal line to other lines fit to the data, we will plot the sum of the squared residuals on a graph."
701,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,71.84,97.64,25.799999999999983," The y-axis on this graph is the sum of the squared residuals. And the x-axis represents different slope values for the fitted line. In this case, the slope for the horizontal line is zero. Now let's increase the slope to 0.2 and calculate a new value for the sum of the squared residuals."
702,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,97.64,123.08,25.44000000000001, Now let's increase the slope to 0.4 and calculate a new value for the sum of the squared residuals. We can keep plugging in new values for the slope and plotting the sum of the squared residuals or we can just plot the curve for this equation. We can see that the best fitting line is at the bottom of the parabola.
703,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,123.08,155.20000000000002,32.12000000000002," In other words, when the slope equals 0.45, we get the lowest sum of the squared residuals. In this example, we simply calculated the sum of the squared residuals for different slopes. Now let's add the ridge regression penalty, aka the L2 norm. Note, if you asked me, it should be called the squared penalty, since that's what it is, and for me, way easier to remember."
704,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,155.20000000000002,183.6,28.40000000000001," The thick blue line that we just drew represents lambda equals 0. This is because when lambda equals 0, the penalty is equal to 0, regardless of the slope, and we are left with the original sum of squared residuals. Now let's see what happens when we set lambda equal to 10. And just like before, we'll start with a horizontal line."
705,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,183.6,207.28,23.67999999999998," Only this time, the line is orange. Just like before, we can calculate the residuals. And we can calculate the sum of the squared residuals plus lambda times the slope squared. In this case, lambda equals 10. And the slope of the horizontal line is 0."
706,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,207.28,233.52,26.24000000000001," So the penalty is 0. So we plot the sum of the squared residuals here. Now let's increase the slope to 0.2. Note, the residuals are smaller than before, so the sum of squared residuals is smaller than before. But now the penalty is 0.4."
707,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,233.52,254.0,20.47999999999999, And that gives us this point on the graph. Now let's increase the slope to 0.4. And the residuals are even smaller. Now the penalty is 1.6. And that gives us this point on the graph.
708,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,254.0,278.92,24.920000000000016," And like we did before, we can keep plugging in new values for the slope and plotting the sum of the squared residuals plus the penalty. Or we can just plot the curve with a thick orange line that represents lambda equals 10. The bottom of the parabola is where the slope gives us the lowest sum of squared residuals plus penalty."
709,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,278.92,307.12,28.19999999999999," And that corresponds to this specific line. And when we compare that to the optimal slope when lambda equals 0, we see that setting lambda equal to 10 results in a smaller optimal slope. Note, we can also see that when lambda equals 10, the lowest point in the parabola is closer to 0 than when lambda equals 0."
710,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,307.12,332.04,24.920000000000016," So either way we look at it, we see that the larger value for lambda shrunk the optimal value for the slope. Likewise, the thick green line represents lambda equals 20. We see that the minimum value is closer to 0. And the optimal slope has shrunk some more."
711,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,332.04,360.96,28.91999999999996," The purple lines represent lambda equals 40, and it shrinks the slope even more. In other words, as we increase lambda for the ridge regression penalty, aka the L2 penalty, aka the square penalty, the optimal slope gets closer and closer. It's closer to 0, but it does not equal 0. Bam!"
712,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,360.96,391.96,31.0," Now let's see what happens if we use the L2 penalty, aka the L1 norm, or if you asked me, I'd call it the absolute value penalty. Unfortunately, no one asked me. Again, the thick blue line represents lambda equals 0, so there is no extra penalty. This is because, when we plug lambda equals 0 into the equation, the penalty becomes 0."
713,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,391.96,415.16,23.19999999999999," And we are left with the original sum of the squared residuals. Note, just like before, we'll keep track of the best fitting line plus penalty in this graph on the left. The thick orange line represents lambda equals 10, so now we are turning on the penalty and shrinking the slope."
714,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,415.16,452.28,37.12000000000006," Note, when lambda equals 10, we start to see a king in the curve where the slope is 0. The thick green line represents lambda equals 20, and this king at 0 is becoming more prominent. Lastly, the thick purple line represents lambda equals 40, and now the king at 0 is super obvious. Now the lowest point in the purple curve, aka the optimal slope given the absolute value penalty when lambda equals 40 is 0."
715,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,452.28,486.16,33.87999999999994," And that means the slope of the optimal line is 0. And that means when lambda equals 40, we ignore weight as a variable when predicting height. Double bound. In summary, when we increase the ridge regression penalty, aka the L2 penalty, aka the square penalty, the optimal slopes shift towards 0, but we retain a nice parabola shape."
716,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,486.16,516.88,30.71999999999997," And even when we set lambda to something crazy high like 400, we still end up with an optimal value greater than 0. In contrast, when we increase the last penalty, aka the L1 penalty, aka the absolute value penalty, the optimal value shifts towards 0, but since we have a king at 0, 0 ends up being the optimal slope."
717,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,516.88,538.32,21.44000000000005," Bam! We've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just"
718,"Ridge vs Lasso Regression, Visualized!!!",https://www.youtube.com/watch?v=Xm2C_gTAl8c,Xm2C_gTAl8c,538.32,545.16,6.839999999999918," donate. The links are in the description below. Until next time, quest on!"
719,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,0.0,33.0,33.0," A last-tick-netic regression sounds so crazy fancy, but it's way, way simpler than you might expect. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to do part three of our series on regularization. We're going to cover elastic-net regression and it's going to be clearly explained."
720,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,33.0,60.0,27.0," This Stat Quest follows up on the Stat Quest on regularization and last-tick regression, so if you aren't already familiar with them, check them out. We ended the Stat Quest on last-tick regression by saying that it works best when your model contains a lot of useless variables. So if this was the model we were using to predict size, then last-tick regression would keep the terms for weight and high-fat diet,"
721,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,60.0,90.0,30.0," and it would eliminate the terms for astrological sign and the air speed of a swallow, African or European, creating a simpler model that is easier to interpret. We also said that rigid regression works best when most of the variables in your model are useful. So if we were trying to predict size using a model we're most of the variables we're useful, then rigid regression will shrink the parameters, but we'll not remove any of them."
722,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,90.0,117.0,27.0," Great. When we know a lot about all of the parameters in our model, it's easy to choose if we want to use last-tower regression or rigid regression. But what do we do when we have a model that includes tons more variables? Last week I went to a deep learning conference and people there were using models that included millions of parameters, far too many to know everything about."
723,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,117.0,152.0,35.0," And when you have millions of parameters, then you will almost certainly need to use some sort of regularization to estimate them. However, the variables in those models might be useful or useless, we don't know in advance. So how do you choose if you should use lasso or rigid regression? The good news is that you don't have to choose, instead use elastic net regression. Alastic net regression sounds super fancy, but if you already know about lasso and rigid regression, it's super simple."
724,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,152.0,173.0,21.0," Just like lasso and rigid regression, elastic net regression starts with least squares. Then it combines the lasso regression penalty. Yihah! With the rigid regression penalty. Or, all together, elastic net combines the strengths of lasso and rigid regression."
725,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,173.0,214.0,41.0," Note, the lasso regression penalty and the rigid regression penalty get their own land as. Land as sub 1 for lasso and land as sub 2 for ridge. We use cross validation on different combinations of land as sub 1 and land as sub 2 to find the best values. When both land as sub 1 and land as sub 2 equals 0, then we get the original least squares parameter estimates. When land as sub 1 is greater than 0 and land as sub 2 equals 0, then we get lasso regression."
726,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,215.0,258.0,43.0," When land as sub 1 equals 0 and land as sub 2 is greater than 0, then we get ridge regression. And when both land as sub 1 is greater than 0 and land as sub 2 is greater than 0, then we get a hybrid of the 2. The hybrid elastic net regression is especially good at dealing with situations when there are correlations between parameters. This is because, on its own, lasso regression tends to pick just one of the correlated terms and eliminate the others. Whereas ridge regression tends to shrink all of the parameters for the correlated variables together."
727,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,258.0,284.0,26.0," By combining lasso and ridge regression, elastic net regression groups and shrinks the parameters associated with the correlated variables and leaves them in the equation where we remove them all at once. Bam! In summary, elastic net regression combines the lasso regression penalty. Yihah! With the ridge regression penalty."
728,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,284.0,298.0,14.0," Burr. And by doing so, gets the best of both worlds. Plus it does a better job dealing with correlated parameters. Hurray! We've made it to the end of another exciting stat quest."
729,Regularization Part 3: Elastic Net Regression,https://www.youtube.com/watch?v=1dKRdX9bfIo,1dKRdX9bfIo,298.0,310.0,12.0," If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, consider buying one or two of my original songs. Alright, until next time, quest on!"
730,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,0.0,36.0,36.0," E-lasted regression won't help you to sing into, but if you do it and are, it's not that hard. Stat Quest. Hello, I'm Josh Starman, welcome to Stat Quest. Today we're going to talk about Ridge, Lazo, and Elastic Network, and R. Note, this Stat Quest assumes you're already familiar with the concepts behind Ridge, Lazo, and Elastic Networks."
731,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,36.0,65.0,29.0," If not, check out the quests. To do Ridge, Lazo, and Elastic Networks, and R, we will use the GLMNet library. The GLM part of GLMNet stands for generalized linear models, which means that this tool can be applied to linear regression and logistic regression. As well as a few other models. The net part of GLMNet is from ElasticNet, at least I'm pretty sure that's where it's from."
732,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,65.0,106.0,41.0," If you know otherwise, put it in the comments below. In the Stat Quest on Elastic Networks, we talked about how the ElasticNet combines the Lazo regression penalty, e-how with the Ridge regression penalty. And that you can control how much either penalty is included by adjusting Lambda sub-1 or Lambda sub-2. However, GLMNet does things just a little differently. Instead of two different Lambdas, GLMNet has a single Lambda, and another parameter called Alpha."
733,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,106.0,136.0,30.0," Alpha can be any value from zero to one. When Alpha equals zero, then the whole Lazo penalty goes to zero, and goes away. And we're left with just the Ridge regression penalty. And the whole thing reduces to Ridge regression, e-how. When Alpha equals one, then the whole Ridge penalty goes to zero, and goes away."
734,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,137.0,174.0,37.0," And we're left with just the Lazo regression penalty. And the whole thing reduces to Lazo regression, e-how. And when Alpha is between zero and one, we get a mixture of the two penalties that does a better job shrinking correlated variables than either Lazo or Ridge does on their own. Lambda controls how much of the penalty to apply to their regression. When Lambda equals zero, then the whole penalty goes away, and we're just doing standard least squares for linear regression, or maximum likelihood for logistic regression."
735,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,174.0,213.0,39.0," And when Lambda is greater than zero, then the elastic net penalty kicks in, and we start shrinking parameter estimates. Thus, when we use the GLMNet package to do elastic net regression, we will test different values for Lambda and Alpha. Now that we know how GLMNet works, let's do Ridge, Lazo, and ElasticNet regression, and R. Note, the following code is based on an example written by Josh Day. However, the modified code that I'm about to present can be found by following the link in the description below."
736,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,213.0,248.0,35.0," First, load the GLMNet library. Then set a seed for the random number generator so that you'll get the same results as me. Now let's make up a dataset that we can use to test out Ridge, Lazo, and ElasticNet regression. The made-up dataset will have n equals 1,000 samples, and p equals 5,000 parameters to estimate. However, only 15 of those parameters will help us predict the outcome."
737,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,248.0,290.0,42.0," The remaining 4,985 parameters will just be random noise. Now we create a matrix called X that is full of randomly generated data. The matrix has 1,000 rows since n equals 1,000, and 5,000 columns since p equals 5,000. And the values in the matrix come from a standard normal distribution with m equals 0 and standard deviation equals 1. And we'll need n times p, or 1,000 times 5,000 equals 5,000, value since our matrix has n rows and p columns."
738,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,290.0,336.0,46.0," Now we create a vector of values called y that we will try to predict with the data in X. This call to apply will return a vector of 1,000 values that are the sums of the first 15 columns in X since X has 1,000 rows. This is what isolates columns 1 through 15 from X, and this one specifies that we want to perform a function on each row of the data that we've isolated from X. And sum is the function we want to apply to each row. To summarize, this call to apply will return a vector of values that depend on the first 15 columns in X."
739,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,336.0,377.0,41.0," Once we have that vector of sums, we add a little bit of noise using the R norm function, which, in this case, returns 1,000 random values from a standard normal distribution. So this whole thing creates a vector called y that is dependent on the first 15 columns in X plus a little noise to make things interesting. Thus, X is a matrix of data that we will use ridge, lasso, and elastic net regression to predict the values in y. Bam! Now we need to divide the data into training and testing sets."
740,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,377.0,417.0,40.0," So we make a vector of indexes called train underscore rows that contains the row numbers of the rows that will be in the training set. The sample function randomly selects numbers between 1 and n, the number of rows in our data set. And it will select 0.66 times n row numbers. In other words, 2-thirds of the data will be in the training set. Now that we have the indexes for the rows in the training set in train underscore rows, we can make a new matrix X dot train that just contains the training data."
741,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,417.0,447.0,30.0, And we can make a testing set X dot test that contains the remaining rows. This is done by putting a negative sign in front of train underscore rows when we select rows from X. Now we select the training values in y and save them in y dot train. And we select the testing values in y and save them in y dot test. Hurray! We've created our training and testing data sets.
742,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,447.0,478.0,31.0," We'll apply ridge, lasso, and elastic net regression separately to these data sets so that we can see how it's done and see which method works best. We'll start with ridge regression. The first thing we need to do is fit a model to the training data. We do this with the CV dot GLM net function. The CV part means we want to use cross validation to obtain the optimal values for lambda."
743,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,478.0,511.0,33.0," By default, CV dot GLM net uses 10 fold cross validation. The first two parameters specify the training sets. In this case, we want to use X train to predict Y train. Note, unlike the LM or GLM functions, CV dot GLM net does not accept a formula notation. X and Y must be passed in separately. Type dot measure is how the cross validation will be evaluated."
744,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,512.0,545.0,33.0," And it is set to MSE, which stands for mean squared error. Mean squared error is just the sum of the squared residuals divided by the sample size. Note, if we are applying elastic net regression to logistic regression, then we would set this to DVNs. Since we are starting with ridge regression, we set alpha to 0. Lastly, we set family to Gaussian. This tells GLM net that we are doing linear regression."
745,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,545.0,589.0,44.0," Note, if we were doing logistic regression, we would set this to binomial. Altogether, this called a CV dot GLM net will fit a linear regression with a ridge regression penalty using 10 fold cross validation to find optimal values for lambda. In the fit model, along with optimal values for lambda, is saved as alpha0.fit, which will help us remember that we set alpha to 0 for ridge regression. Now we will use the predict function to apply alpha0.fit to the testing data. The first parameter is a fitted model. In this case, it's alpha0.fit."
746,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,589.0,632.0,43.0," S, which I think stands for size, as in the size of the penalty, is set to one of the optimal values for lambda stored in alpha0.fit. In this example, we are setting S to lambda dot 1SE. Lambda dot 1SE is the value for lambda stored in alpha0.fit that resulted in the simplest model. IE, the model with the fewest nonzero parameters, and was within one standard error of the lambda that had the smallest sum. Note, alternatively, we could set S to lambda dot men, which would be the lambda that resulted in the smallest sum."
747,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,633.0,678.0,45.0," However, in this example, we would use lambda dot 1SE because, in a statistical sense, it is indistinguishable from lambda dot men, but it results in a model with fewer parameters. Wait a minute. I thought only last so in a last-to-knit regression could eliminate parameters. What's going on? Since we will compare ridge to lasso and elastic net regression, we will use lambda dot 1SE for all three cases to be consistent. Lastly, we set new X to the testing data set X dot test. And we save the predicted values as alpha0 dot predicted."
748,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,678.0,713.0,35.0," Now we calculate the mean squared error of the difference between the true values stored in y dot test and the predicted values stored in alpha0 dot predicted. And we get 14.47 something something. Bam! Now let's try lasso regression with the same training and testing data sets. Just like before, we call CV dot GLN net to fit a linear regression using 10fold cross validation to determine optimal values for lambda."
749,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,713.0,747.0,34.0," Only this time, we set alpha to 1. And we will store the model and the optimal values for lambda in alpha1 dot fit to remind us that we set alpha to 1. Then we call the predict function just like before. Only this time, we pass an alpha1 dot fit and save the results as alpha1 dot predicted. Then we calculate the mean squared error and we get 1.19 something something."
750,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,747.0,782.0,35.0," 1.19 something something is way smaller than 14.47 something something, so lasso regression is much better with this data than ridge regression. Double bam! Now let's see how well elastic net regression which combines both ridge and lasso penalties performs. Just like before, we call CV dot GLN net to determine optimal values for lambda. Only this time we set alpha to 0.5."
751,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,782.0,816.0,34.0," And we store the model and the optimal values for lambda in alpha 0.5 dot fit to remind us that we set alpha to 0.5. Then we call the predict function with alpha 0.5 dot fit and calculate the mean squared error. And we get 1.24 something something. This is slightly larger than the 1.19 something something we got with lasso regression, so far lasso wins. EHA!"
752,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,816.0,850.0,34.0," But to really know if lasso wins, we need to try a lot of different values for alpha. To try a bunch of values for alpha, we'll start by making an empty list called list of fits that will store a bunch of elastic net regression fits. Then we use a for loop to try different values for alpha. In this for loop, I will be integer values from 0 to 10. First, we paste together a name for the elastic net fit that we are going to create."
753,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,850.0,879.0,29.0," For example, when i equals 0, then fit dot name will be alpha 0, because alpha will be pasted to 0 divided by 10, which equals 0. Here's where we create the elastic net fit using the CV dot GLM net function. Everything is the same as before, except where we set alpha. When i equals 0, then alpha will be 0 and result in ridge regression. So, if we do this, we can do this."
754,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,879.0,897.0,18.0," So, if we do this, we can do this. So, if we do this, we can do this. So, if we do this, we can do this. When i equals 0, then alpha will be 0 and result in ridge regression. And when i equals 1, then alpha will be 0.1."
755,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,897.0,935.0,38.0," It's et cetera, et cetera, until i equals 10 and alpha equals 1, resulting in elastic regression. Each fit will be stored in list of fits under the name we stored in fit dot name. Now we are ready to calculate the mean squared errors for each fit with the testing data set. We'll start by creating an empty data frame called results that will store the mean squared errors and a few other things. Then we'll use another for loop to predict values using the testing data set to calculate the mean squared errors."
756,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,935.0,971.0,36.0," Just like before, the for loop goes from 0 to 10. And just like before, we'll create a variable called fit dot name that contains the name of the elastic net regression fit. However, this time we'll use the predict function to predict values with x dot test, the testing data. We use the list of fits list and fit dot name to pass a specific fit to the predict function. And to pass the lambda dot 1SE value for s."
757,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,971.0,1002.0,31.0," The predicted values are stored in a variable called predicted. Then used to calculate the mean squared error for the fit. Then we store the value for alpha, the mean squared error, and the name of the fit in a temporary data frame called temp. Then we use the r-bind function to append temp to the bottom row of the results data frame. After we run the for loop, we can print out the results."
758,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,1002.0,1031.0,29.0," The first column has the values for alpha ranging from 0 to 1. The second column has the mean squared errors. Note, these are slightly different from what we got before because the parameter values prior to regularization in a variable. Regularization in optimization are randomly initialized. Thus, this is another good reason to use set dot c to ensure consistent results."
759,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,1031.0,1054.0,23.0," And in the last column, we have the name of the fit. The fit where alpha equals 1 is still the best, so last regression is the best method to use with this data. Triple BAM Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
760,"Ridge, Lasso and Elastic-Net Regression in R",https://www.youtube.com/watch?v=ctmNq7FgbvI,ctmNq7FgbvI,1054.0,1064.0,10.0," And if you want to support stat quest, well, consider buying one or two of my original songs. Alright, until next time, quest on."
761,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,0.0,34.0,34.0," Stack West breaks it down into bite size pieces who ray. Hello, I'm Josh Starmer and welcome to Stack Quest. In this Stack Quest, we're going to go through Principal Component Analysis, PCA, one step at a time using singular value decomposition, SVD. You'll learn about what PCA does, how it does it, and how to use it to get deeper insight into your data. Let's start with a simple data set."
762,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,34.0,69.0,35.0," We've measured the transcription of two genes, gene 1 and gene 2, in six different mice. Note, if you're not in a mice and genes, think of the mice as individual samples. In the genes, as variables that we measure for each sample. For example, the samples could be students in high school, and the variables could be test scores in math and reading. Or the samples could be businesses, and the variables could be market capitalization and the number of employees."
763,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,69.0,111.0,42.0," Okay, now we're back to mice and genes, because I'm a geneticist and I work in a genetics department. If we only measure one gene, we can plot the data on a number line. mice 1, 2, and 3 have relatively high values, and mice 4, 5, and 6 have relatively low values. Even though it's a simple graph, it shows us that mice 1, 2, and 3 are more similar to each other than they are to mice 4, 5, and 6. If we measured two genes, then we can plot the data on a two-dimensional xy graph."
764,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,111.0,150.0,39.0," gene 1 is the x-axis, and spans one of the two dimensions in this graph. gene 2 is the y-axis, and spans the other dimension. We can see that mice 1, 2, and 3 cluster on the right side, and mice 4, 5, and 6 cluster on the lower left hand side. If we measured three genes, we would add another x-axis to the graph and make it look 3D, i.e., 3-dimensional. The smaller dots have larger values for gene 3 and are further away."
765,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,150.0,176.0,26.0," The larger dots have smaller values for gene 3 and are closer. If we measured four genes, however, we can no longer plot the data. Four genes require four dimensions. 1, 1. So we're going to talk about how PCA can take four or more gene measurements, and thus four or more dimensions of data,"
766,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,176.0,212.0,36.0," and make a two-dimensional PCA plot. This plot will show us that similar mice cluster together. We'll also talk about how PCA can tell us which gene or variable is the most valuable for clustering the data. For example, PCA might tell us that gene 3 is responsible for separating samples along the x-axis. Lastly, we'll talk about how PCA can tell us how accurate the 2D graph is."
767,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,212.0,242.0,30.0," To understand what PCA does and how it works, let's go back to the data set that only had two genes. We'll start by plotting the data. Then we'll calculate the average measurement for gene 1, and the average measurement for gene 2. With the average values, we can calculate the center of the data. From this point on, we'll focus on what happens in the graph."
768,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,242.0,273.0,31.0," We no longer need the original data. Now we'll shift the data so that the center is on top of the origin in the graph. Note, shifting the data did not change how the data points are positioned relative to each other. This point is still the highest one, and this is still the rightmost point, etc. Now that the data are centered on the origin, we can try to fit a line to it."
769,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,273.0,303.0,30.0," To do this, we start by drawing a random line that goes through the origin. Then we rotate the line until it fits the data as well as it can, given that it has to go through the origin. Ultimately, this line fits best. But I'm getting ahead of myself. First, we need to talk about how PCA decides if a fit is good or not."
770,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,303.0,346.0,43.0," So let's go back to the original, random line that goes through the origin. To quantify how good this line fits the data, PCA projects the data onto it. And then it can either measure the distances from the data to the line and try to find the line that minimizes those distances. Or it can try to find the line that maximizes the distances from the projected points to the origin. If those options don't seem equivalent to you, we can build intuition by looking at how these distances shrink when the line fits better."
771,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,346.0,385.0,39.0," While these distances get larger when the line fits better. Now, to understand what is going on in a mathematical way, let's just consider one data point. This point is fixed and so is its distance from the origin. In other words, the distance from the point to the origin doesn't change when the red dotted line rotates. When we project the point onto the line, we get a right angle between the black dotted line and the red dotted line."
772,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,385.0,434.0,49.0," That means that if we label the sides like this, a, b, and c, then we can use the Pythagorean theorem, to show how b and c are inversely related. Since a and thus a squared doesn't change, if b gets bigger, then c must get smaller. Likewise, if c gets bigger, then b must get smaller. Thus, pca can either minimize the distance to the line, or maximize the distance from the projected point to the origin."
773,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,434.0,475.0,41.0," The reason I'm making such a fuss about this is that, intuitively, it makes sense to minimize b and the distance from the point to the line. But it's actually easier to calculate c, the distance from the projected point to the origin. So pca finds the best fitting line by maximizing the sum of the squared distances from the projected points to the origin. So, for this line, pca projects the data onto it, and then measures the distance from this point to the origin. Let's call it d sub 1."
774,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,475.0,504.0,29.0," Note, I'm going to keep track of the distances we measure up here. And then pca measures the distance from this point to the origin. We'll call that d2, then it measures d3, d4, d5, and d6. Here are all six distances that we measured. The next thing we do is square all of them."
775,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,504.0,541.0,37.0," The distances are squared so that negative values don't cancel out positive values. Then we sum up all these squared distances. And that equals the sum of the squared distances. For short, we'll call this SS distances, or sum of squared distances. Now we rotate the line, project the data onto the line, and then sum up the square distances from the projected points to the origin."
776,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,541.0,571.0,30.0," And we repeat until we end up with the line with the largest sum of square distances between the projected points and the origin. Ultimately, we end up with this line. It has the largest sum of squared distances. This line is called principle component 1, or pc1 for short. pc1 has a slope of 0.25."
777,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,571.0,601.0,30.0," In other words, for every four units that we go out along the gene 1 access, we go up 1 unit along the gene 2 access. That means that the data are mostly spread out along the gene 1 access. And only a little bit spread out along the gene 2 access. One way to think about pc1 is in terms of a cocktail recipe."
778,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,601.0,635.0,34.0," To make pc1, mix four parts gene 1 with one part gene 2. Pour over ice and serve. The ratio of gene 1 to gene 2 tells you that gene 1 is more important when it comes to describing how the data are spread out. Oh no, terminology alert. Mathematicians call this cocktail recipe a linear combination of genes 1 and 2."
779,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,635.0,666.0,31.0," I mentioned this because when someone says pc1 is a linear combination of variables, this is what they're talking about. It's no big deal. Pc1, going over 4 and up 1, gets us to this point. We can solve for the length of the red line using the Pythagorean theorem. The old A squared equals B squared plus C squared."
780,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,666.0,705.0,39.0," Plugging in the numbers gives us A equals 4.12. So the length of the red line is 4.12. When you do pcA with SVD, the recipe for pc1 is scaled so that this length equals 1. All we have to do to scale the triangle so that the red line is 1 unit long is to divide it side by 4.12. For those of you keeping score, here's the math worked out that shows that all we need to do is divide all three sides by 4.12."
781,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,705.0,728.0,23.0," Here are the scaled values. The new values change our recipe. But the ratio is the same. We still use 4 times as much gene 1 as gene 2. So now we're back to looking at the data, the best fitting line, and the unit vector that we just calculated."
782,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,729.0,772.0,43.0," Oh no, another terminology alert. This 1 unit long vector consisting of 0.97 parts gene 1 and 0.242 parts gene 2 is called the singular vector or the eigenvector for pc1. And the proportions of each gene are called loading scores. Also, while I'm at it, pcA calls the average of the sums of the square distances for the best fit line the eigenvalue for pc1. And the square root of the sums of the square distances is called the singular value for pc1."
783,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,772.0,818.0,46.0," Bam, that's a lot of terminology. Now that we've got pc1 all figured out, let's work on pc2. Because this is only a two-dimensional graph, pc2 is simply the line through the origin that is perpendicular to pc1 without any further optimization that has to be done. And this means that the recipe for pc2 is negative 1 parts gene 1 to 4 parts gene 2. If we scale everything so that we get a unit vector, the recipe is negative 0.242 parts gene 1 and 0.97 parts gene 2."
784,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,818.0,855.0,37.0," This is the singular vector for pc2 or the eigenvector for pc2. These are the loading scores for pc2. They tell us that in terms of how the values are projected on the pc2 gene 2 is 4 times as important as gene 1. Lastly, the eigenvalue for pc2 is the average of the sum of the squares of the distances between the projected points and the origin. RAY! We've worked out pc1 and pc2."
785,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,855.0,887.0,32.0," To draw the final pc8 plot, we simply rotate everything so that pc1 is horizontal. Then we use the projected points to find where the samples go in the pc8 plot. For example, these projected points correspond to sample 6. So, sample 6 goes here. sample 2 goes here and sample 1 goes here."
786,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,887.0,912.0,25.0, etc. Double ban. That's how pc8 is done using singular value decomposition. Okay. One last thing before we dive into a slightly more complicated example. Remember the eigenvalues? We got those by projecting the data onto the principal components.
787,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,912.0,957.0,45.0," Measuring the distances to the origin, then squaring and adding them together. Well, if you're familiar with the equation for variation, you will notice that eigenvalues are just measures of variation. The sake of this example, imagine that the variation for pc1 equals 15 and the variation for pc2 equals 3. That means that the total variation around both pc's is 15 plus 3 equals 18. And that means pc1 accounts for 15 divided by 18 equals 0.83 or 83% of the total variation around the pc's."
788,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,957.0,990.0,33.0," pc2 accounts for 3 divided by 18 equals 17% of the total variation around the pc's. Oh no, another terminology alert. A screen plot is a graphical representation of the percentages of variation that each pc accounts for. We'll talk more about screen plots later. Bam! Okay. Now let's quickly go through a slightly more complicated example."
789,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,991.0,1023.0,32.0," pcA with three variables, in this case that means three genes is pretty much the same as two variables. You center the data, you then find the best fitting line that goes through the origin. Just like before, the best fitting line is pc1. But the recipe for pc1 now has three ingredients. In this case, gene 3 is the most important ingredient for pc1."
790,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1023.0,1065.0,42.0," You then find pc2, the next best fitting line given that it goes through the origin and is perpendicular to pc1. Here's the recipe for pc2. In this case, gene 1 is the most important ingredient for pc2. Lastly, we find pc3, the best fitting line that goes through the origin and is perpendicular to pc1 and pc2. If we had more genes, we just keep on finding more and more principal components by adding perpendicular lines and rotating them."
791,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1065.0,1107.0,42.0," In theory, there is one per gene or variable, but in practice, the number of pcs is either the number of variables or the number of samples, whichever is smaller. If this is confusing, don't sweat it. It's not super important, and I'm going to make a separate video on this topic in the next week. Once you have all the principal components figured out, you can use the eigenvalues, i.e. the sums of squares of the distances to determine the proportion of variation that each pc accounts for. In this case, pc1 accounts for 79% of the variation."
792,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1107.0,1151.0,44.0," And pc3 accounts for 6% of the variation. Here's the screen plot. pc1 and pc2 account for the vast majority of the variation. That means that a 2d graph, using just pc1 and pc2, would be a good approximation of this 3d graph, since it would account for 94% of the value. To convert the 3d graph into a 2 dimensional pcA graph, we just strip away everything but the data and pc1 and pc2."
793,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1151.0,1182.0,31.0," Then project the samples onto pc1 and pc2. Then we rotate so that pc1 is horizontal and pc2 is vertical. This just makes it easier to look at. Since these projected points correspond to sample 4, this is where sample 4 goes on our new pcA plot. Etc. Etc. Etc. Etc."
794,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1182.0,1212.0,30.0," Double bam. To review, we started with an awkward 3d graph that was kind of hard to read. Then we calculated the principal components. Then, with the eigenvalues for pc1 and pc2, we determined that a 2d graph would still be very informative. Lastly, we used pc1 and pc2 to draw 2 dimensional graph with the data."
795,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1213.0,1247.0,34.0," If we measured 4 genes per mouse, we would not be able to draw 4 dimensional graph of the data. What? What? But that doesn't stop us from doing the pcA math, which doesn't care if we can draw a picture of it or not, and looking at the screen plot. In this case, pc1 and pc2 account for 90% of the variation, so we can just use those to draw 2 dimensional pcA graph. So we project the samples onto the first 2pcs."
796,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1247.0,1277.0,30.0," These 2 projected points correspond to sample 2, so sample 2 goes here. Bam! Note, if the screen plot looked like this, where pc3 and pc4 account for a substantial amount of variation, then just using the first 2pcs would not create a very accurate representation of the data. Wow!"
797,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1277.0,1301.0,24.0," However, even a noisy pcA plot like this can be used to identify clusters of data. These samples are still more similar to each other than they are to the other samples. Little Bam! Hurray! We've made it to the end of another exciting step quest. If you like this step quest and want to see more, please subscribe."
798,"StatQuest: Principal Component Analysis (PCA), Step-by-Step",https://www.youtube.com/watch?v=FgakZw6K1QQ,FgakZw6K1QQ,1301.0,1317.0,16.0," And if you want to support step quest, please consider buying one or two of my original songs. The link to my bandcamp page is in the lower right corner, and in the description below. All right, until next time, quest on!"
799,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,0.0,21.0,21.0," StackQuist is the best if you don't think so. Then we have different opinions. Hello, I'm Josh Starmer and welcome to StackQuist. Today we're going to be talking about the main ideas behind principle component analysis, and we're going to cover those concepts in five minutes."
800,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,21.0,40.0,19.0," If you want more details than you get here, be sure to check out my other PCA video. Let's say we had some normal cells. If you're not a biologist, imagine that these could be people, or cars, or cities, etc. They could be anything."
801,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,40.0,62.0,22.0," Even though they look the same, we suspect that there are differences. These might be one type of cell, or one type of person, or car, or city, etc. These might be another type of cell, and lastly, these might be a third type of cell."
802,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,62.0,83.0,21.0," Unfortunately, we can't observe differences from the outside. So we sequence the messenger RNA in each cell to identify which genes are active. This tells us what the cell is doing. If they were people, we could measure their weight, blood pressure, reading, level, etc."
803,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,83.0,113.0,30.0," Okay, here's the data. Each column shows how much each gene is transcribed in each cell. For now, let's imagine there are only two cells. If we just have two cells, then we can plot the measurements for each gene. This gene, gene 1, is highly transcribed in cell 1, and lowly transcribed in cell 2."
804,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,113.0,139.0,26.0," And this gene, gene 9, is lowly transcribed in cell 1, and highly transcribed in cell 2. In general, cell 1 and cell 2 have an inverse correlation. This means that they are probably two different types of cells, since they are using different genes. Now let's imagine there are three cells."
805,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,139.0,162.0,23.0," We've already seen how we can plot the first two cells to see how closely they are related. Now we can also compare cell 1 to cell 3. Cell 1 and cell 3 are positively correlated, suggesting they are doing similar things. Lastly, we can also compare cell 2 to cell 3."
806,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,162.0,187.0,25.0," The negative correlation suggests that cell 2 is doing something different from cell 3. Alternatively, we could try to plot all three cells at once on a three-dimensional graph. Cell 1 could be the vertical axis. Cell 2 could be the horizontal axis. And cell 3 could be depth."
807,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,187.0,216.0,29.0, We could then rotate this graph around to see how the cells are related to each other. But what do we do when we have four or more cells? Draw tons and tons of two cell plots and try to make sense of them all? Or draw some crazy graph that has an axis for each cell and makes our brain explode? No. Both of those options are just plain silly.
808,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,216.0,245.0,29.0," Instead, we draw principal component analysis or PCA plot. A PCA plot converts the correlations or lack thereof among the cells into a 2D graph. Cells that are highly correlated cluster together. This cluster of cells are highly correlated with each other. So are these and so are these."
809,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,245.0,272.0,27.0," To make the clusters easier to see, we can color code them. Once we've identified the clusters in a PCA plot, we can go back to the original cells and see that they represent three different types of cells doing three different types of things with their genes. Bam! Here's one last main idea about how to interpret PCA plots."
810,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,272.0,308.0,36.0," The axes are ranked in order of importance. Differences among the first principal component access, PC1, are more important than differences along the second principal component access, PC2. If the plot looked like this, where the distance between these two clusters is about the same as the distance between these two clusters, then these two clusters are more different from each other than these two clusters."
811,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,308.0,344.0,36.0," Before we go, you should know that PCA is just one way to make sense of this type of data. There are lots of other methods that are variations on this theme of dimension reduction. These methods include heat maps, T-snave plots, and multiple dimension scaling plots. The good news is that I've got stat quests for all of these, so you can check those out if you want to learn more. Note, if the concept of dimension reduction is freaking you out, check out the original stat quest on PCA."
812,StatQuest: PCA main ideas in only 5 minutes!!!,https://www.youtube.com/watch?v=HMOI_lkzW08,HMOI_lkzW08,344.0,364.0,20.0," I take it nice and slow so it's clearly explained. Alright, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe. And if you have any ideas for additional stat quests, well, put them in the comments below. Until next time, quest on."
813,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,0.0,20.96,20.96," I've got a few PCA tips Just For you Hello, I'm Josh Starman. Welcome to StecQuest. Today we're gonna be talking about PCA and I'm gonna give you a few practical tips"
814,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,22.08,45.64,23.56," Specifically we're going to talk about one scaling your data 2, centering your data and 3, how many principal components you should expect to get Note this is a follow-up to my video Principal component analysis PCA step-by-step So make sure you've seen that video first or at least already know how PCA works"
815,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,47.56,77.4,29.840000000000003, Practical tip number one make sure the variables are on the same scale and if not scale them Here we have math and reading scores for a bunch of students Let's plot the data Math scores are from zero to 100 and they are spread out between zero and 100 in the graph In contrast reading scores are only from zero to 10 and
816,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,78.44,102.4,23.960000000000008, They are all crammed between zero and 10 on the graph If we centered the data and did PCA on it We'd get this recipe for PC1 to make PC1 Makes 0.99 parts math with zero point one part reading It suggests that math is 10 times better than reading for capturing variation in aptitude
817,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,103.2,137.6,34.39999999999999, But this is only because the math scores are on a scale 10 times larger than the scale for reading scores If we divided the math scores by 10 and Replauded and then centered the data and did PCA on it We'd get this recipe for PC1 to make PC1 makes 0.77 parts math with 0.77 parts reading This suggests that reading and math are equally good capturing variation in aptitude
818,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,138.56,160.32,21.75999999999999, The moral of this story is that you need to make sure the scales for each variable in this case Math and reading scores are roughly equivalent Otherwise you will be biased towards one of them The standard practice is to divide each variable by its standard deviation Thus if a variable has a wide range
819,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,160.48,187.24,26.76000000000002, It will have a large standard deviation and dividing by it will scale the values a lot If a variable has a narrow range it will have a small standard deviation and scaling will be minimal Practical tip number two make sure your data is centered The very first step Centering the data is an important one but not every PCA program does this by default
820,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,188.56,221.88,33.32000000000002," If you do PCA using SVD without centering it will still try to fit a line to the Data that goes through the origin and your PCs will not be what you expect So double check that the PCA program you are using centers the data or center it yourself Practical tip number three how many principal components can you expect to find? In the first example, we had math and reading scores from a bunch of students"
821,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,222.52,246.76,24.24000000000001, We then plotted the data on a two-dimensional graph We then centered the data and then we found the best fitting line that goes through the origin This is PC1 The second PC PC2 was the line perpendicular to PC1 Then we just moved on without asking if there were any more principal components
822,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,247.72,280.56,32.84, So now let's ask the question is there a third principal component? To find a third PC we'd need to find a line perpendicular to both PC1 and PC2 And in two dimensions that's not possible so the answer is no To see why it's not possible to draw a line perpendicular to both PC1 and PC2 we get out of third line And rotate it until it is perpendicular to PC1 and PC2
823,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,281.8,308.0,26.19999999999999," This position is perpendicular to PC2, but not PC1 So we keep rotating This position is perpendicular to PC1, but not PC2 Thus when we measure two things math and reading in this case Per sample i.e. per student at most we can have two principal components"
824,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,309.64,334.08,24.44, Now imagine that math and reading scores are 100% correlated Then we centered the data Then we found the best fitting line principal component one Technically speaking we could find a line perpendicular to PC1 But if we projected the data onto this line it would all just go to the origin
825,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,335.08,364.2,29.120000000000005, This means that the eigenvalue for the new line the sum of squares of the distances between the points projected onto the line and the origin would be zero This means that PC1 accounts for 100% of the variation and the new line accounts for zero percent Thus for all practical purposes PC1 is the only one that matters Likewise if we only had two students and
826,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,364.92,393.24,28.32000000000005, Then we centered the data and then we found the best fitting line PC1 Then just like before we could find a line that is perpendicular to PC1 But the eigenvalue would be zero So just like before there's really only one principal component worth talking about Another perhaps simpler way to think about this is that two points to find a line
827,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,394.84,425.12,30.279999999999973, So these two points can only define a single line In order to define a plane something with two axes we need at least three points Now imagine we had two students and three-teeth scores math reading and gymnastics We can center the data and just like before since we only have two points and two points to find a line We only have one PC
828,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,426.48,447.16,20.680000000000007, Now imagine we had three students and We remember that three points define a plane and A plane has only two axes Then we would predict that there would only be two principal components So we can center the data
829,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,448.2,471.4,23.19999999999999, Find the line through the origin that fits best PC1 Then find the line perpendicular to PC1 that fits best and That's it since a third line would have an eigenvalue equal to zero. There's no PC3 In summary Technically there is a PC for each variable in the data set
830,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,471.84,495.46,23.620000000000005," However, if there are fewer samples than variables Then the number of samples puts an upper bound on the number of PCs with eigenvalues greater than zero Hurray we've made it to the end of another exciting stat quest If you like this stat quest and want to see more please subscribe and if you'd like to support stat quest Well consider buying one or two of my original songs"
831,StatQuest: PCA - Practical Tips,https://www.youtube.com/watch?v=oRvgq966yZg,oRvgq966yZg,496.02,498.66,2.6399999999999864," Okay, until next time quest on"
832,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,0.0,19.0,19.0," Stack West is moving around. Watch it go. Hello, I'm Josh Stormer and welcome to Stack West. Stack West is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill."
833,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,19.0,41.0,22.0," Today we're going to be talking about how to do PCA and R. It's going to be clearly explained. Note, in the description below this video, I've provided a link to the code that I use. So, if you want to do it yourself, all you got to do is follow that link, copy and paste, and you're good to go. Here's what we're going to be talking about."
834,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,41.0,76.0,35.0," How to use the per comp function to do PCA, how to draw PCA plot using base graphics and GG plot 2. How to determine how much variation each principal component accounts for. And lastly, how to examine the loading scores to determine what variables have the largest effects on the graph. First, let's generate a fake data set that we can use in the demonstration. Note, if the details in this section don't make a whole lot of sense because we're talking about genes and read counts, don't worry too much."
835,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,76.0,100.0,24.0," The important thing is that we have some data, and you might have your own data to work with. We will make a matrix of data with 10 samples where we measured 100 genes in each sample. This is where we name the samples. The first five samples will be WT or wild type samples. The WT samples are normal every day samples."
836,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,100.0,128.0,28.0," The last five samples will be KO or knockout samples. These are samples that are missing a gene because we knocked it out. This is where we name the genes. Usually you'd have things like SOx9 and UTX, but since this is a fake data set, we have gene1, gene2, and dot dot dot to gene100. This is where we give the fake genes fake read counts."
837,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,128.0,156.0,28.0," Sticolors for details will recognize that I'm using the Poisson distribution instead of the negative binomial distribution. Like I said, these are fake reads for fake genes, so don't worry about it too much. And here are the first six rows in our data matrix. Note, this samples are columns and the genes are rows. Now that we have our data, we call Percom to do the PCA on our data."
838,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,156.0,197.0,41.0," The goal is to draw a graph that shows how the samples are related or not related to each other. Note, by default, Percom expects the samples to be rows and the genes to be columns. Since the samples and our data matrix are columns and the genes variables are rows, we have to transpose the matrix using the T function. If we don't transpose the matrix, we will ultimately get a graph that shows how the genes are related to each other, and this isn't what we want in this case. Percom returns three things, X, S, Dev and Rotation."
839,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,197.0,229.0,32.0," I'll talk about all three things as we use them. We'll start with X. X contains the principal components, PCs, for drawing a graph. Here we are using the first two columns and X to draw a 2D plot that uses the first two principal components. Remember, since there are 10 samples, there are 10 principal components. The first principal component accounts for the most variation in the original data, the gene expression across all 10 samples."
840,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,229.0,257.0,28.0," The second principal component accounts for the second most variation and so on. To plot a 2D PCA graph, we usually use the first two principal components. However, sometimes we use principal component 2 in principal component 3. Here's our graph using the base graphics function plot. PC1 is on the X-axis because PC1 is the first column in X."
841,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,257.0,302.0,45.0," PC2 is on the Y-axis, it's the second column in X. Five of the samples are on one side of the graph and the other five samples are on the other side of the graph. To get a sense of how meaningful these clusters are, let's see how much variation in the original data, principal component one accounts for. To do this, we use the square of S-dev, which stands for standard deviation, to calculate how much variation in the original data, each principal component accounts for. Since the percentage of variation that each principal component accounts for is way more interesting than the actual value, we calculate the percentages."
842,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,302.0,331.0,29.0," And plotting the percentages is easy with the bar plot function. Principal component one accounts for almost all of the variation in the data. This means that there is a big difference between these two clusters. We can use GG plot 2 to make a fancy PCA plot that looks nice and also provides us with tons of information. First, format the data the way GG plot 2 likes it."
843,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,331.0,357.0,26.0, We make a data frame where one column has the sample IDs. Two columns are for the X and Y coordinates for each sample. Here's what the data frame looks like. We have one row per sample. Each row has a sample ID and XY coordinates for that sample. Here's the call to GG plot. I'll go through this one line at a time.
844,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,357.0,405.0,48.0," But first, let's look at the graph at draws. The X-axis tells us what percentage of the variation in the original data that PC1 accounts for. Now the samples are labels, so we know which ones are on the left and the right. In the first part of our GG plot function call, we pass in the PCA dot data data frame and tell GG plot which columns contain the X and Y coordinates and which column has the sample labels. Then we use GOM text to tell GG plot to plot the labels rather than dots or some other shape."
845,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,405.0,435.0,30.0," Then we use X-labe and Y-labe to add X and Y-axis labels. Here I'm using the paste function to combine the percentage of variation with some text to make the labels look nice. Calling theme BW makes the graphs background white. This is optional, but I prefer the way this looks. Lastly, we add a title to the graph using GG title. Bam!"
846,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,435.0,478.0,43.0," Lastly, let's look at how to use loading scores to determine which genes have the largest effect on where samples are plotted in the PCA plot. The per-comp function calls the loading scores rotation. There are loading scores for each principal component. Here I'm just going to look at the loading scores for principal component 1 since it accounts for 92% of the variation in the data. Gains or variables that push the samples to the left side of the graph will have large negative values and Gains or variables that push the samples to the right will have large positive values."
847,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,478.0,515.0,37.0," Since we're interested in both sets of genes or variables, we use the absolute value function to sort based on the numbers magnitude rather than from high to low. Now we sort the magnitudes of the loading scores from high to low. Now we get the names for the top 10 genes with the largest loading score magnitudes. Lastly, we can see which of these genes have positive loading scores. These push the KO samples to the right side of the graph, and then we see which genes have negative loading scores."
848,StatQuest: PCA in R,https://www.youtube.com/watch?v=0Jp4gsfOLMs,0Jp4gsfOLMs,515.0,537.0,22.0," These push the wild type samples to the left side of the graph. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe. And if you have any suggestions for future stat quests, well, put them in the comments below. Until next time, quest on."
849,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,0.0,24.0,24.0," Step Quest is awesome, comes out every Monday. Hello, I'm Josh Starmer and welcome to Step Quest. Today we're going to be talking about how to do PCA and Python. It's going to be clearly explained. However, before I get started, I need to have a big shout out for Chris Yoga Zinski."
850,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,24.0,52.0,28.0," He wrote the code that I'm going to be talking about, and he's the reason why the Step Quest exists to begin with. Here's what we're going to talk about. First, we'll talk about how to make up some data that we can apply PCA to. Then we'll talk about how to use the PCA function from scikit-learn to do PCA. We'll also talk about how to determine how much variation each principal component accounts for."
851,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,53.0,81.0,28.0," We'll talk about how to draw fancy PCA graph using matte plot lib. And lastly, we'll talk about how to examine the loading scores to determine what variables have the largest effect on the graph. The first thing we do is import the Pandas package. Pandas, sure for panel data, makes it easy to manipulate data in Python. Then we import the Numpy package."
852,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,81.0,107.0,26.0," Numpy will allow us to generate random numbers and do other mathy things. The random package will be useful for generating an example data set. If you're working with real data, you won't need this package. Here, we are importing the PCA function from scikit-learn. Note, just like R, there are a few Python PCA functions to choose from."
853,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,107.0,140.0,33.0," The one in scikit-learn is the most commonly used that I know of. The pre-processing package from scikit-learn gives us functions for scaling the data before performing PCA. Lastly, we'll import matte plot lib so we can draw some fancy graphs. Python, being a general purpose programming language, doesn't have built-in support for tables of data, random number generation, or graphing like R, so we import all the stuff we need."
854,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,140.0,164.0,24.0," It's no big deal. Now that we've imported all the packages and functions we need, let's generate a sample data set. The first thing we do is generate an array of 100 gene names. Since this is just an example data set, our gene names are super boring. Gene1, Gene2, etc."
855,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,164.0,190.0,26.0," Now we create arrays of sample names. We have five wild type or WT samples and five knockout or KO samples. If you don't know what wild type or knockout samples are, don't worry about it. Just know that there's two different types of samples. Note, the range 1,6 function generates values 1,2,3,4 and 5."
856,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,190.0,228.0,38.0," In other words, it generates all integer values from 1 up to the value less than 6. Now we create a pandas data frame to store the made up data. The stars unpack the WT and KO arrays so that the column names are a single array that looks like this. Without the stars, we would create an array of two arrays, and that wouldn't create 12 columns like we want. The gene names are used for the index, which means that they are the equivalent of row names."
857,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,228.0,255.0,27.0," This is where we finally create the random data. For each gene in the index, i.e. Gene1, Gene2, dot, dot, all the way to Gene100, we create five values for the WT samples and five values for the KO samples. The made up data comes from two Poisson distributions. One for the WT samples and one for the KO samples."
858,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,255.0,284.0,29.0," Note, RNA sequencing efficientados will recognize that the Poisson distribution isn't the right one to use for the same. For the right one to use for RNA sequencing data, no big deal is it's just a made up data set so don't worry about it. For each gene, we select a new mean for the Poisson distribution. The means can vary between 10 and 1000. The head method for our data frame returns the first five rows of data."
859,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,284.0,315.0,31.0," This is useful for verifying that the data looked the way we expect they should. The shape attribute returns the dimensions of our data matrix. In this case, we get 100 comma 10, 100 genes by 10 total samples. Now that we've created a sample data set, let's do PCA. Actually, before we do PCA, we have to center and scale the data."
860,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,316.0,344.0,28.0," After centering, the average value for each gene will be zero. And after scaling, the standard deviation for the values for each gene will be one. Notice that we are passing in the transpose of our data. The scale function expects the samples to be in rows instead of columns. Note, we use samples as columns in this example because that is often how genomic data is stored."
861,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,344.0,366.0,22.0," If you have other data, you can store it however is easiest for you. There's no requirement that the samples be rows or columns. Just be aware that if it is columns, you'll need to transpose it before analysis. One other note. This is just one way to use scikit-learn to center and scale the data so that the means for each"
862,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,366.0,395.0,29.0," gene are zero and the standard deviations for each gene are one. Alternatively, we could have used standard scalar fit underscore transform. The second method is more commonly used for machine learning and that's what scikit-learn was designed to do. That's why it's available. One last note about scaling with scikit-learn versus using scale or per comp in R to do the scaling."
863,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,396.0,438.0,42.0," In scikit-learn, variation is calculated as the measurements minus the mean squared divided by the number of measurements. In contrast, in R, using scale or per comp, variation is calculated as the measurements minus the mean squared divided by the number of measurements minus 1. This method results in larger but unbiased estimates of the variation. The good news is that these differences do not affect the PCA analysis. The loading scores in the amount of variation per principle component will be the same either way."
864,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,438.0,476.0,38.0," The bad news is that these differences will have a minor effect on the final graph. This is because the coordinates on the final graph come from multiplying the loading scores by the scaled values. Now we create a PCA object. Rather than just have a function that does PCA in returns results, scikit-learn uses objects that can be trained using one data set and applied to another data set. Since we're only using PCA to explore one data set and not using PCA in a machine learning setting, the additional steps are a little tedious,"
865,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,476.0,503.0,27.0," but they set us up for the machine learning topics that I'll be covering very soon. Then we call the Fit method on the scaled data. This is where we do all the PCA math. IE, we calculate the loading scores and the variation each principle component accounts for. Lastly, this is where we generate coordinates for a PCA graph based on the loading scores and the scaled data."
866,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,503.0,536.0,33.0," Now we're ready to draw a graph. We'll start with a screen plot to see how many principle components should go into the final plot. The first thing we do is calculate the percentage of variation that each principle component accounts for. Now we create labels for the screen plot. These are PC1, PC2, etc. One label per principle component. Now we use MAT plot lib to create a bar plot."
867,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,536.0,584.0,48.0," And here's what the screen plot looks like with all the fancy annotation that we added using the Y label, X label and title methods. Almost all of the variation is along the first principle component, so a 2D graph using PC1 and PC2 should do a good job representing the original data. To draw a PCA plot, we'll first put the new coordinates created by PCA.Transform of the scaled data into a nice matrix where the rows have sample labels and the columns have PC labels. These commands draw scatter plot with the title and nice access labels. And this loop adds sample names to the graph."
868,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,584.0,629.0,45.0," Lastly, we display the graph. There it is. Wow. So awesome. But, um. The WT samples cluster on the left side, suggesting that they are correlated with each other. The KO samples cluster on the right side, suggesting that they are correlated with each other. In the separation of the two clusters along the X access, suggests that the wild type samples are very different from the knockout samples. Lastly, let's look at the loading scores for PC1 to determine which genes had the largest influence on separating the two clusters along the X access."
869,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,629.0,666.0,37.0," We'll start by creating a Pandus series object with the loading scores in principle component 1. Note, the present components are 0 index, so PC1 equals 0. Now we sort the loading scores based on their magnitude, aka we take the absolute value. Here we are just getting the names of the top 10 indexes, which are the gene names. Lastly, we print out the top 10 gene names and their corresponding loading scores."
870,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,666.0,693.0,27.0," And this is what we get. These values are super similar, so a lot of genes played a role in separating the samples rather than just one or two. Double-bound. Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you have any suggestions for future stat quests, well, put them in the comments below."
871,StatQuest: PCA in Python,https://www.youtube.com/watch?v=Lsue2gEM9D0,Lsue2gEM9D0,693.0,696.0,3.0," Until next time, quest on."
872,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,0.0,40.400000000000006,40.400000000000006, at you. Welcome to your request. The Stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill. Today we're going to be talking about linear discriminant analysis.
873,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,40.400000000000006,55.96,15.559999999999995," Which, let's be honest, sounds really fancy. And it kind of is, but not really. I think we can understand it. Let's see what it does and then we'll work it out. That is, let's look at some examples of why we might need linear discriminant analysis"
874,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,55.96,75.48,19.520000000000003," and then we'll talk about the details of how it works. Imagine that we have this cancer drug and that cancer drug works great for some people. But for other people, it just makes them feel worse. Yeah, yeah. We want to figure out who to give the drug to."
875,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,75.48,94.32,18.840000000000003," We want to give it to people who it's going to help. But we don't want to give it to people that it might harm. Since I'm a geneticist and I work in a genetics department, the way I answer all my questions is to look at gene expression. Maybe gene expression can help us decide."
876,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,94.32,111.44,17.11999999999999, Here's an example using one gene to decide who gets the drug and who doesn't. We've got a number line and on the left side we've got fewer transcripts and on the right side we've got more transcripts. The dots represent individual people. The green dots are people who the drug works for.
877,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,111.44,130.88,19.44, The red dots represent people whom the drug just makes them feel worse. We can see that for the most part the drug works for people with low transcription of gene X. And for the most part the drug does not work for people with high transcription of gene X.
878,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,130.88,152.2,21.319999999999997," In the middle we see that there's overlap and that there's no obvious cutoff for who to give the drug to. In summary, gene X doesn't okay job at telling us who should take the drug and who shouldn't. Can we do better? What if we used more than one gene to make a decision?"
879,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,152.2,178.4,26.200000000000017, Here's an example of using two genes to decide who gets the drug and who doesn't. On the X axis we have gene X and on the Y axis we have gene Y. Now that we have two genes we can draw a line that separates the two categories. The green where the drug works and the red where the drug doesn't work. And we can see that using two genes does a better job separating the two categories than
880,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,178.4,194.24,15.840000000000003, just using one gene. However it's not perfect. Would using three genes be even better? Here I've got an example where we're trying to use three genes to decide who gets the drug and who doesn't.
881,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,194.24,215.6,21.360000000000014," Gene Z is on the Z axis which represents depth so imagine a line going through your computer screen and into the wall behind it. And the big circles or the big samples are the ones that are closer to you and the smaller circles, smaller samples are the ones that are further away and those are along with the Z axis."
882,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,215.6,237.56,21.95999999999998," When we have three dimensions we use a plane to try to separate the two categories. Now I'll be honest, I drew this picture but even for me it's hard to tell if this plane separates the two categories correctly. It's hard for us to visualize three dimensions on a flat computer screen. We need to be able to rotate the figure and look at it from different angles to really"
883,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,237.56,252.72,15.159999999999997," now and that's tedious. What if we need four or more genes to separate two categories? Well the first problem is we can't draw four dimensional graph or a 10,000 dimensional graph. We just can't draw it."
884,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,252.72,267.68,14.960000000000008," That's a bummer. Yeah, yeah. We ran into the same problem when we talked about principal component analysis or PCA. And if you don't know about principal component analysis, be sure to check out the stat quest on that subject."
885,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,267.68,285.40000000000003,17.720000000000027," It's got a lot of likes and it's helped a lot of people understand how it works and what it does. PCA, if you can remember about it, reduces dimensions by focusing on genes with the most variation. This is incredibly useful when you're plotting data with a lot of dimensions or a lot"
886,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,285.40000000000003,309.92,24.519999999999985," of genes onto a simple x, y, plot. However, in this case we're not super interested in the genes with the most variation. Instead we're interested in maximizing the separability between the two groups so that we can make the best decisions. When your discriminant analysis, LDA, is like PCA, it reduces dimensions."
887,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,309.92,333.72,23.80000000000001," However, it focuses on maximizing the separability among the categories. Let's repeat that emphasize the point. Well, any or discriminant analysis, LDA, is like PCA, but it focuses on maximizing the separability among the known categories. Here we're going to start with a super simple example."
888,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,333.72,358.44000000000005,24.720000000000027," We're just going to try to reduce a two-dimensional graph to a 1D graph. That is to say we want to take this two-dimensional graph, a k a and x, y graph, and reduce it to a 1-dimensional graph, a k a a a a number line in such a way that maximizes the separability of the two categories. What's the best way to reduce the dimensions?"
889,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,358.88,381.68,22.80000000000001," Well, to answer that, let's start by looking at a bad way and understanding what its flaws are. One bad option would be to ignore gene y. And if we did that, we would just project the data down onto the x-axis. This is bad because it ignores the use-flonformation that gene y provides, projecting the"
890,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,381.68,408.0,26.319999999999997," genes onto the y-axis, i.e. ignoring the gene x, isn't any better. LDA provides a better way. Here we're going to try to reduce this two-dimensional graph to a 1D graph using LDA. LDA uses the information from both genes to create a new axis. And it projects the data onto this new axis in a way to maximize the separation of the"
891,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,408.0,431.72,23.720000000000027, two categories. So the general concept here is that LDA creates a new axis and it projects the data onto that new axis in a way that maximizes the separation of the two categories. Now let's look at the nitty-gritty details and figure out how LDA does that. How does LDA create the new axis?
892,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,431.72,460.36,28.639999999999983," The new axis is created according to two criteria that are considered simultaneously. The first criteria is that once the data is projected onto the new axis, we want to maximize the distance between the two means. Here we have a green new character which is a green character representing the mean for the green category and a red mu representing the mean for the red category."
893,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,460.36,484.3200000000001,23.96000000000004, The second criteria is that we want to minimize the variation which LDA calls scatter and is represented by S squared within each category. On the left side we see the scatter around the green dots. On the right side we see the scatter around the red dots. And this is how we consider those two criteria simultaneously.
894,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,484.3200000000001,504.24,19.91999999999996, We have a ratio of the difference between the two means squared over the sum of the scatter. The numerator is squared because we don't know if the green mu is going to be larger than the red mu or the red mu is going to be larger than the green mu. We don't want that number to be negative. We want it to be positive.
895,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,504.24,525.6800000000001,21.44000000000005," So whatever it is, whether it's negative or positive, begin with, we square it and it becomes a positive number. Now ideally the numerator would be very large. There'd be a big difference or a big distance between the two means. And ideally the denominator would be very small and that the scatter, the variation of the data"
896,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,525.76,553.8399999999999,28.079999999999927," around each mean in each category would be small. Now I know this isn't a very complicated equation, but to make things simpler later on in this discussion, let's call the difference between the two means D for distance. So we can replace the difference between the two means with D. Now I want to show you an example of why both the distance between the two means and the scatter"
897,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,553.84,574.8000000000001,20.96000000000004," are important. Here's a new dataset. We still just have two categories green and red. In this case there's a little bit of overlap on the y-axis, but lots of spread along the x-axis. If we only maximize the distance between the means, then we'll get something like this."
898,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,575.84,594.64,18.799999999999955," And the result is we'll have a lot of overlap in the middle. This isn't a great separation. However, if we optimize the distance between the means and the scatter, then we get nice separation. Here the means are a little closer to each other than they were in the graph on the top,"
899,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,595.4399999999999,613.04,17.600000000000023," but the scatter is much less. So if we optimize both criteria at the same time, we can get good separation. So what if we have more than two genes? That is to say, what if we have more than two dimensions? The good news is that the process is the same."
900,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,613.04,632.64,19.600000000000023," We've created new axis that maximizes the distance between the means for the two categories while minimizing the scatter. So here's an example of trying to do LDA with three genes. We've got that three-dimensional graph that I showed you earlier. Here we've created a new axis,"
901,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,635.12,656.48,21.360000000000014," and the data are projected onto the new axis. This new axis was chosen to maximize the distance between the two means between the two categories that is while minimizing the scatter. What if we have three categories? In this case, two things change, but just bearer."
902,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,657.52,680.0799999999999,22.559999999999945," Here's a plot that has two genes, but now we have three categories. The first difference between having three categories as opposed to just two categories like we had before is how we measure the distances among the means. Instead of just measuring the distance between the two means, we first find a point that is central to all of the data."
903,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,681.4399999999999,700.72,19.280000000000086, Then we measure the distances between a point that is central in each category and the main central point. Now we want to maximize the distance between each category and the central point while minimizing the scatter for each category. And here's the equation that we want to optimize.
904,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,702.0799999999999,724.0799999999999,22.0," And this is the same equation as before, but now there are terms for the blue category. The second difference is LDA creates two axes to separate the data. This is because the three central points for each category to find a plane, remember from high school, two points to find a long, and three points to find a plane."
905,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,724.48,744.08,19.600000000000023," That is to say we create new X and Y axes. However, these are now optimized to separate the categories. When we only use two genes, this is no big deal. The data started out on an XY plot, imploding them on a new XY plot doesn't change all that much."
906,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,745.5200000000001,770.5600000000001,25.039999999999964," But what if we use data from 10,000 genes? That would mean we need 10,000 dimensions to draw the data. Suddenly, being able to create two axes that maximize the separation of the three categories is super cool, it's way better than drawing a 10,000 dimension figure that we can't even imagine what it would look like. Here's an example using real data."
907,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,770.5600000000001,789.0400000000001,18.480000000000015," I'm trying to separate three categories and I've got 10,000 genes. Flodding the raw data would require 10,000 axes. We used LDA to reduce the number to two. And although the separation isn't perfect, it is still easy to see three separate categories."
908,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,790.64,811.0400000000001,20.40000000000009, Now let's use that same data set to compare LDA to PCA. Here's the LDA plot that we saw before. And now we've applied PCA to the exact same set of genes. PCA doesn't separate the categories nearly as well. We can see lots of overlap between the black and the blue points.
909,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,811.4399999999999,833.92,22.480000000000015," However, PCA wasn't even trying to separate those categories. It was just looking for the genes with the most variation. So we've seen the differences between LDA and PCA, but now let's talk about some of the similarities. The first similarity is that both methods rank the new axes that they create in order of importance."
910,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,834.3199999999999,868.4,34.08000000000004," PC1, the first new axis that PCA creates accounts for the most variation in the data. Likewise, PC2, the second new axis, does the second best job. And this goes on and on for the number of axes that are created from the data. LD1, the first new axis that LDA creates accounts for the most variation between the categories. LD2, the second new axis, does the second best job, etc, etc."
911,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,869.8399999999999,898.64,28.800000000000068," Also, both methods like you dig in and see which genes are driving the new axes. In PCA, this means looking at the loading scores. In LDA, one thing you can do is look and see which genes or which variables correlate with the new axes. So, in summary, LDA is like PCA, both try to reduce dimensions. PCA does this by looking at the genes with the most variation."
912,StatQuest: Linear Discriminant Analysis (LDA) clearly explained.,https://www.youtube.com/watch?v=azXCzI57Yfc,azXCzI57Yfc,899.52,910.64,11.120000000000005," In contrast, LDA tries to maximize the separation of known categories. And that's it. Tune in next time for another exciting stack quest."
913,Bam!!! Clearly Explained!!!,https://www.youtube.com/watch?v=i4iUvjsGCMc,i4iUvjsGCMc,0.0,26.64,26.64," When the go-and-get's tough, the tough-get stuff, and when the tough-get stuff, the get-stack quest. Hello, I'm Josh Starman, welcome to Stack Quest. Today we're going to talk about BAM, and it's going to be clearly explained. Note, this stack quest assumes that you want to know what BAM means."
914,Bam!!! Clearly Explained!!!,https://www.youtube.com/watch?v=i4iUvjsGCMc,i4iUvjsGCMc,26.64,59.52,32.879999999999995," If not, it assumes you have a sense of humor. If you watch Stack Quest videos, chances are you've experienced BAM. You may have also experienced double BAM, and if you're really lucky, you might have experienced the rare and coveted triple BAM. Unfortunately, some of you may have also experienced small BAM, or even worse, tiny BAM."
915,Bam!!! Clearly Explained!!!,https://www.youtube.com/watch?v=i4iUvjsGCMc,i4iUvjsGCMc,59.52,84.96,25.44," If you've experienced these BAMs and wondered what they're all about, the good news is that you are not alone. This guy wants to know what BAM means, and so does this dude. When you see a big BAM like BAM, then you know something awesome just happened. And if it didn't seem awesome to you, just rewind and play it over and over again."
916,Bam!!! Clearly Explained!!!,https://www.youtube.com/watch?v=i4iUvjsGCMc,i4iUvjsGCMc,85.84,116.24,30.40000000000001," Sooner or later, you will achieve enlightenment and become one with the BAM. Note, the size and number of BAMs is correlated with the awesomeness. For example, this is just one BAM with font size equal to 88, and it means something awesome just happened. In contrast, this is three BAMs, with font size equal to 248, and it means something totally awesome happened."
917,Bam!!! Clearly Explained!!!,https://www.youtube.com/watch?v=i4iUvjsGCMc,i4iUvjsGCMc,117.28,149.92,32.639999999999986," When you see a small BAM, you know you've just survived another dreaded terminology alert. Or maybe you just saw something else that is lame, but probably important to know. In summary, BAM, double BAM, and triple BAM. Let you know that you're watching, StatQuest. Because StatQuest is totally BAM."
918,Bam!!! Clearly Explained!!!,https://www.youtube.com/watch?v=i4iUvjsGCMc,i4iUvjsGCMc,152.16,167.28,15.120000000000005," Whoay, we've made it to the end of another exciting StatQuest. I hope you guys are all healthy and well, and enjoy this little bit of light humor in this time of uncertainty. All right, until next time, Quest on."
919,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,0.0,27.08,27.08," That quest is groovy, just like a movie groove. Hello, I'm Josh Starmer and welcome to Stac Quest. Today we're going to be talking about multi-dimensional scaling, MDS, and principal coordinate analysis, PCOA. First of all, if you don't have principal component analysis, PCA, down cold, check out"
920,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,27.16,50.2,23.040000000000006," the stat quest, PCA main ideas in five minutes. Principal component analysis and multi-dimensional scaling are both very, very similar. So I want you to be able to understand PCA before we move on with this one. Also, to be clear about what we will cover in this stat quest, there are two types of multi-dimensional scaling."
921,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,50.2,68.36,18.16, There's classical or metric multi-dimensional scaling versus non-metric multi-dimensional scaling. I'm only going to talk about classical multi-dimensional scaling in this stat quest. And classical multi-dimensional scaling is the exact same thing as principal coordinate analysis.
922,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,68.36,89.2,20.840000000000003," Okay, enough preliminary stuff. Let's dive in. Let's say we had some normal cells. If you're not a biologist, these could be people or cars or cities, etc. Even though they look the same, we suspect that there are differences."
923,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,89.2,117.4,28.200000000000003," These might be one type of cell or one type of person or car or city, etc. These might be another type of cell, and these might be a third type of cell. Unfortunately, we can't observe the differences from the outside. So we sequence the messenger RNA in each cell to identify which genes are active. This tells us what each cell is doing."
924,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,117.4,137.76,20.359999999999985," Alternatively, if they were people, we could measure their height, blood pressure, reading, love, etc. Here's the data. Each column shows how much each gene is transcribed in each cell. When we did PCA in the stat quest that explains the main ideas of PCA in five minutes,"
925,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,137.76,165.39999999999998,27.639999999999983," we converted the correlations or lack their ob among the samples into a two-D principal component analysis plot. And we saw that highly correlated samples form clusters. Multidimensional scaling, MDS, and principal coordinate analysis, PCOA, are very similar to PCA, except that instead of converting correlations into a 2D graph, they convert distances"
926,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,165.44,195.92,30.480000000000015," among the samples into a 2D graph. So in order to do MDS or PCOA, we have to calculate the distance between cell 1 and cell 2. And the distance between cell 1 and cell 3, and the distance between cell 1 and cell 4, and the distance between cell 2 and cell 3. Alright, you get the idea, we calculate the distance between every pair of cells."
927,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,195.92,222.44,26.519999999999985," Now let's just talk about how to calculate distances. For now, let's imagine we only needed to calculate the distance between cell 1 and cell 2. One very common way to calculate distances between two things is to calculate the Euclidean distance. If we just had two genes, we could draw a line that represented the difference between the"
928,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,222.44,247.88,25.44," values for gene 1, and we could draw a line that represented the difference between the values for gene 2. Then the Euclidean distance between cell 1 and cell 2 would be the hypotenuse, i.e. the Pythagorean theorem. With more genes, we just add the square of more differences between more genes."
929,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,247.88,277.32,29.44," This is the difference for gene 1, this is the difference for gene 2, and this is the difference for gene 3, et cetera et cetera et cetera. And once we calculated the distance between every pair of cells, MDS and PCOA would reduce them to a two-dimensional graph. The bad news is that if we use the Euclidean distance, the graph would be identical to a PCA"
930,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,277.32,297.16,19.83999999999997," graph. In other words, clustering based on minimizing the linear distances is the same as maximizing the linear correlations. The good news is that there are tons of other ways to measure distance. We don't have to use the Euclidean distance, although sometimes people choose to use it"
931,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,297.16,321.0,23.840000000000032," anyways. For example, another way to measure distances between cells is to calculate the average of the absolute value of the log-fold changes among the genes. Using the data from cells 1 and 2, the log-fold change for gene 1 is the log of 3 divided by 0.25."
932,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,321.0,355.84,34.839999999999975," And the log-fold change for gene 2 is the log of 2.9 divided by 0.8. And we just keep going calculating log-fold changes for each gene for cells 1 and 2. For example, the log-fold change for gene 8 equals the log of 1 divided by 2.7. Here the actual log-2 values of the ratios I've just talked about. We've got 3.58, 1.86, dot, dot, dot, and then negative 1.43."
933,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,355.84,376.04,20.19999999999999," Now we take the absolute value. Lastly, we take the average of all the numbers, that's the average of the absolute value of the log-fold change among the genes. Note, we take the absolute value so that the negative fold changes don't cancel out the positive ones."
934,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,376.04,400.4,24.36000000000007," Ultimately, we'll get graphs that look different. The biologist might choose to use log-fold change to calculate distance because they are frequently interested in log-fold changes among genes. But there are lots of distances to choose from. The Manhattan distance, Hamming distance, Great Circle distance, etc. etc."
935,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,400.4,429.4,28.999999999999943," You can look them up on the web. Selecting the best distance is part of the art of data science. In summary, PCA creates plots based on correlations among samples. And MDS and Principal Coordinate Analysis create plots based on distances among samples. The closer samples are to each other, the more tightly they cluster in the final plot."
936,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,429.4,455.0,25.600000000000023," In other words, PCA starts by calculating the correlations among the samples. In their some fancy math, specifically, eigend composition. And out of that, we get coordinates for a graph. We get the percent of variation each access accounts for. And we get loading scores to determine which variables have the largest effect."
937,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,455.0,472.8,17.80000000000001," In contrast, MDS and Principal Coordinate Analysis start by calculating distances among the samples. However, that's the only difference. The fancy math is the exact same. In the output is similar. You get coordinates for a graph."
938,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,472.8,490.72,17.920000000000016," The percent of variation that each access accounts for. And loading scores to determine which variables have the largest effect. Hooray! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe."
939,StatQuest: MDS and PCoA,https://www.youtube.com/watch?v=GEn-_dAyYME,GEn-_dAyYME,490.72,497.52,6.800000000000011," And if you have any ideas for future stat quest, well, put them in the comments below. Until next time, quest on."
940,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,0.0,21.92,21.92," Stack West on a Monday. Makes for a fun day. Stack West on any day. Makes that day fun. Hello, I'm Josh Starmer and welcome to Stack West."
941,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,21.92,45.24,23.32," Today we're going to talk about doing multidimensional scaling, MDS, and Principal Cordonid Analysis, PCOA, and R. If you don't already know, MDS or Classical or Metric MDS is the exact same thing as PCOA. One last thing before we move on. The code that I use in the Stack West is available on the Stack West website, and the"
942,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,45.24,66.62,21.380000000000003," link to that code is in the description below. First, we load in GG plot 2 since we'll need it later to draw fancy looking graphs. Now, we generate some fake data. This is exactly the same as the data we used in the PCA and R Stack West, so I'm going to breeze through this pretty quickly."
943,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,66.62,91.58,24.959999999999997," If you need more details, check out that Stack West. The data will consist of a matrix with 10 columns corresponding to 10 samples and 100 rows corresponding to measurements from 100 genes. The first five columns will be WT or wild type samples and the last five columns will be KO or knockout samples."
944,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,91.58,115.42,23.83999999999999," The genes will have really creative names like G1 and G2. This is where we generate the fake data, and the head function shows us the first six rows or genes for all 10 samples. Now, just for comparison, we'll do PCA on the data set. I'm going to breeze through these steps since we went over them already in the PCA and"
945,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,115.42,131.86,16.439999999999998," R Stack West. Now we create a PCA plot using GG plot. Note, we covered this command in the PCA and R Stack West, so check that out if this looks totally crazy. Bam!"
946,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,131.86,155.3,23.440000000000023," The wild type samples are on the left side of the graph, and the knockout samples are on the right side. The X-axis, PC1, for the first principal component, accounts for 91% of the variation in the data. In the Y-axis, for PC2, or principal component 2, only accounts for 2.7% of the variation"
947,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,155.3,176.9,21.599999999999994," in the data. This means that most of the differences were between the wild type and the knockout samples. Now let's create an MDS or PCOA plot to compare to this one. Step 1, created Distance Matrix. We do this with the dist function."
948,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,176.9,203.66,26.75999999999999," Just like with PCA, we transpose the matrix to the samples are rows. We also center and scale the measurements for each gene, which are now the columns. Lastly, we tell the dist function that we want to create the matrix using the Euclidean Distance Matrix. Note, the dist function has six different distance metrics to choose from."
949,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,203.66,232.26,28.600000000000023," Step 2, perform multidimensional scaling on the distance matrix using the CMD scale function. CMD scale stands for classical multidimensional scaling. We tell CMD scale that we want it to return the eigenvalues. We use these to calculate how much variation in the distance matrix each axis in the final MDS plot accounts for."
950,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,232.26,254.18,21.919999999999987," We can also get CMD scale to return the doubly centered, i.e. both rows and columns are centered, version of the matrix. This is useful if you want to demonstrate how to do MDS using the eigen function instead of the CMD scale function. Originally, I thought I was going to demonstrate how to use the eigen function to do multidimensional"
951,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,254.18,272.54,18.360000000000014," scaling. But in the end, I really wanted to keep this practical, and if you're going to do MDS, you're going to use the CMD scale function. Step 3, calculate the amount of variation each axis in the MDS plot accounts for using the eigenvalues."
952,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,272.54,300.54,28.0," Step 4, format the data for GG plot. Lastly, call GG plot to make a fancy graph. Just like in the PCA graph, the wild type samples are on the left side of the graph, and the knockout samples are on the right side. And just like in the PCA graph, the x-axis accounts for 91% of the variation in the data."
953,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,300.54,338.46000000000004,37.920000000000016," Actually, the PCA graph and the MDS graph don't just look similar, they are exactly the same. This is because we use the Euclidean metric to calculate the distance matrix. Now let's see what happens when we use a different metric to calculate the distance matrix. Let's use the average of the absolute value of the log-fold change. Psst, for all you gene expression folks, this is what Edge R does when you call the plot"
954,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,338.46000000000004,363.42,24.95999999999998," MDS function. The first thing we do is calculate the log-2 values of the measurements for each gene. Since the average of absolute values of the log-fold change isn't one of the distance matrix built into the disk function, we'll create our own distance matrix by hand. In this step, we're just creating an empty matrix."
955,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,363.42,384.74,21.319999999999997," This is where we fill the matrix with the average of the absolute values of the log-fold changes. And here's what that matrix looks like. Because the full matrix would be symmetrical, we only have to calculate the values for the lower triangle. Now we perform multi-dimensional scaling on our new distance matrix."
956,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,384.74,404.22,19.47999999999996," Here I'm just converting our homemade matrix into a true distance matrix so that CMD scale knows what it's working with. In other words, a true distance matrix only needs the bottom triangle to be computed and not the whole thing. Everything else is the same."
957,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,404.22,426.98,22.75999999999999," Just like before, we calculate the amount of variation each axis in the MDS plot accounts for using the eigenvalues. And again, just like before, we format the data for GG plot. Lastly, we create the graph using GG plot. Double bound."
958,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,426.98,456.66,29.680000000000064," The two different MDS plots, one using the Euclidean distance and the other using the average of the absolute value of the log-fold change are similar, but not the same. In the new graph, the x-axis accounts for more of the variation, 99.2% versus 91%. Here we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more of them, please subscribe."
959,StatQuest: MDS and PCoA in R,https://www.youtube.com/watch?v=pGAUHhLYp5Q,pGAUHhLYp5Q,456.66,463.86,7.199999999999989," And if you have any ideas for additional stat quests, we'll put them in the comments below. Alright, until next time, quest on."
960,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,0.0,19.0,19.0, I'm drawn a graph. Doesn't it look cool? But I didn't know how it would. Until I watched that quest. Hello and welcome to StatQuest.
961,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,19.0,50.0,31.0," StatQuest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill. Today we're going to be talking about T-Sney or Tisney to be honest I don't actually know how it's pronounced, but it's going to be clearly explained I know that bit. Also, this StatQuest is by request. A couple of people put it in the comments below and I got a couple of emails from other people, so I'm doing it because you guys want it. Here goes."
962,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,50.0,93.0,43.0," If you're watching this StatQuest, chances are you've seen an example of a T-Sney graph before. What T-Sney does is it takes a high-dimensional dataset and reduces it to a low-dimensional graph that retains a lot of the original information. If you're not familiar with those terms of taking a high-dimensional dataset and reducing it to a low-dimensional graph, you might want to watch the StatQuest for PCA because I explain what that means in that video. Here's a basic 2D scatter plot. Let's do a walk through of how T-Sney would transform this graph into a flat, one-dimensional plot on a number line."
963,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,93.0,145.0,52.0," I'm going to use this super simple example to explain the concepts behind T-Sney so that when you see it applied to a much larger dataset, a much more complex dataset, you'll still know how that graph was drawn. Note, if we just projected the data onto one of the axes, we just get a big mess that doesn't preserve the original clustering. If we project it onto the Y-axis, instead of two distinct clusters, we just see a MishMash. And the same thing happens if we just project the data onto the X-axis. What T-Sney does is find a way to project data into a low-dimensional space, in this case the one-dimensional number line, so that the clustering in the high-dimensional space, in this case, the two-dimensional scatter plot is preserved."
964,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,145.0,169.0,24.0," So let's step through the basic ideas of how T-Sney does this. We'll start with the original scatter plot. Then we'll put the points on the number line in a random order. From here on out, T-Sney moves these points a little bit at a time until it is clustered them. Let's figure out where to move this first point."
965,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,169.0,195.0,26.0," Should it move a little to the left or a little to the right? Because it is part of this cluster in the two-dimensional scatter plot, it wants to move closer to these points. But at the same time, these points are far away in the scatter plot. So they push back. These points attract while these points repel."
966,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,195.0,221.0,26.0," In this case, the attraction is strongest, so the point moves a little to the right. Bam. Now let's move this point a little bit. These points attract because they are close to each other in the two-dimensional scatter plot. And this point repels a little bit because it is far from the point in the two-dimensional scatter plot."
967,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,221.0,244.0,23.0," So it moves a little closer to the other orange points. Double bam. At each step, a point on the line is attracted to points it is near in the scatter plot, and repelled by points it is far from. Triple bam."
968,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,244.0,271.0,27.0," Now that we've seen what T-Sneat tries to do, let's dive into the nitty-gritty details of how it does what it does. Step one. Determine the similarity of all the points in the scatter plot. For this example, let's focus on determining the similarities between this point and all of the other points. First, measure the distance between two points."
969,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,271.0,297.0,26.0," Then plot that distance on a normal curve that is centered on the point of interest. Lastly, draw a line from the point to the curve. The length of that line is the unscaled similarity. I made that terminology up, but it'll make sense in just a bit, so hold on. Now we calculate the unscaled similarity for this pair of points."
970,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,297.0,323.0,26.0," Now we calculate the unscaled similarity for this pair of points. And now we calculate the unscaled similarity for this pair of points. It's cetera, et cetera, et cetera. Using a normal distribution means that distant points have very low similarity values. And close points have high similarity values."
971,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,323.0,354.0,31.0," Ultimately, we measure the distances between all of the points and the point of interest. Then plot them on a normal curve, and then measure the distances from the points to the curve to get the unscaled similarity scores with respect to the point of interest. The next step is to scale the unscaled similarities so that they add up to one. Why do the similarity scores need to add up to one?"
972,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,354.0,384.0,30.0," It has to do with something I didn't tell you earlier. And to illustrate the concept, I need to add a cluster that is half as dense as the others. The width of the normal curve depends on the density of data near the point of interest. Less dense regions have wider curves. So if these points have half the density as these points, and this curve is half as wide as this curve,"
973,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,384.0,407.0,23.0, then scaling the similarity scores will make them the same for both clusters. Here's an example where I've worked out the math. This curve has a standard deviation equal to one. These are the unscaled similarity values. This curve has a standard deviation equal to two.
974,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,407.0,428.0,21.0," These points are twice as far from the middle. The unscaled similarity values are half of the other ones. To scale the similarity scores so that they sum to one, you take a score and you divide it by the sum of all the scores. That equals the scaled score."
975,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,428.0,456.0,28.0, Here's how the math works out when the distribution has a standard deviation equals to one. We get 0.82 and 0.18 as the scaled similarity scores. And here's the math for when everything is spread out twice as much. We get 0.82 and 0.18. The similarity scores on top are equal to the similarity scores below.
976,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,456.0,487.0,31.0, They're the same. That implies that the scaled similarity scores for this relatively tight cluster are the same for this relatively loose cluster. The reality is a little more complicated but only slightly. T-snay has a perplexity parameter equal to the expected density around each point. And that comes into play but these clusters are still more similar than you might expect.
977,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,487.0,506.0,19.0, Now back to the original scatter plot. We've calculated similarity scores for this point. Now we do it for this point. And we do it for all the points. One last thing and a scatter plot will be all set with similarity scores.
978,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,506.0,532.0,26.0," Because the width of the distribution is based on the density of the surrounding data points, the similarity score for this node might not be the same as the similarity to this node. So T-snay just averages the two similarity scores from the two directions. No big deal. Ultimately, you end up with a matrix of similarity scores."
979,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,532.0,561.0,29.0," Each row and column represents the similarity scores calculated from that point of interest. Red equals high similarity and white equals low similarity. I've drawn the similarity from a point of interest to itself as dark red. However, it doesn't really make sense to say that a point is similar to itself because that doesn't help the clustering. So T-snay actually defines that similarity as zero."
980,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,561.0,591.0,30.0," Her A were done calculating similarity scores for the scatter plot. Now we randomly project the data onto the number line and calculate similarity scores for the points on the number line. Just like before, that means picking a point, measuring a distance, and lastly, drawing a line from the point to a curve. However, this time we're using a T distribution."
981,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,591.0,613.0,22.0," A T distribution is a lot like a normal distribution. Except the T isn't as tall in the middle, and the tails are taller on the ends. The T distribution is the T and T-snay. We'll talk about why the T distribution is used in a little bit."
982,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,613.0,641.0,28.0," So, using a T distribution, we calculate unscaled similarity scores for all the points, and then scale them like before. Like before, we end up with a matrix of similarity scores, but this matrix is a mess compared to the original matrix. The goal of moving this point is, we want to make this row look like this row."
983,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,641.0,660.0,19.0," T-snay moves the points a little bit at a time. At each step, it chooses a direction that makes the matrix on the left more like the matrix on the right. It uses small steps because it's a little bit like a chess game, and can't be solved all at once."
984,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,660.0,676.0,16.0," Instead, it goes one move at a time. Bam. Now to finally tell you why the T distribution is used. Without it, the clusters would all clump up in the middle and be harder to see."
985,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,676.0,694.0,18.0," Triple Bam. And now we know how T-snay works. I've used a really simple example here, but the concepts are the exact same for more complicated data sets. Hooray, we've made it to the end of another exciting stat quest."
986,"StatQuest: t-SNE, Clearly Explained",https://www.youtube.com/watch?v=NEaUSP4YerM,NEaUSP4YerM,694.0,707.0,13.0," If you like this stat quest and want to see more like it, please subscribe. And if you have any idea for future stat quests, just put them in the comments below. Until next time, quest on."
987,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,0.0,24.0,24.0, Going on a quest. On a stat quest. Stat Quest. Hello and welcome to stat quest. Today we're going to be talking about hierarchical clustering.
988,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,24.0,46.0,22.0," hierarchical clustering is often associated with heat maps. If you're not already familiar with what heat maps are, just know that the columns typically represent different samples and that the rows typically represent measurements from different genes. Red typically signifies high expression of a gene"
989,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,46.0,70.0,24.0," and blue or purple means lower expression for a gene. hierarchical clustering orders the rows and or the columns based on similarity. This makes it easy to see correlations in the data. For example, these samples express the same genes. And these genes behave the same."
990,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,70.0,93.0,23.0," On the left, we have a heat map without hierarchical clustering. And on the right, we have a heat map with hierarchical clustering. So you can see that the clustering makes a big difference on how the data is presented. Heat maps often come with danger grams. So we'll talk about those two."
991,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,93.0,118.0,25.0," Let's get started. We'll start with a simple example. Here we've got a simple heat map that has three samples and four genes. For this example, we are just going to cluster or reorder the rows or the genes. Conceptually, the first step is to figure out which gene is most similar to gene number one."
992,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,118.0,146.0,28.0," Genes number one and two are different. We can tell because the colors are very different. Gene one is highly expressed in sample number one, so it has a red color. Gene two, however, is not highly expressed in sample number one, so it has a blue color. In sample number three, gene one is lowly expressed, so it's blue, and gene two is highly expressed."
993,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,146.0,168.0,22.0," So it's red. Gene's one and three are similar. So that means in sample one, both gene one and three are red, they're highly expressed. And in sample three, they're both blue, meaning they're lowly expressed. Gene's one and four are also similar."
994,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,168.0,199.0,31.0," However, gene number one is most similar to gene number three. So the second step is to figure out what gene is most similar to gene number two. So we do all the comparisons and we see that gene number two is most similar to gene number four. And then we do the same thing for gene number three and then gene number four. In step number three, we look at the different combinations and figure out which two genes are the most similar."
995,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,199.0,230.0,31.0," Once we've done that, we merge them into a cluster. In this case, gene number one and three are more similar than any other combination of genes. So genes one and three are now cluster number one. Step four, go back to step one, but now treat the new cluster like it's a single gene. So in step one, we figure out which gene is most similar to cluster number one."
996,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,230.0,256.0,26.0," Cluster number one is most similar to gene number four. And we figure out which gene is most similar to gene number two. In this case, gene number two is most similar to gene number four, but notice that we compare gene number two to cluster number one. And then we do the same thing for gene number four."
997,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,256.0,276.0,20.0," Of the different combinations, figure out which two genes are the most similar. Now merge them into a cluster. In this case, gene two and four are the most similar combination. So we've merged them into a cluster. Now we go back to step one."
998,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,276.0,301.0,25.0," However, since all we have left are two clusters, we merge them. Bam, we're all done. Higher-coco-clustering is usually accompanied by a danger gram. It indicates both the similarity and the order that the clusters were formed. Cluster number one was formed first and is most similar."
999,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,301.0,322.0,21.0," It has the shortest branch. Cluster number two was second and is the second most similar. It has the second shortest branch. Cluster number three, which contains all of the genes, was formed last. It has the longest branch."
1000,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,322.0,343.0,21.0," Now let's go over a few knit picky details. Remember the first step? Figure out which gene is most similar to gene number one? Well, we have to define what most similar means. The method for determining similarity is arbitrarily chosen."
1001,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,343.0,371.0,28.0," However, the Euclidean distance between genes is used a lot. Let's look at an example. We'll use a very simple heat map that just has two samples and two genes. Now we're displaying the values that underlie the colors that we have in the heat map. The Euclidean distance between genes one and two is just the square root of the difference in sample number one, square,"
1002,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,371.0,395.0,24.0," plus the difference in sample number two, square. Here we'll just plug in the values for sample number one. We have 1.6 minus negative 0.5. Now let's plug in the values to calculate the difference in sample number two. We have 0.5 minus negative 1.9."
1003,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,395.0,428.0,33.0," Doing the subtraction gives us the square root of 2.1 squared, plus 2.4 squared. We can think of these values within the parentheses as sides on a triangle. So, while in the x-axis, we have the distance between gene 1 and gene 2 in sample number one. And on the y-axis, we have the distance between gene 1 and 2 in sample number two. The hypotenuse is the distance between genes 1 and 2."
1004,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,428.0,456.0,28.0," The Pythagorean theorem says that the hypotenuse equals the square root of x squared plus y squared. In this case, that means the square root of 2.1 squared plus 2.4 squared. And that gives us 3.2, the distance between gene number one and gene number two. When we have more samples, we just extend the equation. It's no big deal."
1005,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,456.0,486.0,30.0," The Euclidean distance is just 1 method. There are lots more including the Manhattan distance. The Manhattan distance is just the absolute value of the differences. So instead of squaring the differences and then taking the square root, all we do is take the absolute value of the differences. We can think of the Manhattan distance in geometric terms by imagining that each difference is a line segment."
1006,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,486.0,515.0,29.0," If we take all those line segments and put them together, head to tail, head to tail, and then add that total length of all those lines segments together, that's the Manhattan distance. Yes, it makes a difference. Here's a heat map drawn using the Euclidean distance. And here's the same information drawn as a heat map, but now we're using the Manhattan distance."
1007,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,515.0,534.0,19.0," The heat maps are very similar, but there are also a few differences. The choice in distance metric is arbitrary. What? There's no biological or physical reason to choose one and not the other. Pick the one that gives you more insight into your data."
1008,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,534.0,562.0,28.0," Now, do you remember how we merge genes one and three into cluster number one and compare it to other genes? Well, there are different ways to compare clusters two. One simple idea is to use the average of the measurements from each sample, but there are lots more. And these have effect on clustering as well. So let's talk about the different ways to compare clusters."
1009,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,562.0,587.0,25.0," For the sake of visualizing how the different methods work, imagine our data was spread out on an x, y plane. Now imagine that we have already formed these two clusters. And we just want to figure out which cluster this last point belongs to. We can compare that point to the average of each cluster. This is called the centroid."
1010,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,587.0,602.0,15.0, The closest point in each cluster. This is called single linkage. Or we can compare it to the furthest point in each cluster. This is called complete linkage. And there are other methods as well.
1011,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,602.0,628.0,26.0," Here's a heat map that compares the furthest points in the clusters. By the way, if you use R, this is the default setting for the age-cluster function. This heat map compares the average points in the clusters. And this last heat map compares the closest points in the clusters. These heat maps are all very similar, but there are also differences in the way the data is presented."
1012,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,628.0,654.0,26.0," In summary, clusters are formed based on some notion of similarity. You have to decide what that is. However, most programs have reasonable defaults. Once you have a sub-cluster, you have to decide how it should be compared to other rows, columns, or sub-clusters, etc. And most programs have good default settings for this as well."
1013,StatQuest: Hierarchical Clustering,https://www.youtube.com/watch?v=7xHsRkOdVwo,7xHsRkOdVwo,654.0,679.0,25.0," And the height of the branches in the dingergram shows you what is most similar. Hooray, we've made it to the end of another exciting stat quest. If you liked this presentation, please subscribe to my channel and you'll get more like it. Also, if you'd like me to do something specific, feel free to mention it in the comments below."
1014,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,0.0,34.480000000000004,34.480000000000004," Hello, I'm Josh Starmer and welcome to StachQuest. Today we're going to be talking about K means clustering. We're going to learn how to cluster samples that can be put on a line on an x-y graph and even on a heat map. And lastly, we'll also talk about how to pick the best value for K."
1015,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,34.480000000000004,56.96,22.48," Imagine you had some data that you could plot on a line, and you knew you needed to put it into three clusters. Maybe they are measurements from three different types of tumors or other cell types. In this case, the data make three relatively obvious clusters. But rather than rely on our I, let's see if we can get a computer to identify the same three"
1016,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,57.0,76.8,19.8," clusters. To do this, we'll use K means clustering. We'll start with raw data that we haven't yet clustered. Step one, select the number of clusters you want to identify in your data. This is the K in K means clustering."
1017,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,76.8,100.12,23.320000000000007," In this case, we'll select K equals three. That is to say, we want to identify three clusters. There is a fancier way to select a value for K, but we'll talk about that later. Step two, randomly select three distinct data points. These are the initial clusters."
1018,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,100.12,125.56,25.44," Step three, measure the distance between the first point and the three initial clusters. This is the distance from the first point to the blue cluster. This is the distance from the first point to the green cluster. This is the distance from the first point to the orange cluster. Well, it's kind of yellow, but we'll just call it orange for now."
1019,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,125.56,144.28,18.72," Step four, assign the first point to the nearest cluster. In this case, the nearest cluster is the blue cluster. Now we do the same thing for the next point. We measure the distances. Then assign the point to the nearest cluster."
1020,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,144.28,167.72000000000003,23.440000000000023," Now we figure out which cluster the third point belongs to. We measure the distances. Then assign the point to the nearest cluster. The rest of these points are closest to the orange cluster, so they'll go in that one too. Now that all the points are in clusters, we go on to step five."
1021,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,167.72,187.92,20.19999999999999," Now calculate the mean of each cluster. Then we repeat what we just did. Measure and cluster using the mean values. Since the clustering did not change at all during the last iteration, were done. Bam."
1022,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,187.92,214.28,26.360000000000014," The K means clustering is pretty terrible compared to what we did by I. We can assess the quality of the clustering by adding up the variation within each cluster. Here's the total variation within the clusters. Since K means clustering can't see the best clustering, its only option is to keep track of these clusters and their total variance and do the whole thing over again with different"
1023,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,214.28,235.16,20.879999999999995," starting points. So here we are again, back at the beginning. K means clustering picks three initial clusters and then clusters all the remaining points. Calculates the mean of each cluster and then reclusters based on the new means. It repeats until the clusters no longer change."
1024,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,235.16,254.8,19.639999999999983," Bip, bip, bip, bip, bup, bup, bup. Now that the data are clustered, we sum the variation within each cluster. And then we do it all again. At this point, K means clustering knows that the second clustering is the best clustering so far."
1025,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,254.8,268.2,13.400000000000006, But it doesn't know if it's the best overall. So it will do a few more clusters. It does as many as you tell it to do. And then come back and return that one if it is still the best. Question.
1026,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,268.2,288.88000000000005,20.680000000000064," How do you figure out what value to use for K? With this data, it's obvious that we should set K to three, but other times it is not so clear. One way to decide is to just try different values for K. We'll start with K equals one."
1027,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,288.88000000000005,310.52,21.63999999999993, K equals one is the worst case scenario. We can quantify its badness with the total variation. Now try K equals two. K equals two is better and we can quantify how much better by comparing the total variation within the two clusters to K equals one.
1028,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,310.52,327.32,16.80000000000001, Now try K equals three. K equals three is even better. We can quantify how much better by comparing the total variation within the three clusters to K equals two. Now try K equals four.
1029,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,327.32,352.48,25.160000000000025," The total variation within each cluster is less than when K equals three. Each time we add a new cluster, the total variation within each cluster is smaller than before. And when there is only one point per cluster, the variation equals zero. However, if we plot the reduction invariance per value for K, there is a huge reduction"
1030,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,352.48,378.44,25.95999999999998," in variation with K equals three. But after that, the variation doesn't go down as quickly. This is called an elbow plot and you can pick K by finding the elbow in the plot. Question, how is K means clustering different from hierarchical clustering? K means clustering specifically tries to put the data into the number of clusters you"
1031,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,378.44,400.48,22.04000000000002," tell it to. High-archical clustering just tells you, pairwise, what two things are most similar? Question, what if our data isn't plotted on a number line? Just like before, you pick three random points. And we use the Euclidean distance."
1032,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,400.48,426.12,25.639999999999983," In two dimensions, the Euclidean distance is the same thing as the Pythagorean theorem. Then just like before, we assign the point to the nearest cluster. And just like before, we then calculate the center of each cluster and recluster. Bam? Although this looks good, the computer doesn't know that until it does the clustering"
1033,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,426.12,448.72,22.59999999999997," a few more times. Question, what if my data is a heat map? Well, if we just have two samples, we can rename them x and y. Then we can then plot the data in an x, y graph. Then we can cluster just like before."
1034,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,448.72,469.28,20.56," Note, we don't actually need to plot the data in order to cluster it. We just need to calculate the distances between things. When we have two samples or two axes, the Euclidean distance is the square root of x squared plus y squared. Then we have three samples or three axes."
1035,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,469.28,493.8,24.519999999999985," The Euclidean distance is the square root of x squared plus y squared plus z squared. And when we have four samples or four axes, the Euclidean distance is the square root of x squared plus y squared plus z squared plus a squared. It's cetera, et cetera, et cetera. Hooray, we've made it to the end of another exciting stat quest."
1036,StatQuest: K-means clustering,https://www.youtube.com/watch?v=4b5d3muPQmA,4b5d3muPQmA,493.84,509.08,15.240000000000007," If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, click the like button down below and consider buying one or two of my original songs. All right, tune in next time for another exciting stat quest."
1037,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,0.0,27.0,27.0," Deep-y scan, clusters just like a person can. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about clustering with DB scan, and it's going to be clearly explained. Now, imagine we collected weight and height measurements from a bunch of people."
1038,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,27.0,56.0,29.0," And we plotted the people on a two-dimensional graph like this, where we have weight on the x-axis, the first dimension, and height on the y-axis, the second dimension. By eye, we can see two different clusters. By identifying two different, but relatively dense, clumps of people. In contrast, these people that are far from everyone else look a little bit like outliers."
1039,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,56.0,84.0,28.0," So, by eye, clustering this data is pretty easy. However, because these clusters are nested, meaning the green cluster, wraps around the blue cluster, a relatively standard clustering method like K means clustering might have difficulty identifying these two clusters. Instead, because of the nesting, a simple clustering method might get something weird like this,"
1040,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,84.0,110.0,26.0," where these points are assigned to the blue cluster, even though they look like they belong to the green cluster. So, we need a clustering algorithm that can handle nested clusters. Also, remember, this two-dimensional graph only uses weight and height data. But if we wanted to include each person's age, we would have to add a third axis, and now our graph is three-dimensional."
1041,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,111.0,136.0,25.0," Drawing a three-dimensional graph on a two-dimensional computer screen is awkward, but possible. However, if we wanted to include four or more features, we'd need to draw a four or more dimensional graph, and that's not possible. And if we can't draw and look at a four or more dimensional graph, then we need a way to identify nested clusters that we cannot see by eye."
1042,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,137.0,172.0,35.0," The good news is that there are clustering algorithms that can identify nested clusters in high dimensions. One of these algorithms is called DBSGAM, and that's what we'll talk about today. So, let's go back to the original two-dimensional graph and see how DBSGAM tries to mimic what we can easily do by eye. Now, remember, by eye, we identify clusters by the densities of the points. Clusters are in high density regions, and outliers tend to be in low density regions."
1043,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,172.0,206.0,34.0," So, let's see how DBSGAM uses the densities of the points to identify these two clusters. Bam. Now, starting with the raw, uncluster data, the first thing we can do is count the number of points close to each point. For example, if we start with this red point, and we draw an orange circle around it, then we can see that the orange circle overlaps at least partially eight other points. So, the red point is close to eight other points."
1044,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,206.0,251.0,45.0," Note, the radius of the orange circle is user-defined, so when using DBSGAM, you may need to fiddle around with this parameter. Now, this red point is close to five other points, because the orange circle overlaps at least partially five other points. This red point is close to six other points, and tool to seven other points. This red point is only close to two other points, and this red point is not close to any other point, because the orange circle does not overlap anything else. Likewise, for all the remaining points, we count the number of close points."
1045,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,251.0,288.0,37.0," Now, in this example, we will define a core point to be one that is close to at least four other points. Note, the number of close points for a core point is user-defined, so when using DBSGAM, you might need to fiddle with this parameter as well. Anyway, these four points are some of the core points, because their orange circles overlap at least four other points. Hooray! But neither of these points are core points, because their orange circles do not overlap four or more other points. What?"
1046,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,288.0,326.0,38.0," Ultimately, we can call all of these red points core points, because they are all close to four or more other points. And the remaining points are non-core. Now, we randomly pick a core point, and assign it to the first cluster. Next, the core points that are close to the first cluster, meaning they overlap the orange circle, are all added to the first cluster. Then, the core points that are close to the growing first cluster, join it, and extend it to other core points that are close by."
1047,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,326.0,362.0,36.0," Here we see two core points and one non-core point that are all close to the growing first cluster. And at this point, we only add the core points to the first cluster. That said, eventually, we will add this non-core point, but right now we are only adding core points. Ultimately, all of the core points that are close to the growing first cluster are added to it and then use to extend it further. Bam! Note, at this point, every single point in the first cluster is a core point."
1048,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,362.0,400.0,38.0," And because we can no longer add any more core points to the first cluster, we add all of the non-core points that are close to the core points to the first cluster. For example, this point, which is a non-core point, is close to a core point in the first cluster. So, we add it to the first cluster. However, because this is not a core point, we do not use it to extend the first cluster any further. That means that this other non-core point, which is close to the non-core point that was just made part of the first cluster,"
1049,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,400.0,429.0,29.0," will not be added to the first cluster because it is not close to a core point. So, unlike core points, non-core points can only join a cluster. They cannot extend it further. Now we add all of the non-core points that are close to core points in the first cluster to the first cluster. And now we are done creating the first cluster. Double-bow!"
1050,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,429.0,463.0,34.0," To summarize how the first cluster was formed, we picked a random core point and it started the first cluster. Then, neighboring core points joined and extended the first cluster. And non-core points only joined the first cluster. Bam! Now, because none of these core points are close to the first cluster, they form a new second cluster because they are close to each other."
1051,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,463.0,492.0,29.0," And the non-core points that are close to the second cluster are added to it. Lastly, because all of the core points have been assigned to a cluster, we are done making new clusters. And any remaining non-core points that are not close to core points in either cluster are not added to clusters and called outliers. And that is how the DB scan algorithm works. Triple-bow!"
1052,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,492.0,528.0,36.0," Note, as we just saw, clusters are created sequentially. That means if we had a non-core point close to both clusters, then when we built the first cluster, B-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b- perfect. We would add this non-core point to the first cluster because it is close to a core point, along with all of the other non-core points that were close. And now that this point is part of the first cluster, it is no longer eligible to be in any other cluster."
1053,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,528.0,544.0,16.0," Small Bam! Now it's time for some. Shameless self-promotion! If you want to review statistics and machine learning offline, check out the StacQuest study guides at statquest.org. There's something for everyone!"
1054,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,544.0,563.0,19.0," Hooray! We've made it to the end of another exciting StacQuest. If you like this StacQuest and want to see more, please subscribe. And if you want to support StacQuest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie or just donate."
1055,"Clustering with DBSCAN, Clearly Explained!!!",https://www.youtube.com/watch?v=RDZUdRSDOok,RDZUdRSDOok,563.0,569.0,6.0," The links are in the description below. All right, until next time, Quest on!"
1056,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,0.0,30.64,30.64," Step Quest, Step Quest, Step Quest, Hello and welcome to Step Quest. Stack Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill. Today we're going to be talking about the Canearest Neighbors algorithm, which is a super simple way to classify data."
1057,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,30.64,55.64,25.0," In a nutshell, if you already had a lot of data that defined these cell types, we could use it to decide which type of cell this guy is. Let's see it in action. Step one, start with a dataset with known categories. In this case, we have different cell types from an intestinal tumor."
1058,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,55.64,75.08,19.44," We then cluster that data. In this case, we used PCA. Step two, add a new cell with unknown category to the plot. We don't know this cell's category because it was taken from another tumor where the cells were not properly sorted."
1059,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,75.08,94.92,19.83999999999999," And so what we want to do is we want to classify this new cell. We want to figure out what cell it's most similar to and then we're going to call it that type of cell. Step three, we classify the new cell by looking at the nearest annotated cells. I.e. the nearest neighbors."
1060,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,94.92,119.08,24.16000000000001," If the K in K nearest neighbors is equal to one, then we will only use the nearest neighbor to define the category. In this case, the category is green because the nearest neighbor is already known to be the green cell type. If K equals 11, we would use the 11 nearest neighbors."
1061,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,119.08,143.32,24.239999999999995," In this case, the category is still green because the 11 cells that are closest to the unknown cell are already green. Now the new cell is somewhere more interesting. It's about halfway between the green and the red cells. If K equals 11, and the new cells between two or more categories, we simply pick the"
1062,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,143.32,165.64,22.319999999999997," category that gets the most votes. In this case, seven nearest neighbors are red. Three nearest neighbors are orange. One nearest neighbor is green. Since red got the most votes, the final assignment is red."
1063,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,165.64,191.56,25.920000000000016," This same principle applies to heat maps. This heat map was drawn with the same data and clustered using hierarchical clustering. If our new cell ended up in the middle of the light blue cluster and if K equals 1, we just look at the nearest cell and that cell is light blue. So we classify the unknown cell as a light blue cell."
1064,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,191.56,215.92,24.360000000000014," If K equals 5, we look at the 5 nearest cells, which are also light blue. So we'd still classify the unknown cell as light blue. If the new cell ended up closer to the edge of the light blue cells and K equals 11, then we take a vote. Seven nearest neighbors are light blue and four are light green."
1065,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,215.92,242.32,26.399999999999977," So we'd still go with light blue. If the new cell is right between two categories, well, if K is odd, then we can avoid a lot of ties. If we still get a tied vote, we can flip a coin or decide not to assign the cell to a category. Before we go, let's talk about a little machine learning slash data mining terminology."
1066,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,242.32,262.44,20.120000000000005," The data used for the initial clustering, the data where we know the category as an advance is called training data. Bam! A few thoughts on picking a value for K. There is no physical or biological way to determine the best value for K."
1067,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,262.44,293.68,31.24000000000001, So you may have to try out a few values before settling on one. Do this by pretending part of the training data is unknown. And then what you do is you categorize that unknown data using the K nearest neighbor algorithm and you assess how good the new categories match what you know already. Low values for K like K equals 1 or K equals 2 can be noisy and subject to the effects of outliers.
1068,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,293.68,315.56,21.879999999999995," Large values for K smooth over things, but you don't want K to be so large that a category with only a few samples in it will always be outvoted by other categories. Right! We've made it to the end of another exciting stat quest. If you like this stat quest, go ahead and subscribe to my channel and you'll see more"
1069,"StatQuest: K-nearest neighbors, Clearly Explained",https://www.youtube.com/watch?v=HVXime0nQeI,HVXime0nQeI,315.56,329.8,14.240000000000007," like it. And, if you have any ideas of things you'd like me to do a stat quest on, feel free to put those ideas in the comments. Okay, guess that's it. Tune in next time for another exciting stat quest."
1070,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,0.0,14.72,14.72," I'm at home during lockdown. Working on my stack quest, yeah. I'm at home during lockdown. Working on my stack quest, yeah. Stack quest."
1071,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,14.72,32.120000000000005,17.400000000000006," Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about naive bays and it's going to be clearly explained. This stack quest is sponsored by Jad Bio, just add data and their automatic machine-learning algorithms"
1072,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,32.120000000000005,50.96,18.84," will do the rest of the work for you. For more details, follow the link in the pinned comment below. Note, when most people want to learn about naive bays, they want to learn about the multinomial naive bays classifier, and that's what we talk about in this video."
1073,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,50.96,68.24,17.279999999999994," However, just know that there is another commonly used version of naive bays called Gaussian naive bays classification, and I cover that in a follow-up stack quest. So check that one out when you're done with this quest. Bam."
1074,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,68.24,90.03999999999999,21.8," Now, imagine we received normal messages from friends and family, and we also received spam unwanted messages that are usually scams who are unsolicited advertisements. And we wanted to filter out the spam messages. So the first thing we do is make a histogram of all the words"
1075,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,90.03999999999999,111.04,21.0," that occur in the normal messages from friends and family. We can use the histogram to calculate the probabilities of seeing each word given that it was in a normal message. For example, the probability we see the word deer, given that we saw it in a normal message,"
1076,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,111.04,127.2,16.159999999999997," is eight. The total number of times deer occurred in normal messages, divided by 17. The total number of words in all of the normal messages. And that gives a 0.47."
1077,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,127.2,147.64,20.44000000000001," So let's put that over the word deer so we don't forget it. Likewise, the probability that we see the word friend, given that we saw it in a normal message, is five, the total number of times friend occurred in normal messages, divided by 17,"
1078,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,147.64,169.08,21.44," the total number of words in all of the normal messages. And that gives a 0.29. So let's put that over the word friend so we don't forget it. Likewise, the probability that we see the word launch, given that it is in a normal message, is 0.18."
1079,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,169.08,186.52,17.439999999999998," And the probability that we see the word money, given that it is in a normal message, is 0.06. Now we make a histogram of all the words that occur in the spam. And calculate the probability of seeing the word deer,"
1080,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,186.52,207.16,20.639999999999983," given that we saw it in the spam. And that is two, the number of times we saw deer in the spam, divided by seven, the total number of words in the spam. And that gives a 0.29. Likewise, we calculate the probability of seeing"
1081,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,207.16,227.16,20.0," the remaining words given that they were in the spam. Now, because these histograms are taking up a lot of space, let's get rid of them, but keep the probabilities. Oh, no, it's the dreaded terminology alert."
1082,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,227.16,245.88,18.72," Because we have calculated the probabilities of discrete, individual words, and not the probability of something continuous, like weight or height, these probabilities are also called likelihoods. I mentioned this because some tutorials say these are probabilities"
1083,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,245.88,262.92,17.04000000000002," and others say they are likelihoods. In this case, the terms are interchangeable, so don't sweat it. We'll talk more about probabilities versus likelihoods when we talk about Gaussian naive bays in the follow-up quest."
1084,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,262.92,283.16,20.24000000000001," Now, imagine we got a new message that said, dear friend. And we want to decide if it is a normal message or spam. We start with an initial guess about the probability that any message, regardless of what it says, is a normal message."
1085,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,283.16,304.52000000000004,21.360000000000014," This guess can be any probability that we want, but a common guess is estimated from the training data. For example, since eight of the 12 messages are normal messages, our initial guess will be 0.67. So let's put that under the normal messages, so we don't forget it."
1086,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,304.52000000000004,322.48,17.95999999999998," Oh, no, it's another dreaded terminology alert. The initial guess that we observe a normal message is called a prior probability. Now we multiply the initial guess by the probability that the word dear occurs in a normal message"
1087,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,322.48,344.24,21.75999999999999," and the probability that the word friend occurs in a normal message. Now we just plug in the values that we worked out earlier and do the math. Bip, bu, bu, bu, bu, and we get 0.09. We can think of 0.09 as the score that dear friend gets"
1088,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,344.24,359.6,15.360000000000014," if it is a normal message. However, technically, it is proportional to the probability that the message is normal, given that it says dear friend. So let's put that on top of the normal messages,"
1089,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,359.6,376.68,17.079999999999984," so we don't forget. Now, just like we did before, we start with an initial guess about the probability that any message regardless of what it says is spam. And just like before, the guess can be any probability we want,"
1090,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,376.68,397.28,20.59999999999997," but a common guess is estimated from the training data. And since four of the 12 messages are spam, our initial guess will be 0.33. So let's put that under the spam so we don't forget it. Now we multiply that initial guess by the probability"
1091,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,397.28,416.28,19.0," that the word dear occurs in spam. And the probability that the word friend occurs in spam. Now we just plugged in the values that we worked out earlier and do the math. Bip, bu, bu, bu, bu, and we get 0.01."
1092,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,417.36,434.44,17.079999999999984," Like before, we can think of 0.01 as the score that dear friend gets if it is spam. However, technically, it is proportional to the probability that the message is spam, given that it says dear friend."
1093,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,435.6,458.04,22.44," And because the score we got for normal message, 0.09, is greater than the score we got for spam, 0.01, we will decide that dear friend is a normal message. Double bam. Now, before we move on to a slightly more complex situation,"
1094,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,458.2,476.88,18.680000000000007," let's review what we've done so far. We started with histograms of all the words in the normal messages and all of the words in the spam. Then we calculated the probabilities of seeing each word, given that we saw the word in either a normal message or spam."
1095,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,477.84,491.24,13.400000000000034," Then we made an initial guess about the probability of seeing a normal message. This guess can be anything between 0 and 1, but we based hours on the classifications in the training data set."
1096,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,492.24,505.96,13.71999999999997," Then we made the same sort of guess about the probability of seeing spam. Then we multiplied our initial guess that the message was normal by the probabilities of seeing the words dear and friend,"
1097,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,505.96,519.4399999999999,13.47999999999996," given that the message was normal. Then we multiplied our initial guess that the message was spam by the probabilities of seeing the words dear and friend, given that the message was spam."
1098,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,520.28,535.52,15.240000000000007," Then we did the math and decided that dear friend was a normal message because 0.09 is greater than 0.01. Now that we understand the basics of how naive, based classification works,"
1099,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,536.36,556.84,20.480000000000015," let's look at a slightly more complicated example. This time, let's try to classify this message, lunch, money, money, money, money. Note, this message contains the word money for times. And since the probability of seeing the word money"
1100,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,556.84,572.1600000000001,15.32000000000005," is much higher in spam than in normal messages, then it seems reasonable to predict that this message will end up being spam. So let's do the math. Calculating the score for a normal message"
1101,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,572.1600000000001,586.6400000000001,14.480000000000018," works just like before. We start with the initial guess, then we multiply it by the probability we see lunch, given that it is an normal message. And the probability we see money four times,"
1102,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,586.6400000000001,606.88,20.239999999999892," given that it is in a normal message. When we do the math, we get this tiny number. However, when we do the same calculation for spam, we get zero. This is because the probability we see lunch in spam is zero,"
1103,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,606.88,621.6800000000001,14.800000000000068," since it was not in the training data. And when we plug in zero for the probability we see lunch, given that it was in spam, then it doesn't matter what value we picked for the initial guess that the message was spam,"
1104,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,622.8000000000001,641.52,18.719999999999917," and it doesn't matter what the probability is that we see money given that the message was spam. Because anything times zero is zero. In other words, if a message contains the word lunch, it will not be classified as spam."
1105,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,642.48,656.8,14.319999999999936," And that means we will always classify the messages with lunch in them as normal, no matter how many times we see the word money. And that's a problem. To work around this problem,"
1106,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,656.8,675.44,18.6400000000001," people usually add one count represented by a black box to each word in the histograms. Note, the number of counts we add to each word is typically referred to with the Greek letter, alpha. In this case, alpha equals one,"
1107,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,675.44,695.9200000000001,20.480000000000015," but we could have said it to anything. Anyway, now when we calculate the probabilities of observing each word, we never get zero. For example, the probability of seeing lunch, given that it is in spam, is one divided by seven,"
1108,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,695.9200000000001,712.72,16.799999999999955," the total number of words in spam, plus four, the extra counts that we added. And that gives a zero point zero nine. Note, adding counts to each word does not change our initial guess that a message is normal,"
1109,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,713.44,731.84,18.399999999999977," or the initial guess that the message is spam. Because adding a count to each word did not change the number of messages in the training data set that are normal, or the number of messages that are spam, now when we calculate the scores for this message,"
1110,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,733.12,751.2800000000001,18.16000000000008," we still get a small number for the normal message, but now when we calculate the value for spam, we get a value greater than zero. And since the value for spam is greater than the one for a normal message, we classify the message as spam."
1111,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,752.5600000000001,772.64,20.079999999999927," Spam! Now let's talk about why naive bays is naive. The thing that makes naive bays so naive is that it treats all word orders the same. For example, the normal message score for the phrase,"
1112,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,772.64,797.04,24.399999999999977," dear friend, is the exact same for the score for friend, dear. In other words, regardless of how the words are ordered, we get zero point zero eight. Treating all word orders equal is very different from how you and I communicate. Every language has grammar rules and common phrases,"
1113,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,797.04,817.76,20.720000000000027," but naive bays ignores all of that stuff. Instead, naive bays treats language like it is just a bag full of words, and each message is a random handful of them. naive bays ignores all the rules because keeping track of every single reasonable phrase in a language would be impossible."
1114,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,818.9599999999999,843.5200000000001,24.560000000000173," That said, even though naive bays is naive, it tends to perform surprisingly well when separating normal messages from spam. In machine learning lingo, we'd say that by ignoring relationships among words, naive bays has high bias. But, because it works well in practice, naive bays has low variance."
1115,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,844.8000000000001,863.2800000000001,18.480000000000015," Shameless self-promotion. If you were not already familiar with the terms bias and variance, check out the quest. The link is in the description below. Triple spam. Oh no, it's one last shameless self-promotion."
1116,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,864.16,884.24,20.08000000000004," One awesome way to support stat quest is to purchase the naive bays stat quest study guide. It has everything you need to study for an exam or job interview. It's eight pages of total awesomeness. And while you're there, check out the other stat quest study guides. There's something for everyone."
1117,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,886.4,907.68,21.280000000000086," Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below."
1118,"Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=O2L2Uv9pdDA,O2L2Uv9pdDA,908.32,911.68,3.3600000000000136," All right, until next time, quest on."
1119,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,0.0,24.0,24.0," Pippu, pippu, pippu, pippu. Stachoast. Hello, I'm Josh Starman, welcome to Stachoast. Today we're going to talk about Gaussian naive maze, and it's going to be clearly explained. Note, this Stachoast assumes that you are already familiar with the main ideas behind multi-nomial naive maze."
1120,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,24.0,54.0,30.0," If not, check out the quest. The link is in the description below. The Stachoast also assumes that you are familiar with the log function, the normal or Gaussian distribution, and the difference between probability and likelihood. If not, check out the quest. The links are in the description below. Imagine we wanted to predict if someone would love the 1990 movie, Troll 2 or not."
1121,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,54.0,92.0,38.0," So we collected data from people that love Troll 2 and from people that do not love Troll 2. We measured the amount of popcorn they ate each day, how much soda pop they drank, and how much candy they ate. The main for popcorn for the people who love Troll 2 is 24. And the standard deviation is 4. And a Gaussian or normal distribution, with mean equals 24 and standard deviation equals 4, looks like this."
1122,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,92.0,120.0,28.0," Likewise, the average amount of popcorn for people who do not love Troll 2 is 4. And the standard deviation is 2. And that corresponds to this Gaussian or normal distribution. Now we calculate the mean and standard deviation for soda pop for people that love Troll 2, and draw the corresponding Gaussian distribution."
1123,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,120.0,150.0,30.0," Then we do the same thing for the people that do not love Troll 2. Lastly, we draw the Gaussian distributions for candy. Gaussian naive bays is named after the Gaussian distributions that represent the data in the training data set. Now someone new shows up and says they eat 20 grams of popcorn, and drink 500 milliliters of soda pop,"
1124,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,150.0,175.0,25.0," and eat 25 grams of candy every day. Let's use Gaussian naive bays to decide if they love Troll 2 or not. The first thing we do is make an initial guess that they love Troll 2. This guess can be any probability that we want, but a common guess is estimated from the training data."
1125,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,175.0,199.0,24.0," For example, since 8 of the 16 people in the training data love to Troll 2, the initial guess will be 0.5. So we'll put that up here so we don't forget. Likewise, the initial guess for does not love Troll 2 is 0.5. So let's put that here so we don't forget."
1126,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,199.0,232.0,33.0," Oh no, it's the dreaded terminology alert. The initial guesses are called prior probabilities. Now the score for loves Troll 2 is the initial guess that the person loves Troll 2 times the likelihood that they eat 10 grams of popcorn given that they love Troll 2. Note, the likelihood is the y-axis coordinate on the curve that corresponds to the x-axis coordinate."
1127,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,232.0,263.0,31.0, And we multiply that by the likelihood that they drink 500 milliliters of soda pop given that they love Troll 2. Times the likelihood that they eat 25 grams of candy given that they love Troll 2. The initial guess that someone loves Troll 2 is 0.5. The likelihood for popcorn is 0.06. The likelihood for soda pop is 0.004.
1128,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,263.0,300.0,37.0," And the likelihood for candy is a really, really small number. Note, when we get really, really small numbers, it's a good idea to take the log of everything to prevent something called underflow. The general idea of underflow is, every computer has a limit to how close a number can get to 0 before it can no longer accurately keep track of that number. When a number gets smaller than that limit, we run into underflow problems and errors occur."
1129,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,300.0,334.0,34.0," So we use the log function to avoid underflow. Note, any log will do, but the natural log or log-based E is the most commonly used log in statistics and machine learning. So we take the log of everything. And the log turns the multiplication into the sum of the individual logs. The log base E of 0.5 is negative 0.69."
1130,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,334.0,366.0,32.0," The log of 0.06 is negative 2.8. The log of 0.004 is negative 5.5. And the log of this really, really small number is negative 115. Now we just add this up, and we get negative 124. So the log of the Love's Troultool score is negative 124."
1131,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,366.0,391.0,25.0," Bam! Now let's calculate the score for not loving Troultool. We start with the initial guess that someone does not love Troultool too. Times the likelihood that they eat 20 grams of popcorn given that they do not love Troultool too. Times the likelihood that they drink 500 milliliters of soda pop,"
1132,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,391.0,413.0,22.0," times the likelihood that they eat 25 grams of candy. So let's plug in the numbers, bit, bit, bit, bit, bit, bit. And take the log of everything. And that turns the multiplication into the sum of logs. Now we just do the math, bit, bit, bit, bit."
1133,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,413.0,441.0,28.0," And we get negative 48. And since the score for does not love Troultool too is greater than the score for Love's Troultool. We will classify this person as someone who does not love Troultool. Double bam! Note, when we look at the raw data, it almost looks like we should have classified this person as someone who loves Troultool."
1134,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,441.0,473.0,32.0," After all, they ate a lot more popcorn than the average person who doesn't love Troultool. And they drank as much soda as the average person who loves Troultool. However, the big thing is that they ate a lot more candy than the people who loved Troultool. And the log of the likelihoods for candy are way different. And this difference is what made us classify the new person as someone who does not love Troultool."
1135,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,473.0,506.0,33.0," In other words, candy can have a much larger say in whether or not someone loves Troultool than popcorn and soda pop. And this means we might only need candy to make classifications. We can use cross-validation to help us decide which things popcorn, soda pop and or candy help us make the best classifications. Shameless self-promotion. If you don't already know about cross-validation, check out the quest."
1136,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,506.0,528.0,22.0," The link is in the description below. Triple bam! Oh no, it's another Shameless self-promotion. One awesome way to support StacQuest is to purchase the Galcian knife-based StacQuest Study Guide. It has everything you need to study for an exam or job interview."
1137,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,528.0,548.0,20.0," It's seven pages of total awesomeness. And while you're there, check out the other StacQuest Study Guys. There's something for everyone. Hey, we've made it to the end of another exciting StacQuest. If you like this StacQuest and want to see more, please subscribe."
1138,"Gaussian Naive Bayes, Clearly Explained!!!",https://www.youtube.com/watch?v=H3EjCKtlVog,H3EjCKtlVog,548.0,565.0,17.0," And if you want to support StacQuest, consider contributing to my Patreon campaign. Becoming a channel member by one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time, quest on."
1139,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,0.0,33.92,33.92," I like decision trees. How about you, stat quest? Hello, I'm Josh Darmer and welcome to stat quest. Today we're going to talk about decision and classification trees and they're going to be clearly explained. Here is a simple decision tree. If a person wants to learn about decision trees, then they should watch this stat quest. In contrast, if a person does not want to learn about decision trees, then check out the latest"
1140,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,33.92,67.76,33.84," Justin Bieber video instead. In general, a decision tree makes a statement and then makes a decision based on whether or not that statement is true or false. It's no big deal. When a decision tree classifies things into categories, it's called a classification tree. At when a decision tree predicts numeric values, it's called a regression tree. In this case, we're using diet to predict a numeric value for mouse size."
1141,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,69.36,101.68,32.32000000000001," Note, for the remainder of this video, we are going to focus on classification trees. However, if you want to learn more about regression trees, fear not. There's a whole stat quest dedicated to regression trees. The link is in the description below. Now, here's a more complicated classification tree. It combines numeric data with yes no data. So it's okay to mix data types in the same tree."
1142,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,102.88,128.64,25.760000000000005," Also, notice that the tree asks about exercising multiple times and that the amount of time exercising isn't always the same. So numeric thresholds can be different for the same data. Lastly, the final classification can be repeated. For the most part, classification trees are pretty easy to work with. You start at the top"
1143,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,129.44,154.0,24.56," and work your way down and down until you get to a point where you can't go any further. And that's how you'll classify something. Note, so far I've been labeling the arrows with true or false. But usually, it is just assumed that if a statement is true, you go to the left. And if a statement is false, you go to the right."
1144,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,155.2,180.07999999999998,24.879999999999995," So, sometimes you see true and false labels, sometimes you don't. It's no big deal. Oh no, it's the dreaded terminology alert. The very top of the tree is called the root node or just the root. These are called internal nodes or branches. Branches have arrows pointing to them and they have arrows pointing away from them."
1145,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,181.28,199.6,18.32000000000002," Lastly, these are called leaf nodes or just leaves. Leaves have arrows pointing to them, but there are no arrows pointing away from them. Bam. Now that we know how to use an interpret classification trees, let's learn how to build one from raw data."
1146,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,201.04,225.76,24.71999999999997," This data tells us whether or not someone loves popcorn, whether or not they love soda, their age, and whether or not they love the 1991 blockbuster, cool as ice, starring vanilla ice. So we will use this data to build this classification tree that predicts whether or not someone loves cool as ice."
1147,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,227.12,250.56,23.440000000000023," Now, pretend you've never seen this tree before. And let's see how to build a tree, starting with just data. The first thing we do is decide whether loves popcorn, love soda, or age, should be the question we ask at the very top of the tree. To make that decision, we'll start by looking at how well loves popcorn predicts"
1148,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,250.56,271.6,21.04000000000002," whether or not someone loves cool as ice. To do this, we'll make a super simple tree that only asks if someone loves popcorn. And then we'll run the data down the tree. For example, the first person in the data set loves popcorn. So they go to the leaf on a left."
1149,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,272.48,299.04,26.559999999999945," And because they do not love cool as ice, we'll keep track of that by putting a one under the word no. The second person in the data set also loves popcorn. So they also go to the leaf on a left. And because they also do not love cool as ice, we increment no to two. The third person does not love popcorn."
1150,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,300.32,322.0,21.680000000000007," So they go to the leaf on the right. And because they love cool as ice, we put a one under the word yes. Likewise, we run the remaining rows down the tree, keeping track of whether or not each one loves cool as ice. Bam. Now let's do the exact same thing for love soda."
1151,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,322.64,351.84,29.19999999999999," Looking at the two little trees, we see that neither one does a perfect job predicting who will and who will not love cool as ice. Specifically, these three leaves contain mixtures of people that do and do not love cool as ice. Dread, it's another terminology alert. Because these three leaves all contain a mixture of people who do and do not love cool as"
1152,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,352.79999999999995,384.8,32.0," they are called impure. In contrast, this leaf only contains people who do not love cool as ice. Because both leaves and the loves popcorn tree are impure and only one leaf in the love soda tree is impure. It seems like love soda does a better job predicting who will and who will not love cool as ice. But it would be nice if we could quantify the differences between loves popcorn and love soda."
1153,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,386.08,418.96,32.88000000000005," The good news is that there are several ways to quantify the impurity of the leaves. One of the most popular methods is called genie impurity, but there are also fancy sounding methods like entropy and information gain. However, numerically, the methods are all quite similar. So we will focus on genie impurity, since not only is it very popular, I think it is the most straightforward. So let's start by calculating the genie impurity for loves popcorn."
1154,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,419.92,455.12,35.19999999999999," To calculate the genie impurity for loves popcorn, we start by calculating the genie impurity for the individual leaves. The genie impurity for the leaf on the left is, 1 minus the probability of yes squared minus the probability of no squared. So we start out with 1. Then we subtract the squared probability of someone in this leaf loving cool as ice. Which is 1. The number of people in the leaf who loved cool as ice"
1155,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,455.2,490.0,34.80000000000001," divided by the total number of people in the leaf, 4. And then the whole term is squared. Lastly, we subtract the squared probability of someone in this leaf not loving cool as ice. Which is 3. The number of people in the leaf who did not love cool as ice divided by the total number of people in the leaf, squared. And when we do the math, we get 0.375. So let's put 0.375 under the leaf on the left so we"
1156,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,490.0,527.4399999999999,37.43999999999994," don't forget it. Now let's calculate the genie impurity for the leaf on the right. Just like before, we start out with 1. Then we subtract the squared probability of someone in this loving cool as ice and the squared probability of someone in this leaf not loving cool as ice. And when we do the math, we get 0.444. Now because the leaf on the left has 4 people in it and the leaf on the right only has 3 people in it, the leaves do not represent the same number of people."
1157,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,528.4000000000001,556.48,28.079999999999927," Thus, the total genie impurity is the weighted average of the leaf impurities. We start by calculating the weight for the leaf on the left. The weight for the left leaf is the total number of people in the leaf, 4, divided by the total number of people in both leaves, 7. Then we multiply that weight by its associated genie impurity, 0.375."
1158,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,557.7600000000001,586.48,28.719999999999917," Now we add the weighted impurity for the leaf on the right, which is the total number of people in the leaf, 3, divided by the total number of people in both leaves, 7, times the associated genie impurity, 0.444. And when we do the math, we get 0.405. So the genie impurity for loves popcorn is 0.405."
1159,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,587.9200000000001,614.5600000000001,26.639999999999983," Likewise, the genie impurity for love soda is 0.214. Now we need to calculate the genie impurity for age. However, because age contains numeric data and not just yes, no values, calculating the genie impurity is a little more involved. The first thing we do is sort the rows by age from lowest value to highest value."
1160,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,615.52,641.1199999999999,25.59999999999991," Then we calculate the average age for all adjacent people. Lastly, we calculate the genie impurity values for each average age. For example, to calculate the genie impurity for the first value, we put age less than 9.5 in the root. And because the only person with age less than 9.5 does not love cool as eyes,"
1161,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,642.08,668.64,26.559999999999945," we put a 0 under yes and a 1 under no. Then, everyone with age greater than or equal to 9.5 goes to the leaf on the right. Now we calculate the genie impurity for the leaf on the left and get 0. And this makes sense because every single person in this leaf does not love cool as eyes. So there is no impurity."
1162,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,669.52,701.2,31.680000000000064," Then we calculate the genie impurity for the leaf on the right and get 0.5. Now we calculate the weighted average of the two impurities to get the total genie impurity. And we get 0.429. Likewise, we calculate the genie impurities for all of the other candidate values. These two candidate thresholds, 15 and 44 are tied for the lowest impurity, 0.343."
1163,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,702.24,725.52,23.279999999999973," So we can pick either one, in this case, we'll pick 15. However, remember that we are comparing genie impurity values for age, loves popcorn, and loves soda. To decide which features should be at the very top of the tree. Earlier we calculated the genie impurity values for loves popcorn and love soda."
1164,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,726.32,743.9200000000001,17.600000000000023," And now we have the genie impurity for age. And because love soda has the lowest genie impurity overall, we know that its leaves have the lowest impurity. So we put love soda at the top of the tree. Bam!"
1165,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,744.24,766.9599999999999,22.719999999999917," Now, the four people that love soda go to a node on the left. And the people that do not love soda go to a node on the right. Now let's focus on the node on the left. All four people that love soda are in this node. Three of these people love cool as ice."
1166,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,768.16,788.0799999999999,19.91999999999996, And one does not. So this node is impure. So let's see if we can reduce the impurity by splitting the people that love soda based on love's popcorn or age. We'll start by asking the four people that love soda if they also love popcorn.
1167,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,789.52,808.96,19.44000000000005," Because two of the four people that love soda also love popcorn, they end up in the leaf on the left. The remaining two people that love soda but do not love popcorn end up on the right. And the total genie impurity for this split is 0.25."
1168,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,810.16,834.08,23.920000000000076, So let's put 0.25 here so we don't forget. Now we test different age thresholds just like before. Only this time we only consider the ages of people who love soda. And age less than 12.5 gives us the lowest impurity zero. Because both leaves have no impurity at all.
1169,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,835.0400000000001,866.4,31.3599999999999, So let's put 0 here. Now because 0 is less than 0.25 we will use age less than 12.5 to split this node into leaves. Note these are leaves because there is no reason to continue splitting these people into smaller groups. Likewise this node consisting of the three people who do not love soda is also a leaf because there is no reason to continue splitting these people into smaller groups.
1170,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,868.0,896.0,28.0, Now there is just one last thing we need to do before we are done building this tree. We need to assign output values for each leaf. Generally speaking the output of a leaf is whatever category that has the most values. In other words because the majority of the people and these leaves do not love cool as eyes. The output values are does not love cool as eyes.
1171,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,897.36,918.96,21.600000000000023, And because the majority of the people in this leaf love cool as eyes. The output value is loves cool as eyes. Hey we finished building a tree from this data. Double bound. Now if someone new comes along and we want to predict if they will love cool as eyes
1172,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,919.84,938.88,19.039999999999964, then we run the data down our tree. And because they love soda they go to the left. And because they are 15 so age less than 12.5 is false they end up in this leaf. And we predict that they will love cool as eyes. Triple bound.
1173,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,940.0,964.8,24.800000000000068, Okay now that we understand the main ideas of how to build and use classification trees let's discuss one technical detail. Remember when we built this tree only one person in the original data set made it to this leaf. Because so few people made it to this leaf it's hard to have confidence that it will do a great job making predictions with future data.
1174,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,965.52,980.72,15.199999999999932," And it is possible that we have overfit the data. Note if the term overfit is new to you. Don't don't free Greek. Oh, oh, oh, oh. Instead check out the stack quest on bias and variance in machine learning."
1175,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,981.84,1004.08,22.24000000000001, Regardless in practice there are two main ways to deal with this problem. One method is called pruning and there's a whole stack quest dedicated to it so check it out. Alternatively we can put limits on how trees grow. For example by requiring three or more people per leaf. Now we end up with an impure leaf.
1176,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,1004.88,1034.24,29.360000000000014, But also a better sense of the accuracy of our prediction because we know that only 75% of the people in the leaf loved cool as ice. Note even when a leaf is impure we still need an output value to make a classification. And since most of the people in this leaf love cool as ice that will be the output value. Also note when we build a tree we don't know in advance if it is better to require three
1177,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,1034.24,1058.16,23.920000000000076," people per leaf or some other number. So we test different values with something called cross validation and pick the one that works best. And if you don't know what cross validation is check out the quest. Bam, now it's time for some shameless self promotion. If you want to review statistics and machine learning offline, check out the stack quest study guides"
1178,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,1058.16,1081.2,23.039999999999964," as stack quest.org. There's something for everyone. Hooray we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, consider contributing to my Patreon campaign. Becoming a channel member by one or two of my original songs or a t-shirt or a hoodie"
1179,"Decision and Classification Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=_L39rN6gz7Y,_L39rN6gz7Y,1081.2,1087.76,6.559999999999945," or just donate. The links are in the description below. All right, until next time, quest on."
1180,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,0.0,26.0,26.0," When you've got too much data, don't freak out. When you've got missing data, don't freak out. You've got stat quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to be talking about decision trees part two, feature selection and missing data."
1181,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,27.0,58.0,31.0," This is just a short and sweet stat quest to touch on a few topics we didn't get to in the original stat quest on decision trees. In the first stat quest on decision trees, we started with a table of data. And built a decision tree that gave us a sense of how likely a patient might have heart disease if they have other symptoms. We first asked if a patient had good blood circulation. If so, we then asked if they had blocked arteries."
1182,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,58.0,92.0,34.0," And if so, we then asked if they had chest pain. If so, there's a good chance that they have heart disease since 17 people with similar answers did, and only three people with similar answers did not. If they don't have chest pain, there's a good chance that they do not have heart disease. However, remember that if someone had good circulation and did not have blocked arteries, we did not ask about chest pain because there was less impurity in our results if we didn't."
1183,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,92.0,121.0,29.0," In other words, we calculated the impurity after separating the patients using chest pain, then we calculated impurity without separating. And since the impurity was lower when we didn't separate, we made it a leaf node. Now, imagine if chest pain never gave us a reduction in impurity score. If this were the case, we would never use chest pain to separate the patients,"
1184,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,121.0,149.0,28.0," and chest pain would not be part of our tree. Now, even though we have data for chest pain, it is not part of our tree anymore. All that's left are good circulation and blocked arteries. This is a type of automatic feature selection. However, we could also have created a threshold such that the impurity reduction has to be large enough to make a big difference."
1185,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,149.0,174.0,25.0," As a result, we end up with simpler trees that are not overfit. Oh no! Some jargon just snuck up on us. Overfit means our tree does well with the original data, the data we used to make the tree, but doesn't do well with any other data set. Decision trees have the downside of often being overfit."
1186,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,174.0,200.0,26.0," Requiring each split to make a large reduction in impurity helps a tree from being overfit. So, in a nutshell, that's what feature selection is all about. Now let's talk about missing data. In the first video on decision trees, we calculated impurity for blocked arteries. Boed boed boed boed boed boed boed boed boed boed boed boed boed boed boed boed."
1187,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,200.0,229.0,29.0," And we skipped this patient, since we didn't know if they had blocked arteries or not. But it doesn't have to be that way. We could pick the most common option. If, overall, yes occurred more times than no, we could put yes here. Alternatively, we could find another column that has the highest correlation with blocked arteries and use that as a guide."
1188,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,229.0,254.0,25.0," In this case, chest pain in blocked arteries are often very similar. The first patient has no in both categories. The second patient has yes in both categories. The third patient has no in both categories. And so, for the fourth patient, since chest pain is yes,"
1189,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,254.0,280.0,26.0," we'll make blocked arteries yes as well. Now, imagine we had weight data instead of blocked artery data. We could replace this missing value with the mean or the median. Alternatively, we could find another column that has the highest correlation with weight. In this case, height is highly correlated with weight."
1190,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,280.0,300.0,20.0," And then we can do a linear regression on the two columns. And use the least squares line to predict the value for weight. So, you can see that if we're missing some data, there are a lot of ways to guess at what it might be. Bam! Hurray!"
1191,"StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data",https://www.youtube.com/watch?v=wpNl-JwwplA,wpNl-JwwplA,300.0,315.0,15.0," We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more like it, please subscribe. And if you have any ideas for future stat quest, well, put them in the comments below. Until next time, quest on!"
1192,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,0.0,29.0,29.0," Regression tree is for you and for me. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about regression trees and they're going to be clearly explained. This Stat Quest assumes you are already familiar with the tradeoff that plays all of machine learning, the bias variance tradeoff."
1193,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,29.0,52.0,23.0," And the basic ideas behind decision trees. And the basic ideas behind regression. If not, check out the quests, the links are in the description below. Now, imagine we developed a new drug to cure the common cold. However, we don't know the optimal dosage to give patience."
1194,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,52.0,79.0,27.0," So we do a clinical trial with different dosages and measure how effective each dosage is. If the data looked like this, and in general, the higher the dose, the more effective the drug. Then we could easily fit a line to the data. And if someone told us they were taking a 27-millogram dose,"
1195,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,79.0,102.0,23.0," we could use the line to predict that a 27-millogram dose should be 62% effective. However, what if the data looked like this? Low dosages are not effective. Moderate dosages work really well. Some of higher dosages work at about 50% effectiveness,"
1196,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,102.0,130.0,28.0," and high dosages are not effective at all. In this case, fitting a straight line to the data will not be very useful. For example, if someone told us they were taking a 20-millogram dose, then we would predict that a 20-millogram dose should be 45% effective. Even though the observed data says it should be 100% effective."
1197,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,130.0,154.0,24.0," So we need to use something other than a straight line to make predictions. One option is to use a regression tree. Regression trees are a type of decision tree. In a regression tree, each leaf represents a numeric value. In contrast, classification trees have true or false in their leaves,"
1198,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,154.0,183.0,29.0," or some other discrete category. With this regression tree, we start by asking if the dosage is less than 14.5. If so, then we are talking about these six observations in the training data. And the average drug effectiveness for these six observations is 4.2%. So the tree uses the average value, 4.2%,"
1199,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,183.0,215.0,32.0," as its prediction for people with dosages less than 14.5. On the other hand, if the dosage is greater than or equal to 14.5, and greater than or equal to 29, then we are talking about these four observations in the training data set. And the average drug effectiveness for these four observations is 2.5%. So the tree uses the average value, 2.5%,"
1200,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,215.0,250.0,35.0," as its prediction for people with dosages greater than or equal to 29. Now, if the dosages greater than or equal to 14.5, and less than 29, and greater than or equal to 23.5, then we are talking about these five observations in the training data set. And the average drug effectiveness for these five observations is 52.8%. So the tree uses the average value, 52.8%,"
1201,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,250.0,283.0,33.0," as its prediction for people with dosages between 23.5 and 29. Lastly, if the dosages greater than or equal to 14.5, and less than 29, and less than 23.5, then we are talking about these four observations in the training data set. And the average drug effectiveness for these four observations is 100%. So the tree uses the average value, 100%,"
1202,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,283.0,313.0,30.0," as its prediction for people with dosages between 14.5 and 23.5. Since each leaf corresponds to the average drug effectiveness in a different cluster of observations, the tree does a better job reflecting the data than the straight line. At this point, you might be thinking, there are aggression tree is cool, but I can also predict drug effectiveness just by looking at the graph."
1203,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,313.0,342.0,29.0," For example, if someone said they were taking a 27-milligram dose, then just by looking at the graph, I can tell that the drug will be about 50% effective. So why make a big deal about the regression tree? When the data are super simple, and we are only using one predictor, dosage, to predict drug effectiveness, making predictions by I isn't terrible."
1204,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,342.0,370.0,28.0," But when we have three or more predictors, like dosage, age, and sex, to predict drug effectiveness, drawing a graph is very difficult if not impossible. In contrast, a regression tree easily accommodates the additional predictors. For example, if we wanted to predict the drug effectiveness for this patient, we would start by asking if they are older than 50,"
1205,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,370.0,392.0,22.0," and since they are not over 50, we follow the branch on the right, and ask if their dosage is greater than or equal to 29. And since their dosage is not greater than or equal to 29, we follow the branch on the right and ask if they are female. And since they are female, we follow the branch on the left,"
1206,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,392.0,419.0,27.0," and predict that the dosage will be 100% effective. And that's not too far off from the truth, 98%. Okay, now that we know that regression trees can easily handle complicated data, let's go back to the original data with just one predictor dosage. And talk about how to build this regression tree from scratch."
1207,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,419.0,446.0,27.0," And since regression trees are built from the top down, the first thing we do is figure out why we start by asking if dosage is less than 14.5. Going back to the graph of the data, let's focus on the two observations with the smallest dosages. Their average dosage is 3, and that corresponds to this dotted red line."
1208,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,446.0,473.0,27.0," Now we can build a very simple tree that splits the observations into two groups, based on whether or not dosage is less than 3. The point on the far left is the only one with dosage less than 3. And the average drug effectiveness for that one point is 0. So we put 0 in the leaf on the left side for when dosage is less than 3."
1209,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,473.0,509.0,36.0," All of the other points have dosages greater than or equal to 3. And the average drug effectiveness for all of the points with dosages greater than or equal to 3 is 38.8. So we put 38.8 in the leaf on the right side for when dosage is greater than or equal to 3. The values in each leaf are the predictions that this simple tree will make for drug effectiveness. For example, this point on the far left has dosage less than 3."
1210,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,509.0,542.0,33.0," And the tree predicts that the drug effectiveness will be 0. The prediction for this point, drug effectiveness equal 0, is pretty good, since it is the same as the observed value. In contrast, for this point, which has dosage greater than 3, the tree predicts that the drug effectiveness will be 38.8. And that prediction is not very good, since the observed drug effectiveness is 100%."
1211,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,542.0,580.0,38.0," Note, we can visualize how bad the prediction is by drawing a dotted line between the observed and predicted values. In other words, the dotted line is a residual. For each point in the data, we can draw its residual, the difference between the observed and predicted values, and we can use the residuals to quantify the quality of these predictions. Starting with the only point with dosage less than 3, we calculate the difference between its observed drug effectiveness, 0,"
1212,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,581.0,614.0,33.0," and the predicted drug effectiveness 0, and then square the difference. In other words, this is the squared residual for the first point. Now we add the square residuals for the remaining points, with dosages greater than or equal to 3. In other words, for this point, we calculate the difference between the observed and predicted values and square it, and then add it to the first term."
1213,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,614.0,655.0,41.0," Then we do the same thing for the next point, and the next point, and the rest of the points. Until we have added squared residuals for every point, thus, to evaluate the predictions made when the threshold is dosage less than 3, we add up the squared residuals for every point, and get 27,468.5. Note, we can plot the sum of squared residuals on this graph."
1214,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,655.0,683.0,28.0," The y-axis corresponds to the sum of squared residuals, and the x-axis corresponds to dosage thresholds. In this case, the dosage threshold was 3, but if we focus on the next two points in the graph, and calculate their average dosage, which is 5, then we can use dosage less than 5 as a new threshold."
1215,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,684.0,712.0,28.0," And using dosage less than 5 gives us new predictions, and new residuals. And that means we can add a new sum of squared residuals to our graph. In this case, the new threshold, dosage less than 5, results in a smaller sum of squared residuals. And that means using dosage less than 5 as the threshold resulted in better predictions overall."
1216,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,712.0,739.0,27.0," Bam! Now let's focus on the next two points. Calculate their average, which is 7, and use dosage less than 7 as a new threshold. Again, the new threshold gives us new predictions, new residuals, and a new sum of squared residuals."
1217,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,739.0,766.0,27.0," Now shift the threshold over to the average dosage for the next two points, and add a new sum of squared residuals to the graph. And we repeat until we have calculated the sum of squared residuals for all of the remaining thresholds. Bam! Now we can see the sum of squared residuals for all of the thresholds."
1218,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,766.0,800.0,34.0," And dosage less than 14.5 has the smallest sum of squared residuals. So dosage less than 14.5 will be the root of the tree. In summary, we split the data into two groups by finding the threshold that gave us the smallest sum of squared residuals. Bam! Now let's focus on the six observations with dosage less than 14.5 that ended up in the node to the left of the root."
1219,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,800.0,833.0,33.0," In theory, we could split these six observations into two smaller groups just like we did before. By calculating the sum of squared residuals for different thresholds, and choosing the threshold with the lowest sum of squared residuals. Note, this observation has dosage less than 14.5, and does not have dosage less than 11.5. So it is the only observation to end up in this node."
1220,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,833.0,869.0,36.0," And since we can't split a single observation into two groups, we will call this node a leaf. However, since the remaining five observations go to the other node, we can split them once more. Now we have divided the observations with dosage less than 14.5 into three separate groups. These two leaves only contain one observation each, and cannot be split into smaller groups. In contrast, this leaf contains four observations."
1221,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,869.0,919.0,50.0," That said, those four observations all have the same drug effectiveness, so we don't need to split them into smaller groups. So we are done splitting the observations with dosage less than 14.5 into smaller groups. Note, the predictions that this tree makes for all observations with dosage less than 14.5 are perfect. In other words, this observation has 20% drug effectiveness, and the tree predicts 20% drug effectiveness, so the observed and predicted values are the same. This observation has 5% drug effectiveness, and that's exactly what the tree predicts."
1222,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,919.0,948.0,29.0," These four observations all have zero percent drug effectiveness, and that's exactly what the tree predicts. Is that awesome? No. When a model fits the training data perfectly, it probably means it is over fit, and will not perform well with new data. In machine learning lingo, the model has no bias, but potentially large variants."
1223,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,948.0,977.0,29.0," Is there a way to prevent our tree from overfitting the training data? Yes, there are a bunch of techniques. The simplest is to only split observations when there are more than some minimum number. Typically, the minimum number of observations to allow for a split is 20. However, since this example doesn't have many observations, I set the minimum to 7."
1224,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,978.0,1029.0,51.0," In other words, since there are only six observations with dosage less than 14.5, we will not split the observations in this node. Instead, this node will become a leaf, and the output will be the average drug effectiveness for the six observations with dosage less than 14.5, 4.2%. Now we need to figure out what to do with the remaining 13 observations with dosages greater than or equal to 14.5. Since we have more than 7 observations on the right side, we can split them into two groups. And we do that by finding the threshold that gives us the smallest sum of squared residuals."
1225,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1030.0,1064.0,34.0," Note, there are only four observations with dosage greater than or equal to 29. Thus, there are only four observations in this node. Thus, we will make this a leaf because it contains fewer than 7 observations. And the output will be the average drug effectiveness for these four observations, 2.5%. Now we need to figure out what to do with the nine observations with dosages between 14.5 and 29."
1226,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1064.0,1114.0,50.0," Since we have more than 7 observations, we can split them into two groups, by finding the threshold that gives us the minimum sum of squared residuals. Note, since there are fewer than 7 observations in each of these two groups, this is the last split, because none of the leaves have more than 7 observations in them. So we use the average drug effectiveness for the observations with dosages between 14.5 and 23.5, 100% as the output for the leaf on the right. And we use the average drug effectiveness for observations with dosages between 23.5 and 29.52.8% as the output for the leaf on the left."
1227,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1114.0,1146.0,32.0," Since no leaf has more than 7 observations in it, we're done building the tree. And each leaf corresponds to the average drug effectiveness from a different cluster of observations. Double-bound. So far, we have built a tree using a single predictor dosage to predict drug effectiveness. Now let's talk about how to build a tree to predict drug effectiveness using a bunch of predictors."
1228,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1146.0,1177.0,31.0," Just like before, we will start by using dosage to predict drug effectiveness. Thus, just like before, we will try different thresholds for dosage and calculate the sum of squared residuals at each step, and pick the threshold that gives us the minimum sum of squared residuals. The best threshold becomes a candidate for the root. Now we focus on using age to predict drug effectiveness."
1229,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1177.0,1201.0,24.0," With sex, there is only one threshold to try. So we use that threshold to calculate the sum of squared residuals at each step, and pick the one that gives us the minimum sum of squared residuals. The best threshold becomes another candidate for the root. Now we focus on using sex to predict drug effectiveness."
1230,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1201.0,1224.0,23.0," With sex, there is only one threshold to try. So we use that threshold to calculate the sum of squared residuals, and that becomes another candidate for the root. Now we compare the sum of squared residuals SSRs for each candidate, and pick the candidate with the lowest value."
1231,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1224.0,1251.0,27.0," Since age greater than 50 had the lowest sum of squared residuals, it becomes the root of the tree. Then we grow the tree just like before, except now we compare the lowest sum of squared residuals from each predictor. And just like before, when a leaf has less than a minimum number of observations, which is usually 20, but we are using 7, we stop trying to divide them."
1232,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1251.0,1274.0,23.0," Triple bound. In summary, regression trees are a type of decision tree. In a regression tree, each leaf represents a numeric value. We determine how to divide the observations by trying different thresholds and calculating the sum of squared residuals at each step."
1233,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1274.0,1310.0,36.0," The threshold with the smallest sum of squared residuals becomes a candidate for the root of the tree. If we have more than one predictor, we find the optimal threshold for each one, and pick the candidate with the smallest sum of squared residuals to be the root. When we have fewer than some minimum number of observations in a node, 7 in this example, but more commonly 20, then that node becomes a leaf."
1234,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1310.0,1332.0,22.0," Otherwise, we repeat the process to split the remaining observations. Until we can no longer split the observations into smaller groups. And then we are done. Hooray, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe."
1235,"Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=g9c66TUylZ4,g9c66TUylZ4,1332.0,1347.0,15.0," And if you want to support stack quest, consider contributing to my Patreon campaign, buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time, quest on."
1236,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,0.0,21.3,21.3," Smelly-stat, smelly-stat, how are they training you? I hope they're using stat quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to talk about how to prune regression trees. There are several methods for pruning regression trees."
1237,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,21.3,44.82,23.52," The one we'll talk about in this quest is called cost-complexity pruning, aka weakest-link pruning. We'll start by giving a general overview of how cost-complexity pruning works, and then we'll describe how it's used to build regression trees. Note, this stat quest assumes that you are already familiar with regression trees."
1238,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,44.82,68.16,23.34," If not, check out the quest. The link is in the description below. Also note, this stat quest assumes that you are already familiar with cross-validation. If not, check out the quest. In the stat quest on regression trees, we had this data. Given different drug dosages on the x-axis,"
1239,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,68.16,90.34,22.180000000000007," we measured the drug effectiveness on the y-axis. When the drug dosage was too low, or too high, the drug was not effective. Medium dosages were very effective, and moderately high dosages were moderately effective."
1240,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,90.34,112.7,22.36," We then fit a regression tree to the data, and each leaf corresponded to the average drug effectiveness from a different cluster of observations. This tree does a pretty good job reflecting the training data, because each leaf represents a value that is close to the data."
1241,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,112.8,138.5,25.700000000000003," However, what if these red circles were testing data? These three observations are pretty close to the predicted values. So their residuals, the difference between the observed and predicted values, are not very large. Similarly, the residuals for these observations in the testing data are relatively small."
1242,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,138.6,164.4,25.80000000000001," However, the residuals for these observations are larger than before. And the residuals for these observations are much larger. These four observations from the training data with 100% drug effectiveness now look a little bit like outliers. And that means that we overfit the regression tree to the training data."
1243,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,164.4,194.2,29.80000000000001," One way to prevent overfitting a regression tree to the training data is to remove some of the leaves. And replace the split with a leaf that is the average of a larger number of observations. Now, all of the observations between 14.5 and 29 go to the leaf on the far right. The large residuals tell us that the new tree doesn't fit the training data as well as before."
1244,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,194.9,221.9,27.0," But the new subtree does a much better job with the testing data. Thus, the main idea behind pruning a regression tree is to prevent overfitting the training data so that the tree will do a better job with the testing data. Bam! Note, if we wanted to prune the tree more, we could remove these two leaves"
1245,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,222.0,251.0,29.0," and replace the split with a leaf that is the average of a larger number of observations. And we could then remove these two leaves and replace the split with a leaf that is the average of all of the observations. So the question is, how do we decide which tree to use? In this stat quest, we will answer that question with cost complexity pruning."
1246,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,251.1,279.0,27.90000000000001," The first step in cost complexity pruning is to calculate the sum of the squared residuals for each tree. In this example, we'll start with the original full-sized tree. Here is the original full-sized tree. The sum of the squared residuals for the observations with dosages less than 14.5 is... ...due to...due to...due to...due to..."
1247,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,279.0,311.5,32.5, 320.8. So we'll save that sum of squared residuals underneath the corresponding leaf. The sum of squared residuals for observations with dosages greater than or equal to 29 is 75. The sum of squared residuals for observations with dosages greater than or equal to 23 and less than 29 is 148.8.
1248,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,311.5,347.5,36.0," And the sum of squared residuals for observations with dosages greater than or equal to 14.5 and less than 23.5 is zero. Thus, the total sum of squared residuals for the whole tree is 320 plus 75 plus 148.8 plus zero equals 543.8. So let's put SSR equals 543.8 on top of the original full-sized tree."
1249,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,347.5,394.0,46.5," Now let's calculate the sum of squared residuals for the subtree with one fewer leaf. Going back to the data, the sum of squared residuals for when dosages is less than 14.5 is the same as before. And it's the same for when dosage is greater than or equal to 29. But we have to calculate a new sum of squared residuals for when the dosage is between 14.5 and 29. Thus, the total sum of squared residuals for this tree is 320 plus 75 plus 599.8 which equals 544.8."
1250,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,394.0,444.0,50.0," So let's put SSR equals 5494.8 on top of the subtree with three leaves. Similarly, the sum of squared residuals for the subtree with two leaves is 19.243.7. So we put SSR equals 19.243.7 on top of the subtree with two leaves. Lastly, the sum of squared residuals for the subtree with only one leaf is 28.897.2. So let's put SSR equals 28.897.2 on top of the subtree with one leaf."
1251,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,444.0,485.0,41.0," Note, the sum of squared residuals is relatively small for the original full-sized tree. But each time we remove a leaf, the sum of squared residuals gets larger and larger. However, we knew that was going to happen because the whole idea was for the pruned trees to not fit the training data as well as the full-sized tree. So how do we compare these trees? We just link pruning works by calculating a tree score that is based on the sum of squared residuals for the tree or subtree."
1252,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,485.0,520.0,35.0," And a tree complexity penalty that is a function of the number of leaves or terminal nodes in the tree or subtree. The tree complexity penalty compensates for the difference in the number of leaves. Note, alpha is a tuning parameter that we find using cross-validation and we'll talk more about it in a bit. For now, let's let alpha equal 10,000. Now let's calculate the tree score for each tree."
1253,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,520.0,563.0,43.0," The tree score for the original full-sized tree is the total SSR for the tree, which is 543.8, plus 10,000 times t, the total number of leaves, which is 4. So the tree score for the original full-sized tree is 40,543.8. Now let's save the tree score below the tree and calculate the tree score for the subtree with one fewer leaf. The sum of squared residuals for this subtree is 5494.8."
1254,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,563.0,600.0,37.0," And since there are three leaves, t equals 3, and the total tree score equals 35,494.8. The tree score for the subtree with two leaves is... 39,243.7. Lastly, the tree score for the subtree with only one leaf is... 38,897.2."
1255,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,600.0,633.0,33.0," Note, because alpha equals 10,000, the tree complexity penalty for the tree with one leaf was 10,000. And the tree complexity penalty for the tree with two leaves was 20,000. And the tree complexity penalty for the tree with three leaves was 30,000. And the tree complexity penalty for the original full size tree with four leaves was 40,000. Thus, the more leaves the larger the penalty."
1256,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,634.0,664.0,30.0," Now that we have calculated tree scores for all of the trees, we pick this subtree because it has the lowest tree score. Double-bowl. Note, if we set alpha equals 22,000, and calculate the tree scores, then we would use the subtree with only one leaf because it has the lowest tree score."
1257,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,664.0,701.0,37.0," Thus, the value for alpha makes a difference in our choice of subtree. So let's talk about how to build a pruned regression tree, and how to find the best value for alpha. First, using all of the data, build a full-sized regression tree. Note, this full-sized tree is different than before because it was fit to all of the data, not just the training data. Also note, this full-sized tree has the lowest tree score when alpha equals zero."
1258,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,701.0,737.0,36.0," This is because when alpha equals zero, the tree complexity penalty becomes zero. And the tree score is just the sum of the squared residuals, and, as we saw earlier, all of the subtries will have larger sum of squared residuals. So let's put alpha equals zero here to remind us that this tree has the lowest tree score when alpha equals zero. Now we will increase alpha until pruning leaves will give us a lower tree score."
1259,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,737.0,781.0,44.0," In this case, when alpha equals 10,000, we'll get a lower tree score if we remove these leaves, and use this subtree. Now we increase alpha again until pruning leaves will give us a lower tree score. In this case, when alpha equals 15,000, we will get a lower tree score if we remove these leaves, and use this subtree instead. In the end, different values for alpha give us a sequence of trees from full-sized to just a leaf. Now go back to the full data set and divide it into training and testing data sets."
1260,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,781.0,825.0,44.0," And just using the data set to be a little bit more important than the other one. So let's use that set and divide it into training and testing data sets. And just using the training data, use the alpha values we found before to build a full tree and a sequence of subtries that minimize the tree score. In other words, when alpha equals zero, we build a full-sized tree since it will have the lowest tree score. However, when alpha equals 10,000, we will get a lower tree score if we prune these leaves, and use this tree instead."
1261,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,825.0,859.0,34.0," And when alpha equals 15,000, we will get a lower tree score if we prune these leaves, and use this tree instead. Now calculate the sum of squared residuals for each new tree using only the testing data. In this case, the tree with alpha equals 10,000 had the smallest sum of squared residuals for the testing data. Now we go back to the first step of the tree. And then we can see that the tree is going to be a little bit more important than the other tree."
1262,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,859.0,896.0,37.0," And we had the smallest sum of squared residuals for the testing data. Now we go back and create new training data and new testing data. And just using the new training data, build a new sequence of trees from full-sized to a leaf, using the alpha values we found before. We calculate the sum of squared residuals using the new testing data. This time, the tree with alpha equals 0 had the lowest sum of squared residuals."
1263,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,896.0,940.0,44.0," Now we just keep repeating until we have done 10 fold cross validation. And the value for alpha that, on average, gave us the lowest sum of squared residuals with the testing data is the final value for alpha. In this case, the optimal trees built with alpha equals 10,000 had on average the lowest sum of squared residuals. So alpha equals 10,000 is our final value. Lastly, we go back to the original trees and subtries made from the full data and pick the tree that corresponds to the value for alpha that we selected."
1264,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,940.0,964.0,24.0," This subtree will be the final pruned tree. Triple BAM. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member,"
1265,"How to Prune Regression Trees, Clearly Explained!!!",https://www.youtube.com/watch?v=D0efHEJsfHo,D0efHEJsfHo,964.0,975.0,11.0," buying one or two of my original songs or a T-shirt or a hoodie or just donate. The links are in the description below. All right, until next time, quest on."
1266,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,0.0,27.4,27.4," One hot label, target, encoding, yeah, stack West. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about one hot label and target encoding, and they're going to be clearly explained. You don't have to worry about the details of scaling your stuff up in the cloud,"
1267,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,27.64,48.12,20.48," because line it will take care of it for you, bam. This stack quest is also brought to you by the letters, A, B and C. A, always, B, B, C, curious, always B, curious. Imagine we had this data and we wanted to use favorite color and height"
1268,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,48.12,77.24000000000001,29.12000000000001," to predict if someone loves troll too, which is a movie that some people love and some people don't. In this case, favorite color has three discrete values, blue, red, and green. Now, in theory, discrete features, like favorite color, are fine for most machine learning algorithms. But in practice, a lot of popular machine learning algorithms, including neural networks, do not work well with them."
1269,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,77.24000000000001,101.4,24.16," As a result, discrete data are often converted into numerical values before being used for machine learning. One popular method for converting discrete variables or features into numbers is to use something called one-hot encoding. When we have three or more options for a discrete variable, and in the case of favorite color, we have three options."
1270,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,101.4,130.16,28.75999999999999," We start by creating a new column for each option. In this case, that means creating three new columns, blue, red, and green. Now, in the blue column, we set the value to one if we have blue in the original favorite color column. And we set the remaining values to zero. Likewise for the red column, we set the value to one at the one when we have red in n is the original favorite color column."
1271,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,130.16,155.24,25.080000000000013," We set the remaining values to zero. Lastly, for the green column, we set the value to one if we had green in the original favorite color column. And we set the remaining values to zero. Note, the last column, loves true all two, is also discrete, but it only has two options, yes and no. So we simply replace yes with one and no with zero."
1272,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,156.28,179.96,23.680000000000007," And now, all of the columns in our new data set are numeric. And can be used with algorithms that don't do well with discrete data, like neural networks or xg boost. Bam! Using one hot encoding to convert discrete data into numeric data works fine when we don't have too many options. In this case, we only have three options for favored color."
1273,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,180.52,212.84,32.31999999999999," So we replace favored color with three new columns. But when we have a lot of options, for example, if we had a column of postal codes and there are 41,683 postal codes in the United States, then we would end up replacing the one postal code column with 41,683 new columns, which might make the data difficult to work with. So, when we have tons of options for discrete variable, one alternative to one hot encoding is to simply"
1274,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,212.84,240.36,27.519999999999985," assign numbers from low to high to each option. So, in this case, we might set blue to zero, red to one, and green to two. And just like before, we could convert love's troll 2 to be numeric by setting yes to one, and no to zero. Simply converting the discrete values to random numbers, like what we did here, is called label encoding."
1275,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,240.92,272.12,31.200000000000017," And again, just like before, all of the columns are now numeric and we can run the data through a neural network. Double bam! Note, one thing that people don't like about using label encoding is that the numbers we use are just arbitrary. And some machine learning algorithms will treat the order of the numbers as if they might mean something, and that can cause problems. For example, a decision tree splitting on favored color would be forced to group red and green together,"
1276,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,272.76,295.72,22.95999999999998," or blue and red together. Simply because of the random numbers we assign to each color. So, instead of just picking random numbers to represent the options, blue, red, and green, we can calculate the mean value of the target, the thing we want to predict, which in this case is Love's troll 2, for each option."
1277,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,296.84,321.4,24.56," For example, of the three people that like the color blue, only one of them loves troll 2. So the mean value for blue is one divided by 3 or 0.33. So we replace blue with 0.33. Likewise, because only one person likes red and they do not love troll 2, the mean for red is zero."
1278,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,322.12,343.64,21.519999999999985," So we replace red with zero. Lastly, because two of three people who like green also love troll 2, we replace green with zero.67. Because we use the target, the thing we want to predict to determine what values to replace the discrete options, this method is called target encoding."
1279,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,344.6,367.96,23.35999999999996," That being said, we've only talked about the simplest type of target encoding. A more commonly used version of target encoding deals with the fact that we only had one person who liked the color red. And that means we only used one person to determine the mean value for red. And thus, we don't have a lot of data supporting the use of zero to replace red."
1280,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,368.92,386.68,17.75999999999999," In contrast, both blue and green have more data, three people each, supporting the values we use to replace them. Because less data supports the value we replaced red with, we have less confidence that we replaced red with the best value than we have for blue and green."
1281,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,387.8,412.28,24.480000000000015," So, in order to deal with this, target encoding usually is done using a weighted mean that combines the mean for a specific option, like red, with the overall mean of the target, which is love's troll 2. For example, in order to use the fancier target encoding with our data, we start by plugging in the mean of the target for blue, one divided by three."
1282,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,413.08,435.64,22.56," Then, because three people were used to calculate the mean for blue, we plug in three for M. Then we plug in the overall mean for the target Love's troll 2, three divided by seven, because overall, three of the seven people love troll 2. Now we just need to pick a value for M, the weight for the overall mean."
1283,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,436.6,458.04,21.44," M is a user defined parameter or hyper parameter, and in this example, we set M equal to two. Setting M equal to two means we need at least three rows of data before the option mean, the mean we calculated for blue becomes more important than the overall mean. Now we just do the math and get 0.37."
1284,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,458.92,474.84,15.920000000000016," So we plug in 0.37 for blue. Now we calculate the weighted mean for red, B. Report. And we get 0.29. So we plug in 0.29 for red."
1285,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,475.72,501.48,25.75999999999999," Lastly we calculate the weighted mean for green, and again, in 0.57, so we plug in 0.57 for green, bam. Now let's compare the target encoding when we used the weighted mean to the target encoding without the weighted mean. The target encoding for blue and green are similar to what they were before."
1286,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,502.2,535.24,33.04000000000002," And this makes sense because we had a relatively large amount of data for both blue and green. In contrast, with the weighted mean, the value for red is much closer to the overall mean than before. And this also makes sense because we have so little data for red, only one row. In a way, we can think of the overall mean as our best guess given no data. However, as we get more data, more rows for each option, we use the data more rather than our best guess"
1287,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,535.24,564.76,29.519999999999985," to determine the target encoding. Note, if you're familiar with Bayesian methods, this approach may look familiar because a lot of Bayesian methods boil down to calculating weighted average between a guess and the data. As a result, some people call this Bayesian mean encoding. Triple-bound. Note, some of you may have noticed that we are using the target, the thing we want to predict, to modify the values in favorite color."
1288,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,565.48,596.44,30.96000000000004," And doing this sort of thing is a data science no-no, that we call data leakage. Data leakage results in models that work great with training data, but not so well with testing data. In other words, data leakage results in models that are overfit. The good news is that there are a bunch of relatively simple ways to avoid data leakage, or at least reduce the amount of data leakage so that you can use target encoding without overfitting your model."
1289,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,597.32,632.28,34.95999999999992," One of the most popular methods to reduce leakage is called Kfold Target Encoding. So let's go back to the original data set that had blue, red, and green categories for favored color. And talk about Kfold Target Encoding. Note, the word fold in Kfold Target Encoding refers to splitting the data into equal sized subsets. And the K refers to how many subsets we create. For example, if we did two fold Target Encoding, then we would divide the data into two"
1290,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,632.36,670.2,37.84000000000003," equal sized subsets. Note, because we have an uneven number of rows, we just made the subsets as similar in size as possible. Now, to make it easier to keep track of things, let's label the first subset A and the second subset B. Now, to Target Encode blue in subset A, we ignore the target values in this subset. In other words, we ignore the values for Love's Troll 2 in this subset. And instead, plug the target values from subset B into the weighted mean equation."
1291,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,670.92,702.36,31.44000000000005," We start by plugging in the subset B mean of the target for blue, zero divided by one. Because the one person in subset B that likes blue does not love Troll 2. Then, because there is only one person in subset B that likes blue, we plug in one for N. Then we plug in the overall mean for the target in subset B, one divided by three, because, overall, one of the three people in subset B loves Troll 2."
1292,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,703.24,745.64,42.39999999999998," And, just like we did before, we'll set M equal to 2. Now, we just do the math and get 0.22. So, we plug in 0.22 for the two rows and subset A with blue. Now, we need to Target Encode the one row with blue in subset B. So, we ignore the target values in this subset. And, instead, plug the target values from subset A into the equation for the weighted mean. Then, we do the math and get 0.5. So, we plug in 0.5 for blue, but only in subset B."
1293,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,746.84,778.76,31.91999999999996," Note, you may have noticed that the different subsets have different values for blue. This is okay because favorite color is becoming a continuous variable just like height. Now, let's encode the color red in subset A. So, we ignore the target values in subset A, and, instead, plug the target values from subset B into the equation for the weighted mean. Now, because subset B doesn't have anyone who likes the color red, the mean for red is 0."
1294,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,779.48,812.04,32.559999999999945," And, n equals 0. The other values are the same as before. And, we end up replacing red in subset A with 0.33. Likewise, green in subset A uses the target values in subset B, and turns into 0.42. And, green in subset B, uses the target values from subset A, and turns into 0.67. Now that each color in each subset has been encoded, we merge the subsets back together."
1295,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,812.76,841.4000000000001,28.6400000000001," And, we're done. Note, this process reduces data leakage because the rows do not use their own target values to calculate their encoding. Bam! Now, going back to the original data with blue, red, and green, if we set K equal to 7, then we would divide the data into 7 subsets. Now, targeting coding the first subset, which consists of a single row with favorite color equal to blue,"
1296,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,842.36,875.88,33.51999999999998," means we ignore its target value. And, use the target values from all of the other subsets to calculate the weighted mean. Likewise, encoding the other subsets would use all of the other target values except their own. Note, when we use all of the target values except one to do the encoding, it's called Leave 1 Out Target Encoding. A quick scan of the internet shows that some people are successful with Leave 1 Out Target Encoding, and other people are successful with setting K equal to 5."
1297,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,876.68,904.36,27.680000000000064," Triple Bam! Now it's time for some. Shameless Self Promotion! If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book, the StatQuest Illustrated Guide to Machine Learning at StacQuest.org. There's something for everyone. Hooray, we've made it to the end of another exciting StatQuest. If you like this StatQuest and want to see more,"
1298,"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",https://www.youtube.com/watch?v=589nCGeWG1w,589nCGeWG1w,904.44,922.52,18.079999999999927," please subscribe. And if you want to support StatQuest, consider contributing to my Patreon campaign, becoming a channel member by one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time, Quest ON!"
1299,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,0.0,34.96,34.96," The Cision trees from Dot Defendish and Python We've gone to it today. Hit, hit, who ready? Statquist. Right. Well, thank you guys very much for joining me for my webinar in decision trees from Start to Finish and Python. I'm going to share the screen right here. Can you guys all see that? I'm sharing this Jupiter notebook."
1300,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,37.04,65.04,28.000000000000007," Hope everyone can see it. Yes, I got it. Yes, that's great. So what we're going to go through today is this Jupiter notebook and I'm going to email you every single one of you guys a copy of this. It will include the Jupiter notebook, which has to be opened up within Jupiter, but also a copy that can be run directly in Python. So if you don't have Jupiter installed on your computer, but you have Python, you still should be able to run"
1301,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,65.04,101.68,36.63999999999999," everything that we talk about and all of the writing in here. There's lots of writing, lots of comments. All of that will be in comments in the code. So you will get everything one way or the other. Yeah, so today we're going to use Scikit Learn and Cost Complexity Pruning to build this classification tree right here, which uses continuous and categorical data from the UCI machine learning repository to predict whether or not a patient has heart disease. Note all"
1302,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,101.68,137.36,35.67999999999999," these things are hyperlinks. So if you want to learn more about the UCI machine learning repository or you want to learn more about the specific data set we're using, you can click on the links and learn more. So there are lots of hyperlinks in here. Anyways, the classification trees are an exceptionally useful machine learning method. When you need to know what how the decisions are being made. For example, if you have to justify the predictions to your boss,"
1303,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,138.24,175.52,37.28," classification trees are a good method because each step in the decision making process is easy to understand. Now, I know classification trees, some people think they're not the sexiest of machine learning that methods out there, but they are super practical and are actually very frequently used in the medical profession because the decisions you can trace exactly what the rationale is for for everything and that's important in certain fields. And also, like I'm just for like exploring"
1304,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,175.52,204.64,29.119999999999976," data and terms of like looking to see which features or variables are the most important. So there are a lot of cool things we can do with decision trees. And so we're going to learn all about them. So we're going to learn about importing data, which is not very exciting, but it's important. We're going to talk about how to deal with missing data, identifying it, dealing with it. We're going to talk about formatting the data for decision trees. Specifically, we're going to"
1305,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,204.64,243.84,39.20000000000002," talk about one hot and coding. We're also going to build a pre-liminary classification tree and it's going to not be very good, but then we're going to optimize that thing. Using cost complexity pruning, and then once we've optimized it, we're going to build draw, interpret, and evaluate the final classification tree. And this all covers a lot of material, so we're going to move pretty quickly. However, you have questions at the end of each section, feel free to to bring them up."
1306,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,244.72,282.96000000000004,38.24000000000004," You can also, you should all have my email, the statquest.bam at gmail.com. You can email me questions later. So with that, let's just dive right in. Oh, by the way, once you get this code, I strongly encourage you to play around with it. Playing with the code is the best way to learn from it. I've got alternative ways to do things in the comments so you can try the way we're going to do it today, but you can also try alternative versions and see if you get the same results or not."
1307,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,284.24,319.6,35.360000000000014," So, someone just asks for going to share this notebook link. I'm going to actually email everyone this notebook. You're going to get a copy of it plus all the code. So you'll get everything. So don't worry about if I move fast and you're having trouble taking notes or something like that, don't worry about it. You're going to get everything. And I've commented and written everything up with lots and lots of details. Some of which will cover today, and there's actually stuff we won't get to."
1308,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,319.6,353.92,34.31999999999999," So you can read more and learn more once you get the notebook. Okay, so the very first thing we do is load in a bunch of Python modules. Python itself, as many of you may know, but some of you might not, just gives us a basic programming language. These modules give us extra functionality to import the data, clean it up, and format it, and then build, evaluate, and draw the classification tree. Note, I'm doing everything in Python 3, and if you've got your own installation of Python"
1309,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,354.0,385.04,31.039999999999964," going, you're going to need certain versions of of the modules. These are listed here. And I've got a little blurb on how to update modules. Should you need to do that. But since I don't have to do that, we're just going to skip right here. But these are the modules. We're going to load pandas. We're going to do that for manipulating data and for one hot encoding. We're loading numpy to calculate the mean and standard deviation."
1310,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,386.16,420.56,34.400000000000034," And then we're importing mat plot live to draw some graphs. And then a bunch of scikit-learn modules and bits to do classification trees and confusion matrices and cross validation. So with a Jupyter notebook, some of you guys may know this, some of you may not. If you want to run code, you just click in the pane that has the code and it gets highlighted. And then you can go up here and you click that play button and it'll run everything."
1311,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,420.56,453.36,32.799999999999955," There's also key combinations for doing the same thing, or you can go to a run menu. So we could do a control enter to run this selected cell or we could click here. When we do, you will see a star show up here and that means Python is working. And when it's done working, it'll put a number there. The actual number is not very important. So don't worry about that. But when you try to run this, you may see a little star for a little"
1312,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,453.36,482.48,29.120000000000005," bit, but when the star turns into a number, you should be good to go. So leave imported the modules. So now we're ready to move on to the next thing. We're going to import the data. We're going to load in a data set from the UCI machine learning repository. Specifically, we're going to use the heart disease data set. And this data set will allow us to predict if someone has a heart disease based on their sex, aid, blood pressure, and a bunch"
1313,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,482.48,516.5600000000001,34.08000000000004," of other metrics. So we're going to use pandas to read that data frame in. When it does, it returns a data frame, which is a lot like a spreadsheet. The data are organized in rows and columns and each row can contain a mixture of text and columns. And this standard variable name for a data frame is the initials DF for data frame. So that's what we're going to use here. We're going to try to stick to the Python connection conventions. So we've got this code and we've got data"
1314,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,516.5600000000001,550.56,33.999999999999886," frame that's going to be our new DF. That's going to be our data frames, our new variable. And we're setting it to the data we're going to read in using reads CSV, which is a pandas function. The data is, I'm also going to email you the data is relatively small files. So you'll get the data as well. However, as you see below, we can also read it directly from the machine learning repository by just plugging in the URL for the data. So I'm going to run this code, bam."
1315,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,551.4399999999999,585.4399999999999,34.0," Okay. Now we've got the data loaded into a data frame called DF. And we're going to look at the first five rows using the head function. So we've got DF dot head and that will print out the first five rows. I'm going to use control. Enter. That prints that out. By the way, different computers, I've got a Macintosh that I'm using right now. If you're on Windows or a PC or Linux, it may be a different key combination to run the code. Just go up to the run menu and figure out"
1316,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,585.44,618.8000000000001,33.360000000000014," what it is on your platform of choice. All right. So we see the first six rows, they're kind of a mess. We got row numbers and column numbers. However, we do not have column names. And since nice column names would make it easier to know how to format the data. We're going to replace the column numbers with the following column names. I got these names, by the way, off the UCI website. So I didn't just make them up. We've got age sex, chest pain, resting blood pressure,"
1317,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,619.76,655.9200000000001,36.16000000000008," cholesterol, fasting blood sugar, resting electrocardiographic results. This maximum heart rate achieved exercise induced and gyna. And a bunch of other things. The point is, we're going to set the column names by with DF dot columns. And they're going to be set to this array of column names. And then once we've set the column names, we're going to print out the first five rows like we just did. And hopefully we'll see nice, pretty looking column names. So let's run"
1318,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,655.9200000000001,696.8000000000001,40.88," this ban. Okay. So now instead of column numbers, we've got nice column names, which are much easier for remember and manipulate. Okay. So now that we've got the data in our data frame and we've got nice column names. We are ready to identify with missing data. I identify and deal with missing data. I apologize. So I've broken this into two parts. The first part is going to focus on identifying missing data. And then the second part is going to be focused on dealing with missing data."
1319,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,697.04,727.2,30.16000000000008," And unfortunately, the biggest part of any data analysis project is making sure that the data is correctly formatted and fixing it when it is not. The first part of this process is identifying and dealing with missing data. missing data is simply a blank space or a circuit value like NA that indicates that we failed to collect data for one of the features. For example, if we forgot to ask someone's age or forgot to write it down, then we would have a blank space in the"
1320,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,727.2,764.4799999999999,37.27999999999986," data set for that person's age. There are two main ways to deal with missing data. One is just to remove if it's a single column or row that has a lot of missing data, we can remove that row of data or we can remove that column. Alternatively, we can impute the values that are missing. And in this context, impute is just a fancy way of saying we can make an educated guess about what the value should be. So if we were missing a value for age, instead of throwing out that"
1321,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,764.4799999999999,797.44,32.96000000000015," entire row of data, we might fill in a missing value with the average age or the median, or use some more sophisticated approach to guess an appropriate value. So first, what we're going to do is we're going to see what kind of data we have in our data frame. And we'll do that with the D types by looking at D types. So we got D data frame dot D types. And we run that and that tells us that age is a float, which is good because they are just"
1322,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,797.44,830.56,33.11999999999989," supposed to be a number. Sex is a float, maybe that's good, maybe that's not. So we got a bunch of floats. And then we've got CA and Fowl, both have the object data type. And one column, HD, which is just short for hard disease, whether or not someone has hard disease, is an integer. So the fact that the CA and Fowl columns have object data types suggests that there's something funny going on in them. Object data types are used when there is a mixture of things."
1323,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,830.56,859.3599999999999,28.799999999999955," Like a mixture of numbers and letters. And in theory, both CA and Fowl should just have a few values representing different categories. And I know this from the UCI website. So you can if you want to learn more about this data set. I've actually got more about the data set further down in, but you can also read about it on the UCI website. Anyways, so what we're going to do to investigate what's going on in these columns is we're going to"
1324,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,859.36,893.84,34.48000000000002," print out their unique values. So we're going to start with CA. So we've got data frame and then in square brackets and single pose. We've identified the column we are interested in. We're interested in the CA column. And we're interested in seeing the unique values. We're going to use the unique function to print out those values. So we run it. And we see that CA contains numbers 0, 3, 2 and 1 and question marks. The numbers represent a number of blood vests, vessels that"
1325,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,893.84,928.56,34.719999999999914," were lit up during fluoroscopy, which is some sort of diagnostic procedure. I actually don't know the details about it. It's not super important to be able to follow along with what's going on in this webinar. And the question marks, however, those represent missing data. Now we're going to look at the unique values in the column called FAO, which is short for Falium Heart scan. And we're doing the exact same code that we had before. We've got the data frame and in square brackets and"
1326,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,928.56,963.44,34.88000000000011," single pose, we specify which column we are interested in looking at and then we print out the unique values. And again, we see the Falcon Tains a mixture of numbers representing the different diagram noces for the Falium Heart scan and question marks. Which represent different numbers represent missing values. So now that we've identified some missing values, we need to deal with them. And that leads us to missing data part 2, dealing with missing data."
1327,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,965.28,995.76,30.479999999999905," Since scikit learns classification trees, do not support data sets with missing values, we need to figure out what to do with these question marks. We can either delete these patients from the training data set or compute the missing data or compute values for the missing data. So first we're going to see how many rows contain missing values. We do that. We're going to count the number of rows. So we're going to use the LEN, which is short for length, a function,"
1328,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,995.76,1039.44,43.680000000000064," and we're specifying with this line. We want to look at rows in the data frame, the location of which. This is true. So is there a question mark for the CA value or that's a pipe, which represents a logical or or a bitwise or or we want the rows that have a question mark in the foul spot. So we're going to run this code and we see that there are only six rows that have missing values. And since that's not very many, we're just going to print them out. So we're"
1329,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1039.44,1070.4,30.96000000000004," going to run the exact same code we just ran. However, we're not going to wrap it in the length function of the LEN function. So we're not going to count the number of rows. We're just going to print them out. So let's run that. Here we are. We can see a question mark here and the foul column, a question mark here, a question mark here, blah, blah, blah. We've got these question marks. So that's what the data looks like with the question marks."
1330,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1072.08,1104.4,32.319999999999936," Now we're going to see how many rows are in the full data set. So we're using that length function again. Only this time we're not specifying which rows we want to look at. We're just saying, let's count all of the rows. And when we do that, we see we've got a 303 rows. And so six of the 3003 rows are 2% contain missing values. And since 297 is still plenty of data relatively speaking to build this classification tree. We're going to remove the rows with missing"
1331,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1104.4,1137.44,33.04000000000019," values rather than try to impute their values. Note, imputing missing values is a big topic that we will tackle in another webinar because there's a whole lot's of ways to do it. There's lots of nuance. And so that's real high on my to-do list for what the next webinar is going to be. So by taking the easier route, by just deleting the rows of missing values, we can stay focused on what we want to talk about today, which are just decision trees because we still have a lot to talk"
1332,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1137.44,1172.32,34.87999999999988," about what decision trees. So what we're going to do is we're going to remove the rows with missing values by selecting all of the rows that do not contain question marks in either the CA or Thalcolbs. So this looks a lot. This looks very similar to the code we were just running when we wanted to print out the rows. However, instead of looking for rows that have the question mark, we're looking for rows that do not. So that not equals does not match a question mark. And we want to do that for both"
1333,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1172.4,1218.72,46.32000000000016," of these columns. And we want to use the logical and to get all of the rows that do not have question mark here or here. We want to play everything, but those rows. So we'll run that and since and we, oh, by the way, we saved the result in a new variable called DF, no missing. So this is our data frame with no missing values. And since DF, no missing has six fewer rows in the original data frame, it should have 297 rows. We can verify that with this command. And we see that we got"
1334,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1218.72,1248.56,29.83999999999992," the math works out. So hooray, the math works out. However, we can also make sure that CA no longer contains question marks for printing its unique values. So this is just like what we did before. Only this time we're calling on DF no missing instead of just DF alone. And we see that we just have numbers. And there's no question mark here. So that's good. Now we're going to do the same thing for foul."
1335,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1249.28,1288.0,38.72000000000003," Again, we see we just have the numbers. So bam, we have verified that data frame no missing mark, the data frame with no missing values does not contain any missing values. Note, CA and foul still have the object data type. That's okay. Now we're ready to format the data for making a classification trick. All right, the first thing we need to do when we format the data for a classification tree is"
1336,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1288.0,1323.12,35.12000000000012," split the data into two parts. We want to have the columns of data that we will use to make classifications. And we want the one column of data that we want to predict with the data over here. And we're going to use the conventional notation of capital X to represent the columns that we will use to make the classifications and predictions. And lowercase y to represent the thing we want to predict. In this case, we want to predict HD, the column specified by HD, which is short for hard"
1337,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1323.12,1358.96,35.83999999999992," disease. And the reason why we deal with missing data before we split it into x and y is that if we need to remove rows splitting afterwards ensures that each row and x will correctly correspond to a row and y. We do it the other way around everything's going to get mixed up. So what we're going to do is we're going to copy all of the rows, excuse me, all of the columns except for the one column that has that is named HD."
1338,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1359.76,1391.6,31.840000000000146," And I've got some alternative ways to do this code. So you can you can play around with it once you get the Jupyter notebook. And then what we're going to do is once we copy everything but HD, we're going to look at the first five rows just to verify that we did it correctly. So there we go. And we see in the right side where we no longer have that column called HD. So that worked out well. And now we are, we're going to just copy the HD column into our new variable called Y."
1339,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1395.28,1433.76,38.48000000000002," All right, okay. Now that we've created x, which has the data we want to use to make predictions and y, which has the data we want to predict, we are ready to continue formatic x so that it is suitable for making a decision tree. All right, here we get to the fun part, one hot and coding. A lot of you people may already know what one hot and coding is. If you don't, don't worry, this is something we're going to go into in detail. Now we have to split the data frame into, now that we have"
1340,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1433.76,1471.52,37.75999999999999," split the data frame into two pieces x, which can hinge the data we want to use to make classifications and y, which contains the known classifications in our training data set, we need to take a closer look at the variables in x. The list below tells us what each variable represents and the type of data float or categorical, it should contain. Okay, so, so we've got age, which should be a float because that can be any number, and we've got sex, that should be a"
1341,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1471.52,1502.96,31.44000000000005," category that should be a value of either zero for females and one for males. We have chest pain, which should also be a category. We've got four different categories. We'll go through those categories in more detail later. But we see in this list we've got resting blood pressure and that's just a number, so we're going to save that as a float. Serum cholesterol, that's also just a number, so that's a float, and then we've got categories and different things like that. However, just"
1342,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1502.96,1545.6,42.63999999999987," we review, let's go look at the data types and x to remember how Python is seeing the data. So this is how the data should be considered as, as some things are floats and some things are categories. But when we go to xd types or x dot d types, and we see what data type each column has, we see that a lot of these things that are supposed to be categories are like slope is supposed to be a category, but we have it stored as a float. Okay, so there's a problem with that."
1343,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1546.56,1579.04,32.48000000000002," However, before we get to the problem, I'm going to say that we see that age, resting blood pressure, cholesterol, and the latch are all float 64, which is good, because we want them to be floating point numbers. That's the way the data is supposed to be. All of the other columns, however, need to be inspected to make sure that the only contain reasonable values in some of them need to change. This is because while scikit-learned decision trees natively supports continuous data,"
1344,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1579.04,1613.6799999999998,34.63999999999987," like resting blood pressure and maximum heart rate, they do not natively support categorical data like chest pain, which contains four different categories. Thus, in order to use categorical data with scikit-learned decision trees, we have to use a trick that converts a column with categorical data into multiple columns of binary values, and this trick is called one-hot encoding. Okay, at this point you may be wondering what's wrong with treating categorical data like continuous"
1345,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1613.6799999999998,1652.88,39.20000000000027," data, and to answer that question up, we're going to look in an example. For the CP chest pain column, we have four options. One typical angina, two, a typical angina, three, non-anginal pain, and four asymptomatic. Now, if we treated these values one, two, three, and four, like continuous data, then we would assume that four, which means asymptomatic, is more similar to three, which means non-anginal pain than it is to one or two, which are other types of chest pain."
1346,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1654.0800000000002,1684.9599999999998,30.87999999999965," That means the decision tree would be more likely to cluster the patients with fours and threes together than patients with fours and ones together. In contrast, if we treat these numbers like categorical data, then we treat each one as a separate category that is no more or less similar to any of the other categories. Thus, a likelihood of clustering patients with fours and threes is the same as clustering fours and ones, and that approaches more reasonable."
1347,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1684.9599999999998,1717.28,32.320000000000164," Partly because I don't really know what these, what this means is our one and two more somewhat. I don't know because I don't know when he's one-hot encoding to force psychic learn to treat this like categorical data rather than continuous data. Now, let's inspect and if needed, convert the columns that contain categorical and integer data into the correct data types. We'll start with a chest pain by inspecting its unique values."
1348,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1720.0,1757.6,37.59999999999991," Okay, so the good news is that chest pain only contains the values it is supposed to contain. One, two, three, and four. So we'll convert it using one-hot encoding into a series of columns that only contain zeros and ones. Note, I've got a long description on the different ways to do one-hot encoding. There's two major methods, one is called column transformer from psychic learn in the others called get dummies from pandas. Both methods have pros and cons. We're going to use"
1349,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1757.6,1791.6,34.0," get dummies today because I think it's the best way to teach how to do one-hot encoding. I think it by far is the best way to teach it. However, column transformer is more commonly used in production systems. So make sure you're familiar with both. And one way to do that is just to read this right up that I've provided you. It provides you with all the pros and cons of the different methods. So at your leisure, you can go through that."
1350,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1793.44,1826.08,32.63999999999987," However, so we're just going to use get dummies because I think it's better for teaching. So what we're going to do is we're going to start with chest pain. And just to see what happens when we convert chest pain, we're going to do this without saving the results. Just so we can see how get dummies works. So what we're doing is we're going to use this panda function, get dummies, or passing it our data frame, which we're calling access. That's"
1351,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1826.08,1856.08,30.000000000000227," the data we're using to make production. And we're specifying one column. We're just going to specify the chest pain column. We could specify a bunch of columns and convert them all at once. But right now we're just going to specify chest pain. And we're going to print out the first five rows to see what it does to the chest pain column. So let's run that. And we can see in the print out above that get dummies puts all of the columns, it does not"
1352,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1856.08,1893.28,37.200000000000045, process in front. And it puts chest pain at the end right here. So everything we did not touch is up on the left side and everything that we did touch was with just chest pain is on the right side. It also splits chest pain into four columns. Just like we expected it to do. Chest pain 1.0 is one for any patient that scored a one for chest pain and zero for all other patients. Chest pain 2.0 is one for every patient that scored two for chest pain and zero for all
1353,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1893.28,1929.04,35.75999999999976," other patients. Likewise we have chest pain 3 and chest pain 4. And this accounts for all four different options we had for chest pain. So now that we see how get dummies works we're going to use it on the four categorical columns that have more than two categories and we're going to save the result this time. We're not just going to print it out. Okay, note in a real situation and not a tutorial like this what you should do is verify that all five of these columns only contain"
1354,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1929.04,1964.88,35.83999999999992, the accepted categories. I feel like every data set I've ever worked with always has someone just typing in something completely random and we need to get rid of that stuff. So use that unique function to make sure that each one of these columns is correctly formatted. However for this tutorial I've already done that so we're going to skip that stuff. So here we're doing the exact same thing we did before except now we're specifying four columns to process and then we're when we're saving it in a new
1355,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,1964.88,2011.84,46.96000000000004," data frame called X encoded and then we're going to print out the first five rows of X encoded. Pam, there it is. So we've got chest pain resting electrocardiogram, we've got slope and we got fell and so they've all been one hot encoded. Now we need to talk about the three category of columns that only contain zeros and ones. Sex, fasting blood sugar and exercise induced and vagina. As we can see one hot encoding converts a column with more than two columns, excuse me,"
1356,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2012.56,2045.6,33.039999999999964," more than two categories like chest pain into multiple columns of zeros and ones. Since sex, fasting blood sugar and exercise induced and vagina only have two categories to begin with and only contain zeros and ones we do not have to do anything special to them. So we're done formatting the data for the classification tree. For a note again in practice we would use unique to verify that they only contain zeros and ones but to save times just trust me."
1357,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2048.16,2084.08,35.92000000000007," Now one last thing before we build a classification tree. We have Y and that's what we're trying to predict and it doesn't just contain zeros and one. Instead it has five different levels of part disease. Zero for no heart disease and one through four for various degrees of heart disease. We can see this with a unique function so Y dot unique, Pam. So we see that we've got all these different values in the Y column. However in this tutorial we're just going to make a tree that does simple classification"
1358,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2084.08,2116.7200000000003,32.64000000000033," and only care if someone has heart disease or not. So we're going to convert all numbers greater than zero to one. And the way we're going to do that is we're going to store the indices of every time this statement is true. Every time the value in Y is greater than zero we're going to save that index. Then we're going to set all of those indexes to one or all the values that those indexes to one. And then we're going to verify that we only have zeros and ones. We're going to run this code,"
1359,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2116.7200000000003,2151.76,35.03999999999951," Pam, and we did it. Actually that's a double ban. We finally finished formatting the data for making a classification tree. And over here in the chat I see there's a question. Naomi Thompson asks, is there a limit to the number of types of chest pain to make sense to use in one honey coding? Is the method better is one method better than other when one has hundreds of classifying values? It depends. If all of those hundreds of classifying"
1360,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2151.76,2194.16,42.40000000000009," values are unique categories in and of themselves, then yes we need to use one hot and coding. And if we, I mean that could happen if we have a massive data set. If we've got hundreds and hundreds of categories for a single value variable, we'd have to have a huge data set. And that can happen. Yeah, so we would apply one hot and coding and we would then end up with this data frame, our X and code of data frame would then have hundreds and hundreds of extra columns added to it."
1361,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2197.44,2229.92,32.48000000000002," I've never used the data set like that before in scikit learn. So I cannot guarantee that it will not cause the machine to crash. However, there are machine learning methods like XG Boost that are designed to deal with situations like that specifically. So that's another webinar that we'll do in the next couple of months. I've actually already got the Jupiter notebook ready for XG Boost. So we'll be actually just a sneak preview. Next month we're doing support vector"
1362,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2229.92,2261.76,31.840000000000146," machines and then we're going to do the following month we're going to do XG Boost. And then I think after that we're going to do imputing data, imputing missing values and going through all of the various ways for doing that. So that's a little shameless self-promotion right there. Now let's move on and build a preliminary classification tree. This is preliminary because there are lots and lots of actually someone just raised a hand. So before we get to into this,"
1363,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2261.84,2308.88,47.03999999999997," I'll go back and address this. If we use dummy variables, do we run the risk of perfect co-layering, cool in the air, already, among dummies? If yes, how do we deal with them? Um, uh, as you saw, I mean, I guess it's, it is possible to get cool linearity among the variables. The nice thing is with regression trees is they are relatively immune to that as a problem. Typically what regression trees do is they order the columns alphabetically or numerically. There's"
1364,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2308.88,2343.68,34.79999999999973," some way it goes through the, um, uh, through the through the variables. And it just picks the first one that it gets to. And if I've got multiple columns that have the exact same data and, uh, and this, in this call, that first column is really good for, um, classifying. And so it does a great job separating. And the other columns would do just as well. It just uses that first one every time. Um, so it ends up not being an issue if we have redundancy in our data set. And that's one of the nice things"
1365,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2343.68,2384.48,40.80000000000018," about, um, decision trees. Um, all right. So I think we are ready to move on, um, all right. So we're going to build a preliminary classification tree. This is a classification tree, uh, that is not optimized. That's why it's preliminary. Then we'll go through how to optimize it. Once we get this going. So the first thing we're going to do is we're going to split our, our, our data into testing and training data sets subset. So we've got X underscore train, X underscore test,"
1366,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2384.56,2423.12,38.559999999999945," Y underscore train, Y underscore test. And we're using train test to split to take, what X encoded and Y and split them into, uh, training and testing pieces. And I've set the random state to 42 so that when you run this code, you will get the exact same results that I get. After we split the data, we're initializing, uh, a, a decision tree classifier. And then we are going to fit, uh, the data to the training data. Um, and so let's run this."
1367,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2425.04,2460.8,35.75999999999976," And it's un-eventful because we didn't print anything out and we didn't, uh, draw anything. However, uh, this piece of code will draw, um, uh, the decision tree that we just created. It's a huge tree. Um, I'm using the plot tree function that comes with psychic learn. Um, and we just pass it, the tree that we created and trained, the classification decision tree. Uh, and, and we've got a few, uh, parameters that we're passing it to make it easier to look at. So let's, let's draw this."
1368,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2463.12,2494.0,30.88000000000011," There it is. This is a monster decision tree. It's a lot, a lot bigger than the, um, than the, um, then the tree I showed you at the very top of this, uh, Jupiter notebook. I also, by the way, I see some people are raising their hands. I'll get to those questions once we're done with the section we're almost done. Um, okay. So we've built this classification tree this monster. We're going to see how, and we, and so far, it's only seen the training data set. So we're going to see"
1369,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2494.0,2531.52,37.51999999999998," how, and now, how it performs on the testing data set by running the testing data set down the tree. And then drawing a confusion matrix. Um, and we're going to do that, um, with this function called plot confusion matrix. And we pass it the tree, uh, that we've created plus the testing data sets. And we're going to add labels, uh, so the confusion matrix is easy to look at. So let's run that. And there is our confusion matrix. Okay. So we see that of the 42 people that did not have heart disease."
1370,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2531.52,2564.88,33.36000000000013," 31 of them are 71% are correctly classified. And of the 33 people that have heart disease, uh, 26 or 79% were class correctly classified. So the question is, can we do better? One thing that might be holding this classification tree back is that it may have overfit the training data set. So we're going to prune the tree pruning in theory should solve the overfitting problem and give us a better results. Okay. So we finish that section. I'm going to look and see,"
1371,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2566.72,2611.2,44.48000000000002," at some question. I know some people raised some raised their hand. Uh, and I've got we've got some stuff in the Q and A real quick. Uh, yes, someone asked about whether the, uh, I'm going to answer this law. If someone asked if a train test split is 70 30, I believe that is the default. So right here when we're running train tests split, we're just using the default splitting and I believe that's a 70 30. Um, someone asked why I say this is a huge tree. Um, and the reason why I say it's a huge tree is"
1372,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2611.2,2644.0800000000004,32.880000000000564," is actually I know with the final tree, the best optimal tree is. We actually looked at it very at the very beginning and it's much, much smaller. So this tree may be huge, maybe not huge. It's relative to the optimal tree that does the best job with testing data set. In this case, I know that it's huge. And how do I know it's overfit? I guess I know, uh, because I've, I've already optimized the tree and I've seen that it performs much better and it's, and the much smaller ones, uh,"
1373,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2644.0800000000004,2678.96,34.879999999999654," performs better. So in general, when you're doing machine learning, that's a big step you need to do. You need to, uh, you, you make a preliminary tree just like we did, make your, make your confusion matrix and then try to optimize it. Because this confusion matrix is sort of a base. Can we improve on that? If we can, then we know that original tree was overfit. But we also know that by setting parameters and trying to optimize, we actually improve things. Uh, so this is sort of our base ground base"
1374,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2679.04,2713.2000000000003,34.16000000000031," that we're starting from and we're going to try to do better than this. Um, someone asked if this was production could would be used. Psychet learn or would be used something else. Um, I think psychet learn is fine. I mean, it's just sort of depends on the situation. Um, if you've got tons and tons of data and need a lot of optimization for a massive data set, um, psychet learn is not great for that. But for relatively small data, so it's like what we're using. Sure, go ahead and use it."
1375,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2713.84,2751.2000000000003,37.36000000000013," Uh, it's, it's, uh, it's fine. Um, I've answered, I hope I've answered these questions. Uh, also, uh, I see one in the chat that says what happens? We remove the missing values after splitting the data into testing and training. Um, um, uh, it depends. Uh, if you're going to remove rows of data, try to do it beforehand because you don't want a severe imbalance. Uh, if we, you know, uh, say like we have got testing data set and most of the, uh, uh,"
1376,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2751.2000000000003,2780.4,29.199999999999815," rows that we're going to remove or in the testing data set, we will, or we will do, you know, the testing data set will shrink and things will get out of balance. So that's just, that's something you need to be aware about where of if you want to wait. I recommend just getting it over with earlier on. However, in peuding values is another story. If we want to impute values, it's best to do that after splitting and testing. And so that's something we'll talk about when we,"
1377,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2780.4,2814.24,33.840000000000146," when we talk about in the web, uh, in the webinar for imputing values. Okay, I think I've addressed everyone's question. And I think we're ready to move on, uh, to cost complexity pruning. Um, so decision trees are notorious for being over fit to the training data set. And there are a lot of parameters like maximum depth or the minimum number of samples like decision trees have lots of parameters that we can set and they're all designed to reduce overfitting. However,"
1378,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2815.04,2843.36,28.31999999999971," pruning a tree with cost complexity pruning can simplify the whole process of finding a smaller tree that improves the accuracy with the training date or the testing data set. So that's what we're going to do here. And it's going to allow us to skip having a deal with a lot of these, um, sort of tedious parameters because pruning with cost complexity and pruning just takes care of them all and one fell swoop. Pruning a decision tree is all about finding the right value for the"
1379,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2844.32,2876.7200000000003,32.40000000000009," pruning parameter alpha, which controls how little or how much pruning happens. One way to find the optimal value for alpha is to plot the accuracy of the tree as a function of different values. We do this for both the training data set and the testing data set. So first, we're going to extract the different values for alpha that are available for this tree and build a prune tree for each value for alpha. Um, I note I'm amitting the maximum value for alpha because"
1380,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2876.7200000000003,2908.96,32.23999999999978," that just leaves us the root of the tree and we don't want to just we don't want to use that. So I'm going to run this code. We are running a little bit behind schedule. So I'm going to freeze through this by just telling you that this is how this, this chunk of code which you'll get so you don't have to memorize it. But this is how we extract the values for alpha. CCP stands for cost complexity pruning. So these are the cost complexity pruning"
1381,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2908.96,2941.44,32.48000000000002, alphas. And here's when we're peeling off the maximum value for alpha and we're not going to use that. And here is where we're going to create an array of decision trees and we're going to use a for loop for each value for alpha. We're going to create a decision tree. And we're going to see how it performs. And we already did that. Now we're going to graph the accuracy of the trees using the training
1382,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2941.44,2975.36,33.92000000000007," data set and the testing data set as functions of alpha. All this code is doing right here is it's drawing this graph. So the blue is the accuracy for our training data set and the orange is the accuracy for the testing data set. You can see that with the full size tree when alpha equals zero and we have the full size tree, we do the best with the training data set but we do not do very well with the testing data set. We see that as we prune, we increase alpha. So as we increase alpha,"
1383,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,2975.36,3017.84,42.48000000000002," as the size of the trees get smaller and as the trees get smaller, our testing accuracy improves. And that's good. That means we can prune the tree and we can actually perform better with the testing data. And just by looking at this, we can kind of guess that a good value for alpha is zero point zero one six. Note, I don't know if you guys watched the cost complexity pruning video was that quest. In that we are pruning our regression tree and that the score of"
1384,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3017.84,3052.56,34.7199999999998, we evaluate that tree with the sum of the squared residuals. When we evaluate a decision tree we are using gene and gene values range from zero to one. So these values for alpha are way smaller than the ones that are in the the step quest on pruning. But there is a reason for that because gene values only go from zero to one and they don't wear as the sum of square residuals can be this huge number. Okay. Now what we are going to do now that we have seen how to use cross complexity
1385,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3052.64,3093.68,41.03999999999997, pruning to improve the actually we are actually going to use cross validation. So before in this section we just use the the the way the data was split between testing and or excuse me training and testing that original split. But we only used one split. We didn't use tenfold cross validation to validate that that wasn't actually optimal across all of the different ways we could subdivide the data. So now we are going to use cross validation. This first bit of code is going to use a cross
1386,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3093.68,3131.28,37.60000000000037, validation to show that if we just eyeball this number without using cross validation. If we just we don't use cross validation we just pick the first number we get. We actually don't get the optimal tree. We get it at one point. But we see that the another splitting of training and testing data set another fold gives us really bad accuracy. And so we want to avoid that. And that could that's just a function of how the data was split. So we're using cross validation to
1387,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3131.44,3167.6,36.159999999999854, make sure that we don't get tricked. Okay. So the graph above shows that using different training and data sets with the same alpha resulted in different accuracy. So justing that alpha is sensitive to the data sets. So instead of picking a single training data set and a single testing data set we're going to use cross validation to find the optimal value for cost complexity pruning alpha. So here we're doing the exact same thing we did before. Now over now we're calculating the
1388,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3167.6,3222.48,54.88000000000011, accuracy with cross validation for each value for alpha. And then we're going to plot a graph of the accuracy. Let's run that. And here we see using cross validation that overall instead of using this is the value we've been using before 0.016 this value to the left might be better. Overall over the over the each fold of cross validation. So instead of setting ccccccp alpha to 0.016 we need to set it something closer to 0.014. So we're going to find the exact value by sort of narrowing
1389,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3222.48,3261.6800000000003,39.20000000000027," down the range that we're looking at between 0.014 and 0.05. And here's that value for alpha. And we're going to store that in a new variable. So we're going to we've got a new variable called ideal cost complexity pruning alpha. That stores it there. However, at this point Python thinks that this variable that we just created ideal cost complexity pruning alpha is a series, which is a type of array. We can tell that because when we printed it out we got two bits of stuff."
1390,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3262.4,3300.24,37.84000000000015," The first was 20 right here which is the index in the series and the second one is the value for alpha. We need to convert this to a float before we pass it to a classification tree. So that's what we're going to do. And we do that just by asking for alpha. So hooray, now we have the ideal value for alpha and we can build evaluate and draw the final classification tree. Okay, now we're on the last section of but before there's a couple of questions I want to answer real quickly. Someone asked if the"
1391,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3300.24,3338.88,38.63999999999987," alpha value can be found with richer search. Yes, definitely. Grid searches a way of of looking at different parameters or or trying different values with different values at different values with different parameters at the same time as it basically it tries all different combinations. See we've got a bunch of questions in the Q and A. Is there a many method to find the intersection from the graph instead of guessing it?"
1392,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3341.44,3377.76,36.32000000000016, I'm sure there is. I'm blanking on it right now. Where is stack exchange when we need it? I would just Google that on stack exchange. I'm sure there's something good. And someone asked can we just do cross validation from the get go and the answer is yes. There's no need to to do it the way I did it where I just kind of did it simply and then use cross validation basically I was just trying to emphasize why we do cross validation
1393,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3378.64,3417.36,38.720000000000255," to show that we'll actually get a different value once we use it. A single cross validation sufficient for model evaluation or is nested cross validation better. I'll be honest I don't know the answer to that question. So I will say that's another stack exchange question. Okay, so moving on we got just a few minutes I'm shooting for 1205 as the optimal finished time since we started five minutes late let's end five minutes late and we'll be on time."
1394,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3417.92,3451.1200000000003,33.20000000000027," If you have to leave early don't worry you'll get a link to this video to this webinar you'll be able to watch it again and you'll get this Jupiter notebook so you can go through it at your own pace later. Okay, now that we have the ideal value for alpha we can build the final classification by setting this parameter CC P alpha to the ideal value. So that's what we do we use the exact same call that we did before however this time we're setting this parameter and then we are fitting it to"
1395,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3451.1200000000003,3495.92,44.79999999999973, the training data set and now what we're doing is we're plotting a confusion matrix but now we're using the prune to tree. Gray the prune tree is better at classifying patients than the full-sized tree. Of the 42 people that did not have hard to see disease now we're up to 81% class correctly classified before we only got 74% and for the people with hard disease we're up to 85% and before we only had 79%. So now we're ready for the last thing which is to to draw the prune tree and discuss
1396,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3495.92,3536.24,40.32000000000016, how to interpret it. So we're going to draw the tree and this is the prune tree. Now just for reference this was the original tree which is huge and after we're pruning it we got down to this and this little guy performs better than that big tree does and it's because that big tree over fits the data of the training data it fits it like a glove and someone I recently read somewhere I wish I could remember where I read it but someone someone said that overfitting is sort of like memorizing
1397,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3536.24,3577.2,40.95999999999958, all the answers to an exam rather than understanding what the questions actually are. So by pruning it we're forcing the tree to not memorize the answers but to do a better job classifying. So we're going to discuss how to interpret the tree. So each node in the tree has a column name that was used to split so we used CA values less than 0.5 go to the left of values greater so if this statement is false we go to the right we got the gene
1398,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3577.2,3617.28,40.08000000000038, impurity for the node the number of samples in the node and the number of classifications in that node so we've got 118 people that do not have heart disease and 104 people that do have heart disease and lastly the class tells us whatever category has the majority and so since no heart disease has the majority the class is no heart disease. It's also colored according to whoever has the majority. So all of these orange or oranges nodes have a majority of no heart disease and the blueish
1399,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3617.28,3653.68,36.39999999999964, nodes have a majority of yes heart disease. The darker the color the lower the gene impurity. So the better the split the better the the the sort of bias towards one category or the other. So this guy has a relatively low gene impurity especially compared to where we started where there was almost a 50 50 split between people with heart disease and without heart disease but by the time we get down here we've got a extreme bias towards people without heart disease after looking at CA
1400,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3654.56,3687.84,33.2800000000002, the valium heart scan old peak and then getting to this leave the leafs by the way don't have column names because there no longer splitting the data. So in conclusion we have imported data identified and dealt with missing data format of the data for a decision tree using one hot and coding built a preliminary decision tree for classification and that's what we use as a reference to know if pruning or optimizing the tree was going to make any different difference. Then we
1401,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3687.84,3728.56,40.7199999999998, pruned with cost concloat complexity pruning then we built drew interpreted and evaluated the final classification tree. So hooray triple ban we made it. Oh a couple things before we go I want to say a lot of people when they register they they'll use one email to register with zoom and another email because that's their PayPal account or another email associated with the payment. When I email this out I will email it out make sure you check both because I get I get a bunch of lists of
1402,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3728.56,3775.52,46.96000000000004," emails and I can't make sense of which one is the one you check the most often. So if you have multiple email addresses, make sure you check both. What else do I need to notice? Oh the video and this Jupiter notebook should be available by tomorrow at the latest. Oh somebody asked for views on a C4.5 tree and I'm going to be honest and say I don't actually know what we've gone through is cart and I can't remember the difference between cart and C4.5 right now. I will say that cart"
1403,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3775.52,3804.56,29.039999999999964, uses genie. We could have used entropy and actually I created I did I used entropy I did just earlier this morning I was like hey I want to what happened if I used entropy instead of genie. And so shocking to me is that the tree performed a much worse. I was I was on the impression that entropy and genie were this roughly equivalent and that you could just use the manner changeably. It turns out that at least with this data set that is not the case.
1404,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3806.56,3843.04,36.48000000000002," Anyways, I want to someone asked me to confirm a recommendation for using a production system as an alternative psychic learn. I unfortunately do not have an opinion on that. I wish I did and that's something I will look into and hopefully I have an answer for you guys later. I'll try to if there's questions I've given super lane answers to I will try to research and send answers as part of the email that"
1405,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3843.04,3883.04,40.000000000000455," I send out to people. Someone asked if there's a way to know which features have the most influence. Yes, yes there is. You can just look at the tree to get a sense of which Grammar's had were useful. You can also look at drop and genie scores. We can print all the stuff out. All these values that we're looking at we can actually store those in variables and then parse algorithmically. So we can see which variables are associated with the greatest drop in genie."
1406,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3884.0,3928.32,44.32000000000016, That's a way we can figure out which variables are the most influential or the most important for separation. Thank you very much. I really appreciate you for joining the discussion. I'll try to address any other questions in the email if I don't get them to them right now. Thank you very much. It really means a lot to me that you're here and I hope everyone is safe and we'll talk to you. I guess in the next live stream or the next web and our next month we're
1407,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3928.32,3967.36,39.03999999999997," doing support vector machines. So there you go. Oh, someone did ask if a decision trees are better for medical data than random forests and I don't know. I like I love random forests. I'm a random forest dude. I'm always thinking about it. I think it might be appropriate in a medical setting. It just sort of depends. I would like for it to be because I like random forests. But in terms of like sheer interpretability, simplicity, decision trees are the best."
1408,Classification Trees in Python from Start to Finish,https://www.youtube.com/watch?v=q90UDEgYqeI,q90UDEgYqeI,3967.36,3981.92,14.559999999999944," Rain and forest are similar but still a little more challenging to interpret. That's the answer to that. All right. I hope everyone's doing okay. Until next time, Queston."
1409,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,0.0,27.96,27.96," Wandering around, around them, forest, I won't get lost because of stat quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to be starting part one of a series on random forests, and we're going to talk about building and evaluating random forests. Note, random forests are built from decision trees, so if you don't already know about"
1410,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,27.96,53.64,25.68," those, check out my stat quest and be-fuck. Decision trees are easy to build, easy to use, and easy to interpret. But in practice, they are not that awesome. To quote from the elements of statistical learning, aka the Bible of Machine Learning, trees have one aspect that prevents them from being the ideal tool for predictive learning, namely"
1411,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,53.64,74.88,21.239999999999995," in accuracy. In other words, they work great with the data used to create them, but they are not flexible when it comes to classifying new samples. The good news is that random forests combine the simplicity of decision trees with flexibility resulting in a vast improvement in accuracy."
1412,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,74.88,91.76,16.879999999999995," So let's make a random forest. Step one, create a bootstrap dataset. Imagine that these four samples are the entire dataset that we are going to build a tree from. I know it's crazy small, but just pretend for now."
1413,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,91.76,114.08,22.32000000000002," To create a bootstrap dataset that is the same size as the original, we just randomly select samples from the original dataset. The important detail is that we're allowed to pick the same sample more than once. This is the first sample that we randomly select. So it's the first sample in our bootstrap dataset."
1414,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,114.08,133.68,19.599999999999994," This is the second randomly selected sample from the original dataset. So it's the second sample in our bootstrap dataset. Here's the third randomly selected sample. So here it is in the bootstrap dataset. Lastly, here's the fourth randomly selected sample."
1415,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,133.68,156.39999999999998,22.71999999999997," Note, it's the same as the third. And here it is. Bam, we've created a bootstrap dataset. Step two for creating a random forest is to create a decision tree using the bootstrap dataset, but only use a random subset of variables or columns at each step."
1416,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,156.39999999999998,188.12,31.720000000000027," In this example, we will only consider two variables or columns at each step. Note, we'll talk more about how to determine the optimal number of variables to consider later. Thus, instead of considering all four variables to figure out how to split the root node, we randomly select two. In this case, we randomly selected good blood circulation and blocked arteries as candidates for the root node."
1417,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,188.2,224.24,36.03999999999999," Just for the sake of the example, assume that good blood circulation did the best job separating the samples. Since we used good blood circulation, I'm going to gray it out so that we focus on the remaining variables. Now we need to figure out how to split samples at this node. Just like for the root, we randomly select two variables as candidates instead of all three remaining columns. And we just build the tree as usual, but only considering a random subset of variables at each step."
1418,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,224.24,250.8,26.55999999999997," Double bound. We built a tree, one, using a bootstrap dataset, and two, only considering a random subset of variables at each step. Here's the tree we just made. Now, go back to step one and repeat. Make a new bootstrap dataset and build a tree considering a subset of variables at each step."
1419,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,250.8,275.68,24.880000000000024," Ideally, you do this hundreds of times, but we only have space to show six. But you get the idea. Using a bootstrap sample and considering only a subset of variables at each step results in a wide variety of trees. The variety is what makes random forests more effective than individual decision trees. Sweet."
1420,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,275.68,296.52,20.83999999999997," Now that we've created a random forest, how do we use it? Well, first we get a new patient. We've got all the measurements. And now we want to know if they have heart disease or not. So we take the data and run it down the first tree that we made."
1421,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,296.68,320.08,23.399999999999977," Boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, to the first tree says yes. The patient has heart disease, and we keep track of that here. Now we run the data down the second tree that we made. The second tree also says yes. And we keep track of that here."
1422,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,320.08,339.72,19.640000000000043," And then we repeat for all the trees we made. After running the data down all of the trees in the random forest, we see which option received more votes. In this case, yes, received the most votes, so we will conclude that this patient has heart disease."
1423,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,339.72,363.88,24.159999999999968," Bam! Oh no, terminology alert. Boots strapping the data, plus using the aggregate to make a decision, is called bagging. Okay, now we've seen how to create and use a random forest. How do we know if it's any good?"
1424,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,363.88,387.4,23.52000000000004," Remember when we created the Boots strapped data set? We allowed duplicate entries in the Boots strapped data set. As a result, this entry was not included in the Boots strapped data set. Typically, about one-third of the original data does not end up in the Boots strapped data set."
1425,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,387.4,408.2000000000001,20.80000000000001," Here's the entry that didn't end up in the Boots strapped data set. Just. If the original data set were larger, we'd have more than just one entry over here. This is called the Out of Bag Data Set. If it were up to me, I would have named it the Out of Boot Data Set, since it's the"
1426,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,408.2,431.96,23.75999999999999," entries that didn't make it into the Boots strapped data set. Unfortunately, it's not up to me. Since the Out of Bag Data was not used to create this tree, we can run it through and see if it correctly classifies the sample as no heart disease. In this case, the tree correctly labels the Out of Bag sample, no."
1427,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,431.96,453.92,21.96000000000004," Then we run this Out of Bag sample through all of the other trees that were built without it. This tree incorrectly labeled the Out of Bag sample, yes. These trees correctly labeled the Out of Bag sample, no. Since the label with the most votes wins, it is the label that we assign this Out of Bag"
1428,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,453.92,478.84,24.91999999999996," sample. In this case, the Out of Bag sample is correctly labeled by the random forest. We then do the same thing for all of the Out of Bag samples for all of the trees. This Out of Bag sample was also correctly labeled. This Out of Bag sample was incorrectly labeled."
1429,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,478.84,493.92,15.079999999999984," Etc. Etc. Ultimately, we can measure how accurate our random forest is by the proportion of Out of Bag samples that were correctly classified by the random forest."
1430,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,493.92,514.28,20.360000000000014," The proportion of Out of Bag samples that were incorrectly classified is the Out of Bag error. Okay. We now know how to 1 build a random forest, 2 use a random forest, and 3 estimate the accuracy of a random forest."
1431,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,514.68,544.0799999999999,29.399999999999977," However, now that we know how to do this, we can talk a little more about how to do this. Remember when we built our first tree and we only use two variables, columns of data, to make a decision at each step. Now we can compare the Out of Bag error for a random forest built using only two variables per step, to a random forest built using three variables per step."
1432,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,544.08,568.4000000000001,24.32000000000005," And we test a bunch of different settings and choose the most accurate random forest. In other words, 1 we build a random forest, and then 2 we estimate the accuracy of a random forest. Then we change the number of variables used per step, and we do this a bunch of times and then choose the one that is the most accurate."
1433,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,568.4000000000001,589.24,20.83999999999992," Typically, we start by using the square of the number of variables, and then try a few settings above and below that value. Triple BAM Hooray, we've made it to the end of another exciting stat quest. Toon in next week, and we'll talk about how to deal with missing data and how to cluster"
1434,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,589.24,593.36,4.1200000000000045," the samples. All right, until then, quest on."
1435,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,0.0,20.4,20.4," Random Force, Part 2, Eipit, who reads true, Stankwist. Hello, I'm Josh Starmer and welcome to Stankwist. Today we're doing Random Force Part 2, and we're going to focus on missing data in sample clustering. To be honest, the sample clustering aspect of Random Force is my favorite part, so I'm really excited"
1436,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,20.4,37.6,17.2," we're going to cover it. Here's our data set. We've got data for four separate patients. However, for patient number four, we've got some missing data. Random forests consider two types of missing data."
1437,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,37.6,61.84,24.239999999999995," One, missing data in the original dataset used to create the random forest, and two, missing data in a new sample that we want to categorize. We'll start with this one. So we want to create a random forest from this data. However, we don't know if this patient has blocked arteries or their weight."
1438,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,61.84,91.76,29.919999999999995," The general idea for dealing with missing data in this context is to make an initial guess that could be bad, and then gradually refine the guess until it is hopefully a good guess. Because this person did not have heart disease, the initial and possibly bad, guess for the blocked arteries value is just the most common value for blocked arteries found in the other samples that do not have heart disease."
1439,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,91.76,108.52,16.760000000000005," Among the people that do not have heart disease, no is the most common value for blocked arteries. It occurs in two out of two samples. So no is our initial guess. This weight is numeric."
1440,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,108.52,136.48,27.959999999999997," Our initial guess will be the median value of the patients that did not have heart disease. In this case, the median value is 167.5. Here's our new dataset with the filled in missing values. Now we want to refine these guesses. We do this by first determining which samples are similar to the one with missing data."
1441,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,136.48,150.95999999999998,14.47999999999999, So let's talk about how to determine similarity. Step 1. Build a random forest. Step 2. Run all of the data down all of the trees.
1442,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,150.95999999999998,177.95999999999998,27.0, We'll start by running all of the data down the first tree. Go to V��20. Notice the sample 3 and sample 4 both ended up at the same leaf node. That means they're similar. At least that cell similarity is defined in random forests.
1443,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,177.95999999999998,206.74,28.78," We keep track of similar samples using a proximity matrix. The proximity matrix has a row for each sample. And it has a column for each sample. Because sample 3 and sample 4 ended up in the same leaf node, we put a 1 here. We also put a 1 here, since this position also represents samples 3 and 4."
1444,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,206.74,242.82,36.08000000000001," Because no other pair of samples ended in the same leaf node, our proximity matrix looks like this after running the samples down the first tree. Now we run all of the data down the second tree. Note, samples 2, 3 and 4 all ended up in the same leaf node. This is what the proximity matrix looked like after running the data down the first tree."
1445,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,242.82,262.86,20.04000000000002," And after the second tree, we add 1 to any pair of samples that ended up in the same leaf node. Samples 3 and 4 ended up in the same node together again. And sample 2 also ended up in that same node. Now we run all of the data down the third tree."
1446,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,262.86,287.26,24.399999999999977," And here's the updated proximity matrix. Only samples 3 and 4 ended up in the same leaf node. Ultimately, we run the data down all the trees and the proximity matrix fills in. Then we divide each proximity value by the total number of trees. In this example, assume we had 10 trees."
1447,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,287.26,308.3,21.04000000000002," Now we use the proximity values for sample 4 to make better guesses about the missing data. For blocked arteries, we calculate the weighted frequency of yes and no using proximity values as the weight. Yes, occurs in one third of the samples."
1448,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,308.3,344.02,35.71999999999997," No occurs in two thirds of the samples. The weighted frequency for yes is the frequency of yes times the weight for yes. The weight for yes equals the proximity of yes divided by all of the proximity. The proximity for yes is the proximity value for sample 2, the only one with yes. And we divide that by the sum of the proximity for sample 4."
1449,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,344.02,367.66,23.639999999999983," So the weight for yes is 0.1. Thus, the weighted frequency for yes is 0.03. The weighted frequency for no is the frequency of no, which is 2 thirds times the weight for no. Samples 1 and 3 both have no."
1450,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,367.66,389.7,22.04000000000002," With that in mind, we can plug in the values for the proximity of no divided by all proximity. Thus, the weight for no is 0.9. And the weighted frequency for no is 0.6. No has a way higher weighted frequency so we'll go with it."
1451,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,389.7,414.1,24.399999999999977," In other words, our new improved and revised guests based on the proximity is no for blocked arteries. For weight, we use the proximity to calculate a weighted average. In this case, the weighted average equals sample 1's weight, sample 1's weighted average weight."
1452,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,414.1,437.42,23.32000000000005," Sorry if there's any confusion between a patient's weight or a sample's weight and the weight used in the weighted average. To calculate that weight, we start with the proximity for sample 1. We've divided by the sum of the proximity. So sample 1's weighted average weight is 0.1."
1453,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,437.42,467.46,30.039999999999964," Here's the weighted value for sample number 2, who weighs 180. Here's the weighted average value for sample number 3, who weighs 2.10. Ultimately, the weighted average of weight is 198.5. And remember, the weights that we used in the weighted average were based on proximity. Now that we've revised our guesses a little bit, we do the whole thing over again."
1454,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,467.46,487.46,20.0," We build a random forest, run the data through the trees, recalculate the proximity and recalculate the missing values. We do this six or seven times until the missing values converge. IE no longer change each time we recalculate. Bam!"
1455,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,487.46,511.98,24.52000000000004," Now it's time for an inner-lude of awesomeness. Let me show you something super cool we can do with the proximity matrix. This is the proximity matrix before we divided each value by 10, the number of trees in the pretend random forest. Thus for the sake of easy math, imagine if samples 3 and 4 ended up in the same leaf"
1456,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,511.98,534.94,22.96000000000004," node in all 10 trees. Now we have a 10 here and here. After dividing by 10, the number of trees in the forest, we see that the largest number in the proximity matrix is 1. 1 in the proximity matrix means the samples are as close as close can be."
1457,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,534.94,569.74,34.799999999999955," That means 1 minus the proximity values equals distance. Closes can be equals node distance between and not close equals far away. This is a distance matrix and that means we can draw heat map with it. If you don't know what a heat map is, check out the stat quest. And we can also draw an MDS plot with it."
1458,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,569.74,591.02,21.279999999999973," And if you don't know what an MDS plot is, well, check out the stat quest. I think this is super cool because it means that no matter what the data are, ranks, multiple choice, numeric, et cetera. If we can use it to make a tree, we can draw heat map or an MDS plot to show how the samples are related to each other."
1459,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,591.02,604.2199999999999,13.199999999999932," This is awesome. Drip all bound. OK, enough fun stuff. Let's get back to the missing data problem. At long last, we'll talk about the second method."
1460,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,604.2199999999999,624.18,19.96000000000004, This is when we have missing data in a new sample that we want to categorize. Imagine we had already built a random forest with existing data and wanted to classify this new patient. So we want to know if they have heart disease or not. But we don't know if they have blocked arteries.
1461,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,624.18,646.02,21.840000000000032, So we need to make a guess about blocked arteries so we can run the patient down all the trees in the forest. The first thing we do is create two copies of the data. One that has heart disease and one that doesn't have heart disease. Then we use the iterative method we just talked about to make a good guess about the missing
1462,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,646.02,669.22,23.200000000000045, values. These are the guesses that we came up with. Then we run the two samples down the trees in the forest. And we see which of the two is correctly labeled by the random forest the most times. This option was correctly labeled yes in all three trees.
1463,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,669.22,691.34,22.120000000000005, This option was only correctly labeled no in one tree. This option wins because it was correctly labeled more than the other option. Bam. We filled in the missing data and we've classified our sample. Hey we've made it to the end of another exciting stat quest.
1464,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,691.34,708.54,17.199999999999932," If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign. Coming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate. The links are in the description below."
1465,StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,708.54,711.62,3.080000000000041," Alright until next time, quest on."
1466,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,0.0,28.0,28.0," You don't need a ukulele to do statistics, but it makes it more fun. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about how to build, use and evaluate random forests in R. This stack quest builds on two stack quests that I've already created that demonstrate the theory behind random forests. So if you're not familiar with it, check them out."
1467,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,28.0,60.0,32.0," Just so you know, you can download all the code that I described in this tutorial using the link in the description below. The first thing we do is load in GGplot 2 so we can draw fancy graphs. And when I do that, our prints out a little message, it's no big deal. Then we load CalPlot which just improves some of GGplot 2's default settings. It overrides the GG save function and that's fine with me, so no worries here."
1468,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,60.0,85.0,25.0," The last library we need to load is random forest. So we can make random forests. It also prints out some stuff in red, but it's no big deal we can move on from here. For this example, we're going to get a real data set from the UCI machine learning repository. Specifically, we want the heart disease data set."
1469,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,85.0,108.0,23.0," So we make a variable called URL and set it to the location of the data we want for our random forest. And this is how we read the data set into R from the URL. The head function shows us the first six rows of data. Unfortunately, none of the columns are labeled. Wow, wow."
1470,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,108.0,128.0,20.0," So we name the columns after the names that were listed on the UCI website. The UCI website actually lists a whole lot of information about this data. So it's worth checking out if you haven't done that already. Hey. Now when we look at the first six rows with the head function, things look a lot better."
1471,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,129.0,169.0,40.0," However, the stir function, which describes the structure of the data, tells us that some of the columns are messed up. Sex is supposed to be a factor where zero represents female and one represents male. CP, aka chest pain, is also supposed to be a factor where levels one through three represent different types of pain and four represents no chest pain. CA and thaw are correctly called factors, but one of the levels is question mark when we need it to be an NA. So we've got some cleaning up to do."
1472,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,169.0,196.0,27.0," The first thing we do is change the question marks to NA's. Then, just to make the data easier on the eyes, we convert the zeros in sex to F for female. And the ones to M for male. Lastly, we convert the column into a factor. Then we convert a bunch of other columns into factors since that's what they're supposed to be."
1473,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,196.0,223.0,27.0," See the UCI website or the sample code on the stat quest blog for more details. Since the CA column originally had a question mark in it, rather than NA, our thinks it's a column of strings. We correct that assumption by telling our it's a column of integers. And then we convert it to a factor. Then we do the exact same thing for thaw."
1474,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,223.0,259.0,36.0," The last thing we need to do to the data is make HD aka heart disease, a factor that is easy on the eyes. Here I'm using a fancy trick with if else to convert the zeros to healthy and the ones to unhealthy. We could have done a similar trick for sex, but I wanted to show you both ways to convert numbers towards. Once we're done fixing up the data, we can check that we've made the appropriate changes with the stir function. Sex is now a factor with levels F and M."
1475,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,259.0,289.0,30.0," And everything else looks good too. Hey, we're done with the boring part. Now we can have some fun. Since we're going to be randomly sampling things, let's set the seed for the random number generator so that we can reproduce our results. Now we impute values for the NA's and the data set with RF impute. The first argument to RF impute is HD tilde dot."
1476,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,289.0,327.0,38.0," And that means we want the HD aka heart disease column to be predicted by the data in all of the other columns. Here's where we specify which data set to use. In this case, there's only one data set and it's called data. Here's where we specify how many random forests RF impute should build to estimate the missing values. In theory, 4 to 6 iterations is enough. Just for fun, I set this parameter, itter equal to 20, but it didn't improve the estimates."
1477,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,328.0,372.0,44.0," Lastly, we save the results. The data set with imputed values instead of NA's as data dot imputed. After each iteration, RF impute prints out the out of bag, OOB, error rate. This should get smaller if the estimates are improving. Since it doesn't, we can conclude that our estimates are as good as they're going to get with this method. Here's where we actually build a proper random forest using the random forest function. Just like when we imputed values for the NA's, we want to predict HD aka heart disease using all of the other columns in the data set."
1478,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,372.0,407.0,35.0," However, this time we specify data dot imputed as the data set. We also want random forest to return the proximity matrix, who will use this to cluster the samples at the end of the stat quest. Lastly, we save the random forest and associated data like the proximity matrix as model. To get a summary of the random forest and how well it performed, we can just type model on the command prompt and then hit enter. Here's what gets printed to the screen."
1479,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,408.0,441.0,33.0," The first thing is the original call to random forest. Next, we see that the random forest was built to classify samples. If we had used the random forest to predict weight or height, it would say regression. And if we had omitted the thing, the random forest was supposed to predict entirely, it would say unsupervised. Then it tells us how many trees are in the random forest. The default value is 500."
1480,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,441.0,474.0,33.0," Later, we will check to see if 500 trees is enough for optimal classification. Then it tells us how many variables or columns of data were considered at each internal node. Classification trees have a default setting of the square root of the number of variables. Regression trees have a default setting of the number of variables divided by 3. Since we don't know if 3 is the best value, we'll fiddle with this parameter later on."
1481,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,474.0,509.0,35.0," Here's the out of bag, OOB, error estimate. This means that 83.5% of the OOB samples were correctly classified by the random forest. Lastly, we have a confusion matrix. There were 141 healthy patients that were correctly labeled healthy. Prey, there were 27 unhealthy patients that were incorrectly classified as healthy."
1482,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,509.0,548.0,39.0," There were 23 healthy patients that were incorrectly classified on healthy. Lastly, there were 112 unhealthy patients that were correctly classified on healthy. Prey, to see if 500 trees is enough for optimal classification, we can plot the error rates. Here, we created data frame that formats the error rate information so that GG plot 2 will be happy. This is kind of complicated, so let me walk you through it."
1483,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,548.0,586.0,38.0," For the most part, this is all based on a matrix within model called ER.rate. This is what the ER.rate matrix looks like. There's one column for the out of bag error rate. One column for the healthy error rate, i.e. how frequently healthy patients are misclassified. Each row reflects the error rates at different stages of creating the random forest."
1484,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,586.0,614.0,28.0, The first row contains the error rates after making the first tree. The second row contains the error rates after making the first two trees. The last row contains the error rates after making all 500 trees. So what we're doing here is making a data frame that looks like this. There's one column for the number of trees.
1485,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,614.0,634.0,20.0, There's one column for the type of error. And one column for the actual error value. And here's the call to GG plot. Bam. The blue line shows the error rate when classifying unhealthy patients.
1486,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,634.0,661.0,27.0," The green line shows the overall out of bag error rate. The red line shows the error rate when classifying healthy patients. In general, we see the error rates decrease when our random forest has more trees. If we added more trees with the error rate go down further. To test this hypothesis, we make a random forest with 1000 trees."
1487,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,661.0,686.0,25.0, The out of bag error rate is the same as before. And the confusion matrix shows that we didn't do a better job classifying patients. And we can plot the error rates just like before. Double bam. And we see that the error rates stabilize right after 500 trees.
1488,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,686.0,716.0,30.0," So adding more trees didn't help, but we would not have known this unless we used more trees. Now we need to make sure we are considering the optimal number of variables at each internal node in the tree. We start by making an empty vector that can hold 10 values. And then we create a loop that tests different numbers of variables at each step. Each time we go through the loop, I increase by 1."
1489,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,716.0,748.0,32.0," It starts at 1 and ends after 10. In this line, we are building a random forest using I to determine the number of variables to try at each step. Specifically, we are setting Mtri equals I and I equals values between 1 and 10. This is where we store the out of bag error rate after we build each random forest that uses a different value for Mtri. This is a bit of complex code."
1490,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,748.0,774.0,26.0," Here's what's going on. Temp.model contains a matrix called Er.rate, just like model did before. And we want to access the value in the last row and in the first column. Ie, the out of bag error rate when all 1000 trees have been made. Now we can print out the out of bag error rates for different values for Mtri."
1491,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,774.0,805.0,31.0," The third value corresponding to Mtri equals 3, which is the default in this case, has the lowest out of bag error rate. So the default value was optimal, but we wouldn't have known that unless we tried other values. Lastly, we want to use the random forest to draw an Mds plot with samples. This will show us how they are related to each other. If you don't know what an Mds plot is, don't freak out, just check out the stack quest on."
1492,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,805.0,835.0,30.0," We start by using the dist function to make a distance matrix from 1 minus the proximity matrix. Then we run CMD scale on the distance matrix. CMD scale stands for classical multi-dimensional scaling. Then we calculate the percentage of variation in the distance matrix that the X and Y axes account for. Again, see the other stack quest for details."
1493,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,835.0,854.0,19.0, Then we format the data for Gg plot. And then we draw the graph with Gg plot. Triple BAM. Unhealthy samples are on the left side. Healthy samples are on the right side.
1494,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,854.0,887.0,33.0," I wonder if patient 253 was misdiagnosed, and actually has heart disease. Note, the X axis accounts for 47% of the variation in the distance matrix. The Y axis only accounts for 14% of the variation in the distance matrix. That means that the big differences are along the X axis. Lastly, if we got a new patient and didn't know if they had heart disease and they clustered down here,"
1495,StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,887.0,909.0,22.0," we'd be pretty confident that they had heart disease. Her A, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more of them, please subscribe. And if you have any suggestions for future stack quests, well, put them in the comments below. Until next time, quest on."
1496,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,0.0,17.68,17.68," The chain rule is cool. Stat Quest? Yeah. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about the chain rule."
1497,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,17.68,43.2,25.520000000000003," And it's going to be clearly explained. Note, this Stat Quest assumes that you are already familiar with the basic idea of a derivative and just want a deeper understanding of the chain rule. That said, let's do a super quick review. Imagine we collected these measurements from a bunch of people."
1498,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,43.2,69.24,26.039999999999992," On the X-axis, we measured how much they liked Stat Quest. And on the Y-axis, we measured awesomeness. We can then fit this orange parabola to the data. The equation for the parabola is, awesomeness equals likes Stat Quest squared. The derivative of this equation tells us the slope of the tangent line at any point along"
1499,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,69.24,86.92,17.680000000000007, the curve. The slope of the tangent line tells us how quickly awesomeness is changing with respect to likes Stat Quest. We can calculate the derivative of awesomeness with respect to likes Stat Quest by using the power rule.
1500,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,86.92,113.36,26.44000000000001," The power rule tells us to multiply likes Stat Quest by the power, which is 2, and raise Stat Quest by the power, 2 minus 1. And since 2 minus 1 equals 1, and raising something by 1 is the same as omitting the power, we end up with 2 times likes Stat Quest. Okay, bam, that's the review."
1501,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,113.4,140.12,26.72," Now let's dive into the chain rule with a super simple example. Imagine we collected weight and height measurements from three people, and then we fit a line to the data. Now if someone tells us they weigh this much, we can use the green line to predict that they are this tall."
1502,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,140.12,157.51999999999998,17.399999999999977," Bam. Now imagine we collected height and shoe size measurements, and we fit a line to the data. Now if someone tells us that they are this tall, we can use the orange line to predict that this is their shoe size."
1503,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,157.51999999999998,181.92,24.400000000000038," Bam. Now if someone tells us that they weigh this much, then we can predict their height, and we can use the predicted height to predict shoe size. And if we change the value for weight, we see a change in shoe size. Bam."
1504,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,181.92,210.0,28.079999999999984," Now let's focus on this green line that represents the relationship between weight and height. We see that for every one unit increase in weight, there is a two unit increase in height. In other words, the slope with a line is two divided by one, which equals two. And since the slope is two, the derivative, the change in height, with respect to a change"
1505,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,210.0,239.36,29.360000000000014," in weight, is two. Now since the slope with a green line is the same as its derivative, two, the equation for height is height equals the derivative of height with respect to weight times weight, which equals two times weight. Note, the equation for height has no intercept because the green line goes through the origin."
1506,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,239.36,260.52,21.159999999999968," Now let's focus on the orange line that represents the relationship between height and shoe size. In this case, we see that for every one unit increase in height, there is a one quarter unit increase in shoe size. And I admit that it's hard to see the one quarter unit increase in shoe size so just"
1507,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,260.52,285.8,25.28000000000003," trust me. Anyway, because we go up one quarter unit for every one unit we go over, the slope is, one quarter divided by one, which equals one quarter. And since the slope is one quarter, the derivative or the change in shoe size, with respect to a change in height is one quarter."
1508,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,285.8,309.64000000000004,23.840000000000032," Now since the slope of the orange line is the same as its derivative, the equation for shoe size is. shoe size equals the derivative of shoe size with respect to height times height, which equals one quarter times height. But again, because the orange line goes through the origin, the equation for shoe size"
1509,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,309.64000000000004,340.0,30.35999999999996," has no intercept. Now because weight can predict height and height can predict shoe size, we can plug the equation for height into the equation for shoe size. Now if we want to determine exactly how shoe size changes with respect to changes in weight, we can take the derivative of shoe size with respect to weight."
1510,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,340.0,367.16,27.160000000000025," And the derivative of the equation for shoe size with respect to weight is just the product of the two derivatives. In other words, because height connects weight to shoe size, the derivative of shoe size with respect to weight is the derivative of shoe size with respect to height times the derivative of height with respect to weight."
1511,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,367.16,388.44,21.279999999999973," This relationship is the essence of the chain rule. Plugging in numbers gives us one half. And that means for every one unit increase in weight, beep, beep, beep. There is a one half unit increase in shoe size. Bam!"
1512,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,388.48,409.92,21.44," Now let's look at a slightly more complicated example. Imagine we measured how hungry a bunch of people were and how long it had been since they last had a snack. As time since the last snack increases on the X-axis, people got hungrier and hungrier at a faster rate."
1513,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,409.92,435.92,26.0," So we fit an exponential line with intercept one half to the measurements to reflect the increasing rate of hunger. Then we measured how much people craved ice cream and how hungry they were. The hungerier someone was, the more they craved ice cream. But after a certain amount of hunger, the craving did not continue to increase very much."
1514,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,435.92,457.48,21.56, So we fit a square root function to the data to reflect how the increase in craving tapers off. Now if we want to see how the rate of craving ice cream changes with respect to the time since the last snack. Plugging the equation for hunger into the equation for craves ice cream gives us an
1515,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,457.48,478.52,21.039999999999964," equation without an obvious derivative. To convince yourself that taking the derivative of this is no fun at all, pause the video and give it a try. However, because hunger links time since last snack to craves ice cream, we can use the chain rule."
1516,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,478.52,505.32,26.80000000000001," To solve for this derivative. First the power rule tells us that the derivative of hunger with respect to the time since the last snack is two times time. Likewise, the power rule tells us that the derivative of craves ice cream with respect to hunger is one divided by two times the square root of hunger."
1517,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,505.32,544.96,39.64000000000004," So with these two derivatives, the chain rule tells us that the derivative of craves ice cream with respect to time is the derivative of craves ice cream with respect to hunger times the derivative of hunger with respect to time since last snack. So we plug in the derivatives and plug in the equation for hunger and cancel out the twos and we get the derivative of craves ice cream with respect to time since last snack."
1518,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,544.96,566.08,21.120000000000005," This derivative tells us how quickly or slowly our craving for ice cream changes with respect to time. Double bam. In this last example, it was obvious that hunger was the link between time since last snack and craves ice cream."
1519,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,566.08,588.56,22.479999999999905," And we had an equation for hunger in terms of time and an equation for craves ice cream in terms of hunger. However, usually these relationships are not so obvious. Instead of having two separate equations, we usually get the first equation jammed into the second."
1520,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,588.56,613.0400000000001,24.48000000000013," And when all you have is this, it's not so obvious how the chain rule applies. So we can talk about how to apply the chain rule in this situation. Let us scooch the equation to the left so we have room to work. Now, one thing we can do in this situation is look for things in the equation that can be put in parentheses."
1521,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,613.0400000000001,648.76,35.719999999999914," For example, the square root symbol can be replaced with parentheses. Now we can say that the stuff inside the parentheses is time squared plus one half. The craves ice cream can be rewritten as the square root of the stuff inside. Now the chain rule tells us that the derivative of craves ice cream with respect to time is the derivative of craves ice cream with respect to the stuff inside times the derivative"
1522,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,648.76,680.12,31.360000000000014, of the stuff inside with respect to time. The power rule gives us the derivative of craves ice cream with respect to the stuff inside and the power rule gives us the derivative of the stuff inside with respect to time. Now we just plug the derivatives into the chain rule and plug in the equation for the stuff inside cancel out the twos and we get the derivative of craves ice cream with respect
1523,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,680.12,699.08,18.96000000000004," to the time since last snack. And that's exactly what we got before. Bam! Now let's look at how the chain rule applies to the residual sum of squares, a commonly used loss function in machine learning."
1524,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,699.08,723.6800000000001,24.600000000000023," Note, if this does not make any sense to you, just imagine I said, now let's look at one last example. Imagine we measured someone's weight and height and we wanted to fit this green line to the measurement. Now to keep things simple, let's assume we can only move the green line up and down."
1525,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,723.6800000000001,750.36,26.67999999999995," The equation for the green line is height equals the intercept plus one times weight. And we can change the intercept, but to keep things simple we can't change the slope which is set to one. If we set the intercept to zero, then this location on the green line is the predicted height."
1526,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,750.36,778.96,28.600000000000023," And we can calculate the residual, the difference between the observed height and the value predicted by the line. And we can plot the residual on this graph, which has the intercept on the x-axis and the residual on the y-axis. If we change the intercept here, then we can see the change in the residual here."
1527,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,778.96,813.04,34.07999999999993," And because a common way to evaluate how good the green line fits the data is the squared residual, we can plot the squared residual. Here, where we have the residuals on the x-axis and the squared residuals on the y-axis. Now if we change the intercept here, then we change the residual here and here. And changing the residual here changes the squared residual here."
1528,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,813.04,846.92,33.879999999999995," In order to find the value for the intercept that minimizes the squared residual, we are going to find the derivative of the squared residual with respect to the intercept. And then we're going to find where the derivative equals zero, because given the function y equals the residual squared, the derivative is zero at the lowest point. The chain rule says that because the residual links the intercept to the squared residual,"
1529,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,846.92,873.72,26.800000000000068," then the derivative of the squared residual with respect to the intermediate. It's the derivative of the squared residual with respect to the residual, when the derivative of theия dot data goes straight into power. So let's plug that in. To solve for the derivative of the residual with respect to the intercept,"
1530,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,873.72,898.72,25.0," we move the equation for the residual over here so we have room to work. Then we plug in the equation for the predicted height. Then in order to remove these parentheses, we multiply everything inside by negative 1. Now, the derivative of the residual with respect to the intercept is zero"
1531,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,898.72,923.72,25.0," because this term does not contain the intercept, plus negative 1 because the derivative of the negative intercept equals negative 1, plus zero because the last term does not contain the intercept. Now do the math and we are left with negative 1. And that makes sense because the derivative is just the slope of the orange line."
1532,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,923.72,957.72,34.0," And by I, we can see that the slope of the orange line is negative 1. So let's plug this derivative in here and do a little math and plug in the equation for the residual. Now we have the derivative for the residual squared in terms of the intercept. Note, even instead of starting with separate equations for the residual and the residual squared, we started with just the equation for the residual squared with the equation for the predicted height"
1533,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,957.72,991.72,34.0," jammed into it, then just like before, we can use parentheses to help us out. In this case, we'll call everything between the outermost parentheses the stuff inside, which equals the observed minus the intercept minus 1 times weight. And that means the residual squared can be rewritten as the square of the stuff inside. Now we can use the chain rule to determine the derivative of the residual squared with respect to the intercept."
1534,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,991.72,1021.72,30.0," It's the derivative of the residual squared with respect to the stuff inside, times the derivative of the stuff inside with respect to the intercept. Just like before, the derivative of the residual with respect to the stuff inside is two times the stuff inside. So we plug that into the chain rule. And the derivative of the stuff inside with respect to the intercept is negative 1."
1535,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,1021.72,1046.72,25.0," So we plug that into the chain rule. Now we just plug in the stuff inside, multiply 2 with negative 1, and we end up with the exact same derivative as before. Bam. Now we want to find the value for the intercept such that the derivative of the residual squared equals zero."
1536,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,1046.72,1076.72,30.0," So we plug in the observed height and the observed weight, set the derivative equal to zero, and solve for the intercept. And at long last, we see that when the intercept equals 1, we minimize the squared residual, and we have the best fitting line. Triple-Bam."
1537,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,1076.72,1096.72,20.0," Hooray! We've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of the stack quest study gods, or a t-shirt or a hoodie, or just donate."
1538,The Chain Rule,https://www.youtube.com/watch?v=wl1myxrtQHQ,wl1myxrtQHQ,1096.72,1102.72,6.0," The links are in the description below. Alright, until next time, quest on."
1539,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,0.0,28.0,28.0," Gradient descent is decent at estimating parameters. Statquest. Hello, I'm Josh Starmer and welcome to Statquest. Today we're going to learn about gradient descent and we're going to go through the algorithm step by step. Note, this Statquest assumes you already understand the basics of least squares and linear regression."
1540,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,28.0,55.0,27.0," So if you're not already down with that, check out the Quest. In statistics, machine learning, and other data science fields, we optimize a lot of stuff. When we fit a line with linear regression, we optimize the intercept and the slope. When we use logistic regression, we optimize a squiggle. And when we use T-S-S-E, we optimize clusters."
1541,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,55.0,85.0,30.0," These are just a few examples of the stuff we optimize. They're tons more. The cool thing is that gradient descent can optimize all these things and much more. So if we learn how to optimize this line using gradient descent, then we'll have learned the strategy that optimizes this squiggle. And these clusters. And many more of the optimization problems we have in statistics, machine learning, and data science."
1542,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,85.0,116.0,31.0," So let's start with a simple data set. On the x-axis, we have weight. On the y-axis, we have height. If we fit a line to the data, and someone tells us that they weigh 1.5, we can use the line to predict that they will be 1.9 tall. So let's learn how gradient descent can fit a line to data by finding the optimal values for the intercept and the slope."
1543,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,116.0,147.0,31.0," Actually, we'll start by using gradient descent to find the intercept. Then, once we understand how gradient descent works, we'll use it to solve for the intercept and the slope. So for now, let's just plug in the least squares estimate for the slope, 0.64. And we'll use gradient descent to find the optimal value for the intercept. The first thing we do is pick a random value for the intercept."
1544,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,147.0,176.0,29.0," This is just an initial guess that gives gradient descent something to improve upon. In this case, we'll use 0, but any number will do. And that gives us the equation for this line. In this example, we will evaluate how well this line fits the data with the sum of the squared residuals. Note, in machine learning lingo, the sum of the squared residuals is a type of loss function."
1545,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,176.0,207.0,31.0," We'll talk more about loss functions towards the end of the video. We'll start by calculating this residual. This data point represents a person with weight, 0.5, and height, 1.4. We get the predicted height, the point on the line, by plugging weight equal 0.5 into the equation for the line. And the predicted height is 0.32."
1546,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,207.0,231.0,24.0, The residual is the difference between the observed height and the predicted height. So we calculate the difference between 1.4 and 0.32. And that gives us 1.1 for the residual. We'll keep track of the sum of the squared residuals up here. Here's the square of the first residual.
1547,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,231.0,265.0,34.0," The second residual is 0.4, and the third residual is 1.3. In the end, 3.1 is the sum of the squared residuals. Now, just for fun, we can plot that value on a graph. This graph has the sum of squared residuals on the y-axis, and different values for the intercept on the x-axis. This point represents the sum of the squared residuals when the intercept equals 0."
1548,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,266.0,295.0,29.0," However, if the intercept equals 0.25, then we would get this point on the graph. And if the intercept equals 0.5, then we would get this point. And for increasing values for the intercept, we get these points. Of the points that we calculated for the graph, this one has the lowest sum of squared residuals. But is it the best we can do?"
1549,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,295.0,315.0,20.0, What if the best value for the intercept is somewhere between these values? A slow and painful method for finding the minimal sum of the squared residuals is to plug and chug a bunch more values for the intercept. Ug. Don't despair. Gradient descent is way more efficient.
1550,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,315.0,344.0,29.0," Gradient descent only does a few calculations far from the optimal solution. And increases the number of calculations closer to the optimal value. In other words, gradient descent identifies the optimal value by taking big steps when it is far away, and baby steps when it is close. So let's get back to using gradient descent to find the optimal value for the intercept, starting from a random value."
1551,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,344.0,377.0,33.0," In this case, the random value was 0. When we calculated the sum of the squared residuals, the first residual was the difference between the observed height, which was 1.4, and the predicted height, which came from the equation for this line. So we replaced predicted height with the equation for the line. Since the individual weighs 0.5, we replace weight with 0.5."
1552,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,377.0,407.0,30.0," So for this individual, this is their observed height, and this is their predicted height. Note, we can now plug in any value for the intercept and get a new predicted height. Now let's focus on the second data point. Just like before, the residual is the difference between the observed height, which is 1.9, and the predicted height, which comes from the equation for the line."
1553,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,407.0,438.0,31.0," And since this individual weighs 2.3, we replace weight with 2.3. Now let's focus on the last person. Again, the residual is the difference between the observed height, which is 3.2, and the predicted height, which comes from the equation for the line. And since this person weighs 2.9, we'll replace weight with 2.9."
1554,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,438.0,475.0,37.0," Now we can easily plug in any value for the intercept and get the sum of the squared residuals. Thus, we now have an equation for this curve, and we can take the derivative of this function and determine the slope at any value for the intercept. So let's take the derivative of the趣, which comes from the root of the Después and the measured length, and the derivative of the individual weight that passes up to value to be a Reese's absolute value. So, let's take the derivative of the squared residuals, with the speeds of the five and eight."
1555,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,475.0,504.0,29.0," So let's take the derivative ofường, a positive, and opening your signal messages, let's start by taking the derivative of the first part. First, we'll move this part of the equation up here so that we have room to work. To take the derivative of this, we need to apply the changing rule. So we start by moving the square to the front, and multiply that by the derivative of the stuff inside the parentheses."
1556,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,504.0,521.0,17.0," Whoop-pip-pip-pip-pip-pip-pip-pip-pip-pip. These parts don't contain a term for the intercept, so they go away. Pip-pip-pip. Then we simplify by multiplying two by negative one. And this is the derivative of the first part."
1557,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,521.0,540.0,19.0, So we plug it in. Now we need to take the derivative of the next two parts. I'll leave that as an exercise for the viewer. Bam! Let's move the derivative up here so that it's not taking up half the screen.
1558,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,540.0,579.0,39.0," Now that we have the derivative, gradient descent will use it to find where the sum of squared residuals is lowest. Note, if we were using least squares to solve for the optimal value for the intercept, we would simply find where the slope of the curve equals zero. In contrast, gradient descent finds the minimum value by taking steps from an initial guess until it reaches the best value. This makes gradient descent very useful when it is not possible to solve for where the derivative equals zero. And this is why gradient descent can be used in so many different situations."
1559,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,579.0,609.0,30.0," Bam! Remember, we started by setting the intercept to a random number, in this case, that was zero. So we plug zero into the derivative, and we get negative 5.7. So in the intercept equals zero, the slope of the curve equals negative 5.7. Note, the closer we get to the optimal value for the intercept, the closer the slope of the curve gets to zero."
1560,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,609.0,655.0,46.0," This means that when the slope of the curve is close to zero, then we should take baby steps because we are close to the optimal value. And when the slope is far from zero, then we should take big steps because we are far from the optimal value. However, if we take a super huge step, then we would increase the sum of the squared residuals. So the size of the step should be related to the slope, since it tells us if we should take a baby step or a big step, but we need to make sure the big step is not too big. Gradient descent determines the step size by multiplying the slope by a small number called the learning rate."
1561,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,655.0,682.0,27.0," Note, we'll talk more about learning rates later. When the intercept equals zero, the step size equals negative 5.7. With the step size, we can calculate a new intercept. The new intercept is the old intercept minus the step size. So we plug in the numbers, and the new intercept equals 0.57."
1562,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,683.0,714.0,31.0," Bam! In one big step, we moved much closer to the optimal value for the intercept. Going back to the original data and the original line with the intercept equals zero, we can see how much the residual shrink when the intercept equals 0.57. Now let's take another step closer to the optimal value for the intercept. To take another step, we go back to the derivative and plug in the new intercept."
1563,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,714.0,738.0,24.0," And that tells us the slope of the curve equals negative 2.3. Now let's calculate the step size. By plugging in negative 2.3 for the slope and 0.1 for the learning rate. Ultimately, the step size is negative 2.3. And the new intercept equals 0.8."
1564,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,738.0,765.0,27.0," Now we can compare the residuals when the intercept equals 0.57 to when the intercept equals 0.8. Overall, the sum of the squared residuals is getting smaller. Notice that the first step was relatively large compared to the second step. Now let's calculate the derivative at the new intercept. And we get negative 0.9."
1565,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,765.0,796.0,31.0, The step size equals negative 0.09 and the new intercept equals 0.89. Now we increase the intercept from 0.8 to 0.89. Then we take another step and the new intercept equals 0.92. And then we take another step and the new intercept equals 0.94. And then we take another step and the new intercept equals 0.95.
1566,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,796.0,827.0,31.0," Notice how each step gets smaller and smaller the closer we get to the bottom of the curve. After 6 steps, the gradient to send estimate for the intercept is 0.95. Note, the least squares estimate for the intercept is also 0.95. So we know that gradient to send has done its job, but without comparing its solution to a gold standard, how does gradient to send no to stop taking steps?"
1567,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,827.0,866.0,39.0," Gradient to send stops when the step size is very close to 0. The step size will be very close to 0 when the slope is very close to 0. In practice, the minimum step size equals 0.001 or smaller. So if this slope equals 0.009, then we would plug in 0.009 for the slope and 0.1 for the learning rate. And gets 0.009, which is smaller than 0.001, so gradient to send would stop."
1568,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,867.0,902.0,35.0," That said, gradient to send also includes a limit on the number of steps it will take before giving up. In practice, the maximum number of steps equals 1.000 or greater. So, even if the step size is large, if there have been more than the maximum number of steps, gradient to send will stop. Okay, let's review what we've learned so far. The first thing we did is decide to use the sum of the squared residuals as the loss function to evaluate how well align fits the data."
1569,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,902.0,921.0,19.0," Then we took the derivative of the sum of the squared residuals. In other words, we took the derivative of the loss function. Then we picked a random value for the intercept. In this case, we set the intercept to be equal to 0. Then we calculated the derivative when the intercept equals 0."
1570,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,921.0,948.0,27.0," Plug that slope into the step size calculation, and then calculated the new intercept, the difference between the old intercept and the step size. Lastly, we plugged the new intercept into the derivative and repeated everything until step size was close to 0. Double-bowl. Now that we understand how gradient to send can calculate the intercept,"
1571,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,948.0,974.0,26.0," let's talk about how to estimate the intercept and the slope. Just like before, we'll use the sum of the squared residuals as the loss function. This is a 3D graph of the loss function for the different values for the intercept and the slope. This axis is the sum of the squared residuals. This axis represents different values for the slope."
1572,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,974.0,1003.0,29.0," And this axis represents different values for the intercept. We want to find the values for the intercept and slope that give us the minimum sum of the squared residuals. So, just like before, we need to take the derivative of this function. And just like before, we'll take the derivative with respect to the intercept. But unlike before, we'll also take the derivative with respect to the slope."
1573,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1004.0,1028.0,24.0," We'll start by taking the derivative with respect to the intercept. Just like before, we'll take the derivative of each part. And just like before, we'll use the chain and move the square to the front. And multiply that by the derivative of the stuff inside the parentheses. Boop-ba-ba-ba-ba-ba-ba."
1574,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1029.0,1048.0,19.0," Since we're taking the derivative with respect to the intercept, we treat the slope like a constant. And the derivative of a constant is 0. So, we end up with negative 1 just like before. Beep-boop. Then we simplify by multiplying 2 by negative 1."
1575,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1048.0,1072.0,24.0," And this is the derivative of the first part. So we plug it in. Likewise, we replace these terms with their derivatives. So this whole thing is the derivative of the sum of squared residuals with respect to the intercept. Now let's take the derivative of the sum of the squared residuals with respect to the slope."
1576,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1072.0,1097.0,25.0," Just like before, we take the derivative of each part. And just like before, we'll use the chain rule to move the square to the front. And multiply that by the derivative of the stuff inside the parentheses. Beep-boop-boop. Since we are taking the derivative with respect to the slope, we treat the intercept like a constant."
1577,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1097.0,1122.0,25.0," And the derivative of a constant is 0. So we end up with negative 0.5. Beep-boop. Then we simplify by moving the negative 0.5 to the front. Note, I left the 0.5 and bold instead of multiplying it by 2 to remind us that 0.5 is the weight for the first sample."
1578,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1122.0,1151.0,29.0," And this is the derivative of the first part. So we plug it in. Likewise, we replace these terms with their derivatives. Again, 2.3 and 2.9 are in bold to remind us that they are the weights of the second and third samples. Here's the derivative of the sum of the squared residuals with respect to the intercept."
1579,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1151.0,1175.0,24.0," And here's the derivative with respect to the slope. And when you have two or more derivatives of the same function, they are called a gradient. We will use this gradient to descend to the lowest point in the loss function, which, in this case, is the sum of the squared residuals. Thus, this is why the algorithm is called gradient-assent."
1580,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1175.0,1192.0,17.0," Bam. Just like before, we'll start by picking a random number for the intercept. In this case, we'll set the intercept to be equal to 0. And we'll pick a random number for the slope. In this case, we'll set the slope to be 1."
1581,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1192.0,1218.0,26.0," Thus, this line with intercept equal 0 and slope equal 1 is where we will start. Now let's plug in 0 for the intercept and 1 for the slope. And that gives us 2 slopes. Now we plug the slopes into the step-size formulas. And multiply by the learning rate, which this time we set to 0.01."
1582,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1218.0,1251.0,33.0," Note, the larger learning rate that we used in the first example doesn't work this time. Even after a bunch of steps, gradient-assent doesn't arrive at the correct answer. This means that gradient-assent can be very sensitive to the learning rate. The good news is that in practice, a reasonable learning rate can be determined automatically by starting large and getting smaller with each step. So, in theory, you shouldn't have to worry too much about the learning rate."
1583,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1251.0,1285.0,34.0," Anyway, we do the math and get two step sizes. Now we calculate the new intercept and new slope by plugging in the old intercept and the old slope and the step sizes. And we end up with a new intercept and a new slope. This is the line we started with, and this is the new line after the first step. Now we just repeat what we did until all of the step sizes are very small or we reach the maximum number of steps."
1584,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1285.0,1312.0,27.0," This is the best fitting line with intercept equal 0.95 and slope equal 0.64. The same values we get from least squares. Double-bowl. We now know how gradient-assent optimizes two parameters, the slope and the intercept. If we had more parameters, then we just take more derivatives and everything else stays the same."
1585,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1312.0,1335.0,23.0," Triple-bowl. Note, the sum of the squared residuals is just one type of loss function. However, there are tons of other loss functions that work with other types of data. Regardless of which loss function you use, gradient-assent works the same way. Step 1."
1586,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1335.0,1350.0,15.0," Take the derivative of the loss function for each parameter in it. In fancy machine learning lingo, take the gradient of the loss function. Step 2. Pick random values for the parameters. Step 3."
1587,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1350.0,1360.0,10.0, Plug the parameter values into the derivatives. A hem. The gradient. Step 4. Calculate the step sizes.
1588,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1360.0,1382.0,22.0," Step 5. Calculate the new parameters. Now go back to step 3 and repeat until step sizes very small or you reach the maximum number of steps. One last thing before we're done. In our example, we only had three data points so the math didn't take very long."
1589,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1382.0,1409.0,27.0," But when you have millions of data points, it can take a long time. So there is a thing called stochastic gradient-assent that uses a randomly selected subset of the data at every step rather than the full data set. This reduces the time spent calculating the derivatives of the loss function. That's all. Stochastic gradient-assent sounds fancy, but it's no big deal."
1590,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1409.0,1428.0,19.0," Hooray! We've made it to the end of another exciting step quest. If you like this step quest and want to see more, please subscribe. And if you want to support step quest, well, consider buying one or two of my original songs or buying a step quest T-shirt or hoodie. The links to do this are in the description below."
1591,"Gradient Descent, Step-by-Step",https://www.youtube.com/watch?v=sDv4f4s2SB8,sDv4f4s2SB8,1428.0,1431.0,3.0," Alright, until next time, quest on."
1592,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,0.0,12.0,12.0, Gonna do something crazy. Gonna do something random. It's got a B still cast it. It's got a B. Stach quest.
1593,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,12.0,31.0,19.0," Hello, I'm Josh Stormer and welcome to Stach quest. Today we're going to talk about Stochastic Gradient Assent and it's going to be clearly explained. Note, this Stach quest assumes that you are already familiar with Gradient Assent. If not, check out the quest. The link is in the description below."
1594,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,31.0,55.0,24.0," This video picks up where the original leaves off, providing more details about how Stochastic Gradient Assent works and some of its more subtle advantages. Now, even though I just said you need to watch the Stach quest on Gradient Assent, let's do a little review to demonstrate the problem that Stochastic Gradient Assent solves. In the Stach quest on Gradient Assent,"
1595,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,55.0,82.0,27.0," we took this simple data set, height, and weight measurements from three different people, and we wanted to fit a line to it using Gradient Assent. However, at first we started out with this generic equation for a line. And the goal was to find the optimal values for the intercept and the slope. For example, if we started with the intercept equals zero and the slope equals one,"
1596,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,83.0,119.0,36.0," then we could use weight to predict height. Then we would use the sum of the squared residuals as the loss function to determine how well the initial line fit the data. Note, the sum of the squared residuals is just one of many different loss functions that can evaluate how well something fits the data. In this case, that something is a line. To find the optimal values for the intercept and slope, we plug the equation for the predicted height into the sum of the squared residuals."
1597,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,119.0,143.0,24.0," Then we took the derivative of the sum of the squared residuals with respect to the intercept, and with respect to the slope. Then we plugged in the values from the observed data into the derivative with respect to the intercept, and then we did the same thing for the derivative with respect to the slope. Then we plugged in the initial gas for the intercept, zero,"
1598,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,143.0,168.0,25.0," and the initial gas for the slope, one. We did the math, plugged the slopes into the step size formulas, and multiplied by the learning rate, which we set to 0.01. Then we did the math, calculated the new intercept, and new slope by plugging in the old intercept and old slope,"
1599,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,168.0,197.0,29.0," and the step sizes, and we did the math, and we ended up with a new intercept and a new slope. Then we went back to the derivatives and repeated the process a lot of times until we took the maximum number of steps, or the steps became very, very small. In this super simple example, we were just fitting a line with two parameters, the intercept and the slope,"
1600,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,197.0,217.0,20.0," and we only had three data points. So we only had three terms to compute each step for the intercept, and we only had three terms to compute each step for the slope. So each step didn't require much math. But what if we had a more complicated model,"
1601,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,217.0,247.0,30.0," like a logistic regression that used 23,000 genes to predict if someone will have a disease? Then we will have 23,000 derivatives to plug the data into. And what if we had data from 1 million samples? Then we would have to calculate 1 million terms for each of the 23,000 derivatives. In other words, we'd have to calculate 23 billion terms for each step."
1602,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,247.0,273.0,26.0," And since it's common to take at least 1,000 steps, we would calculate at least 2.3 trillion terms. So for big data, gradient descent is slow. This is where stochastic gradient descent comes in handy. Going back to our super simple example, stochastic gradient descent would randomly pick one sample for each step,"
1603,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,273.0,296.0,23.0," and just use that one sample to calculate the derivatives. Thus, in this super simple example, stochastic gradient descent reduced the number of terms computed by a factor of 3. If we had one million samples, then stochastic gradient descent would reduce the amount of terms computed by a factor of 1 million."
1604,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,296.0,320.0,24.0," So that's pretty cool. Stochastic gradient descent is especially useful when there are redundancies in the data. For example, we have 12 data points, but there is a lot of redundancy that forms three clusters. So we start with a line with the intercept equal zero and the slope equals one,"
1605,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,320.0,346.0,26.0," then we randomly pick this point. So we plug in the weight, three, and height, 3.3. Do the math, plug in the slopes, then multiply by the learning rate. Note, just like with regular gradient descent, stochastic gradient descent is sensitive to the value you choose for the learning rate."
1606,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,346.0,366.0,20.0," And just like for regular gradient descent, the general strategy is to start with a relatively large learning rate and make it smaller with each step. And lastly, just like for regular gradient descent, many implementations of stochastic gradient descent will take care of this for you by default."
1607,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,367.0,393.0,26.0," Oh no, it's a terminology alert. The way the learning rate changes from relatively large to relatively small is called the schedule. So if you fail to converge on parameter estimates, try fudging with this setting. In this simple example, however, we're just setting the learning rate to zero point zero one. Now we do the math."
1608,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,394.0,417.0,23.0, Calculate the new intercept and the new slope. Bam! The new parameters give us this new line. Then we randomly pick another point and calculate the intercept and slope for another line. Then we just repeat everything a bunch of times.
1609,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,418.0,449.0,31.0," And ultimately, we end up with a line where the intercept equals zero point eight five and the slope equals zero point six eight. And the least squares estimates, aka the gold standard, gives a line where the intercept equals zero point eight seven and the slope equals zero point six eight. Bam! Note, the strict definition of stochastic gradient descent is to only use one sample per step."
1610,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,449.0,489.0,40.0," However, it is much more common to select a small subset of data or many batch for each step. For example, we could use three samples per step instead of just one. Using a mini batch for each step takes the best of both worlds between using just one sample and all of the data at each step. Similar to using all of the data, using a mini batch can result in more stable estimates for the parameters in fewer steps. And like using just one sample per step, using a mini batch is much faster than using all of the data."
1611,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,490.0,534.0,44.0," In this example, using three samples per step, we ended up with an intercept equals zero point eight six and the slope equals zero point six eight. Which means that the estimate for the intercept was just a little closer to the gold standard zero point eight seven, then when we used one sample and got zero point eight five. Double bam! One cool thing about stochastic gradient descent is that when we get new data, we can easily use it to take another step for the parameter estimates without having to start from scratch. In other words, we don't have to go all the way back to the initial guesses for the slope and intercept and redo everything."
1612,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,534.0,584.0,50.0," Instead, we pick up right where we left off and take one more step using the new sample. So we plug in the weight from the new sample 1.1 and the height 2 to the math, plug in the slopes, then multiply by the learning rate zero point zero one, do the math, calculate the new intercept, not from the initial guess, but from the most recent estimate. And calculate the new slope from the most recent estimate. And the new line has intercept equals zero point eight seven eight and slope equals zero point seven. Triple bam!"
1613,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,584.0,623.0,39.0," We updated the parameters for the line with just the new data. In summary, stochastic gradient descent is just like regular gradient descent, except it only looks at one sample per step, or a small subset or mini batch for each step. Stochastic gradient descent is great when we have tons of data and lots of parameters. In these situations, regular gradient descent may not be computationally feasible, and it's cool that we can easily update the parameters when new data shows up."
1614,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,623.0,645.0,22.0," Whoay! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, consider buying one or two of my original songs or getting a t-shirt. The links to do this are in the description below. Alright, until next time, quest on!"
1615,"Stochastic Gradient Descent, Clearly Explained!!!",https://www.youtube.com/watch?v=vMh0zPT0tLI,vMh0zPT0tLI,647.0,649.0,2.0, Thanks for watching!
1616,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,0.0,29.0,29.0," Machine learning sounds so complicated but it's not so complicated. Stack Quest Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to cover adaboozed and it's going to be clearly explained. Note, this Stack Quest shows how to combine adaboozed with decision trees because that is the most common way to use adaboozed."
1617,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,29.0,64.0,35.0," So, if you're not familiar with decision trees, check out the Quest. We will also mention random forests, so if you don't know about them, check out the Quest. We'll start by using decision trees and random forests to explain the three concepts behind adaboozed. Then we'll get into the nitty-gritty details of how adaboozed creates a forest of trees from scratch and how it's used to make classifications. So let's start by using decision trees and random forests to explain the three main concepts behind adaboozed."
1618,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,64.0,95.0,31.0," In a random forest, each time you make a tree, you make a full-sized tree. Some trees might be bigger than others but there's no predetermined maximum depth. In contrast, in a forest of trees made with adaboozed, the trees are usually just a node and two leaves. Oh, no, it's the dreaded terminology alert. A tree with just one node and two leaves is called a stump."
1619,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,95.0,123.0,28.0," So this is really a forest of stumps rather than trees. Stumps are not great at making accurate classifications. For example, if we were using this data to determine if someone had heart disease or not, then a full-sized decision tree would take advantage of all four variables that we measured, chest pain, blood circulation, blocked arteries and weight to make a decision."
1620,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,123.0,149.0,26.0," But a stump can only use one variable to make a decision. Thus, stumps are technically weak learners. However, that's the way adaboozed likes it and it's one of the reasons why they are so commonly combined. Now back to the random forest. In a random forest, each tree has an equal vote on the final classification."
1621,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,149.0,179.0,30.0," This trees vote is worth just as much as this trees vote or this trees vote. In contrast, in a forest of stumps made with adaboozed, some stumps get more say in the final classification than others. In this illustration, the larger stumps get more say in the final classification than the smaller stumps. Lastly, in a random forest, each decision tree is made independently of the others."
1622,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,179.0,210.0,31.0," In other words, it doesn't matter if this tree was made first or this one. In contrast, in a forest of stumps made with adaboozed, order is important. The ares that the first stump makes, influence how the second stump is made, and the ares that the second stump makes, influence how the third stump is made. It's et cetera, et cetera."
1623,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,210.0,236.0,26.0," To review, the three ideas behind adaboozed are, one, adaboozed combines a lot of weak learners to make classifications. The weak learners are almost always stumps. Two, some stumps get more say in the classification than others. Three, each stump is made by taking the previous stumps mistakes into account."
1624,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,236.0,262.0,26.0," Bam! Now let's dive into the nitty-gritty detail of how to create a forest of stumps using adaboozed. First, we'll start with some data. We create a forest of stumps with adaboozed to predict if a patient has heart disease. We will make these predictions based on a patient's chest pain and blocked artery status and their weight."
1625,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,263.0,287.0,24.0," The first thing we do is give each sample a weight that indicates how important it is to be correctly classified. Note, the sample weight is different from the patient weight, and I'll do the best I can to be clear about which of the two I'm talking about. At the start, all samples get the same weight. One divided by the total number of samples."
1626,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,287.0,312.0,25.0," In this case, that's one divided by eight. And that makes the samples all equally important. However, after we make the first stump, these weights will change in order to guide how the next stump is created. In other words, we'll talk more about the sample weights later. Now we need to make the first stump in the forest."
1627,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,312.0,340.0,28.0," This is done by finding the variable, chest pain, blocked arteries, or patient weight, that does the best job classifying the samples. Note, because all of the weights are the same, we can ignore them right now. We start by seeing how well chest pain classifies the samples. Of the five samples with chest pain, three were correctly classified as having heart disease,"
1628,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,340.0,367.0,27.0," and two were incorrectly classified. Of the three samples without chest pain, two were correctly classified as not having heart disease, and one was incorrectly classified. Now we do the same thing for blocked arteries, and for patient weight. Note, we used the techniques described in the decision tree stat quest"
1629,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,367.0,400.0,33.0," to determine that 176 was the best weight to separate the patients. Now we calculate the genie index for the three stumps. The genie index for patient weight is the lowest, so this will be the first stump in the forest. Now we need to determine how much say this stump will have in the final classification. Remember, some stumps get more say in the final classification than others."
1630,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,400.0,434.0,34.0," We determine how much say a stump has in the final classification, based on how well it classified the samples. This stump made one error. This patient, who weighs less than 176, has heart disease, but the stump says they do not. The total error for a stump is the sum of the weights associated with the incorrectly classified samples. Thus, in this case, the total error is 1-8."
1631,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,434.0,479.0,45.0," Note, because all of the sample weights add up to 1, total error will always be between 0 for a perfect stump and 1 for a horrible stump. We use the total error to determine the amount of say this stump has in the final classification with the following formula. A amount of say equals 1 half times the log of 1- the total error divided by the total error. We can draw a graph of the amount of say by plugging in a bunch of numbers between 0 and 1 for total error. The blue line tells us the amount of say for total error values between 0 and 1."
1632,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,479.0,522.0,43.0," When a stump does a good job and the total error is small, then the amount of say is a relatively large positive value. When a stump is no better at classification than flipping a coin, i.e., half the stumps are correctly classified and half are incorrectly classified, and the total error equals 0.5, then the amount of say will be 0. And when a stump does a terrible job and the total error is close to 1, in other words, if the stump consistently gives you the opposite classification, then the amount of say will be a large negative value."
1633,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,522.0,561.0,39.0," So if a stump votes for heart disease, the negative amount of say will turn that vote into not heart disease. Note, if total error is 1 or 0, then this equation will freak out. In practice, a small error term is added to prevent this from happening. With patient weight greater than 176, the total error is 1-8, so we just plug and chug. But you did it, but you do, but you do, and the amount of say that this stump has in the final classification is 0.97."
1634,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,561.0,592.0,31.0," Bam! Now that we've worked out how much say this stump gets when classifying a sample, let's work out how much say the chest pain stump would have if it had been the best stump. Note, we don't need to do this, but I think it helps illustrate the concepts we've covered so far. Chest pain made 3 errors. And the total error equals the sum of the weights for the incorrectly classified samples."
1635,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,592.0,622.0,30.0, B-d-d-d-d-d-d-d-d. So the total error for chest pain is 3-8s. We can get a sense of what the amount of say will be by looking at the graph when total error equals 3-8s. So we are expecting the amount of say to be between 0 and 0.5. Now we plug 3-8 into the formula for the amount of say and do the math.
1636,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,622.0,655.0,33.0, The amount of say that the chest pain stump would have had on the final classification is 0.42. I'll leave the blocked artery stump as an exercise for the viewer. Now we know how the sample weights for the incorrectly classified samples are used to determine the amount of say each stump gets. Bam! Now we need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account.
1637,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,655.0,688.0,33.0," Let's go back to the first stump that we made. When we created this stump, all of the sample weights were the same. In that meant, we did not emphasize the importance of correctly classifying any particular sample. But since this stump incorrectly classified this sample, we will emphasize the need for the next stump to correctly classify it by increasing its sample weight. And decreasing all of the other sample weights."
1638,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,688.0,724.0,36.0," Let's start by increasing the sample weight for the incorrectly classified sample. This is the formula we will use to increase the sample weight for the sample that was incorrectly classified. We plug in the sample weight from the last stump and we scale one eighth with this term. To get a better understanding of how this part will scale the previous sample weight, let's draw a graph. The blue line is equal to E raised to the amount of say."
1639,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,724.0,756.0,32.0," When the amount of say is relatively large, IE, the last stump did a good job classifying samples. Then we will scale the previous sample weight with a large number. This means that the new sample weight will be much larger than the old one. And when the amount of say is relatively low, IE, the last stump did not do a very good job classifying samples. Then the previous sample weight is scaled by a relatively small number."
1640,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,756.0,784.0,28.0," This means that the new sample weight will only be a little larger than the old one. In this example, the amount of say was 0.97. And E raised to the 0.97 equals 2.64. That means the new sample weight is 0.33, which is more than the old one. Bam."
1641,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,784.0,816.0,32.0," Now we need to decrease the sample weights for all of the correctly classified samples. This is the formula we will use to decrease the sample weights. The big difference is the negative sign in front of a amount of say. Just like before, we plug in the sample weight. And just like before, we can get a better understanding of how this will scale the sample weight by plotting a graph using different values for a amount of say."
1642,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,816.0,851.0,35.0," The blue line represents E raised to the negative amount of say. When the amount of say is relatively large, then we scale the sample weight by a value close to 0. This will make the new sample weight very small. If the amount of say for the last stump is relatively small, then we will scale the sample weight by a value close to 1. This means that the new sample weight will be just a little smaller than the old one."
1643,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,851.0,876.0,25.0," In this example, the amount of say was 0.97. And E raised to the negative 0.97 equals 0.38. The new sample weight is 0.05, which is less than the old one. Bam. We will keep track of the new sample weights in this column."
1644,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,876.0,907.0,31.0," We plug in 0.33 for the sample that was incorrectly classified. All of the other samples get 0.05. Now we need to normalize the new sample weights so that they will add up to 1. Right now, if you add up the new sample weights, you get 0.68. So we divide each new sample weight by 0.68 to get the normalized values."
1645,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,907.0,929.0,22.0," Now, when we add up the new sample weights, we get 1. Plus or minus a little rounding error. Now we just transfer the normalized sample weights to the sample weights column, since those are what we will use for the next stump. Now we can use the modified sample weights to make the second stump in the forest."
1646,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,929.0,954.0,25.0," Bam. In theory, we could use the sample weights to calculate weighted genie indexes to determine which variable should split the next stump. The weighted genie index would put more emphasis on correctly classifying this sample. The one that was misclassified by the last stump, since this sample has the largest sample weight."
1647,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,954.0,984.0,30.0," Alternatively, instead of using a weighted genie index, we can make a new collection of samples that contains duplicate copies of the samples with the largest sample weights. So we start by making a new, but empty, data set that is the same size as the original. Then we pick a random number between 0 and 1. And we see where that number falls when you use the sample weights like a distribution."
1648,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,984.0,1012.0,28.0," If the number is between 0 and 0.7, then we would put this sample into the new collection of samples. And if the number is between 0.7 and 0.14, then we would put this sample into the new collection of samples. And if the number is between 0.14 and 0.21, then we would put this sample into the new collection of samples."
1649,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1012.0,1035.0,23.0," And if the number is between 0.21 and 0.70, then we would put this sample into the new collection of samples. Et cetera, et cetera. For example, imagine the first number I picked was 0.72. Then I would put this sample into my new collection of samples."
1650,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1035.0,1055.0,20.0, Then I pick another random number and get 0.42. And I would put this sample into my new collection of samples. Then I pick 0.83. And I would put this sample into my new collection of samples. Then I pick 0.51.
1651,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1055.0,1094.0,39.0," I would put this sample into my new collection of samples. Note, this is the second time we have added this particular sample to the new collection of samples. We then continue to pick random numbers and add samples to the new collection until the new collection is the same size as the original. Ultimately, this sample was added to the new collection of samples for times reflecting its larger sample weight. Now we get rid of the original samples and use the new collection of samples."
1652,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1095.0,1133.0,38.0," Lastly, we give all of the samples equal sample weights, just like before. However, that doesn't mean the next sample will not emphasize the need to correctly classify these samples. Because these samples are all the same, they will be treated as a block, creating a large penalty for being misclassified. Now we go back to the beginning and try to find the stump that does the best job classifying the new collection of samples. So that is how the errors that the first tree makes influence how the second tree is made."
1653,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1133.0,1159.0,26.0," And how the errors that the second tree makes influence how the third tree is made. It's et cetera, et cetera, et cetera. Double bam. Now we need to talk about how a forest of stumps created by Attabooost makes classifications. Imagine that these stumps classified a patient as has heart disease."
1654,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1159.0,1180.0,21.0, And these stumps classified the patient as does not have heart disease. These are the amounts of say for these stumps. And these are the amounts of say for these stumps. Now we add up the amounts of say for this group of stumps. And for this group of stumps.
1655,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1180.0,1211.0,31.0," Ultimately, the patient is classified as has heart disease because this is the larger sum. Triple bam. To review, the three ideas behind Attabooost are, one, Attabooost combines a lot of weak learners to make classifications. The weak learners are almost always stumps. Two, some stumps get more say in the classification than others."
1656,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1211.0,1238.0,27.0," And three, each stump is made by taking the previous stumps mistakes into account. If we have a weighted genie function, then we use it with the sample weights. Otherwise, we use the sample weights to make a new data set that reflects those weights. Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
1657,"AdaBoost, Clearly Explained",https://www.youtube.com/watch?v=LsK-xG1cLYA,LsK-xG1cLYA,1238.0,1252.0,14.0," And if you want to support stat quest, well, consider buying a t-shirt or a hoodie or buying one or two of my original songs. The links to do this are in the description below. Alright, until next time, quest on."
1658,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,0.0,18.88,18.88," Gradient Boost Part 1. Regression Main Ideas. Stat Quest. Hello, I'm Josh Stormer and welcome to Stat Quest. Today we're going to talk about the gradient boost machine learning algorithm."
1659,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,18.88,41.52,22.640000000000004," Specifically, we're going to focus on how gradient boost is used for regression. Note, this Stat Quest assumes you already understand decision trees. So if you're not already down with those, check out the Quest. This Stat Quest also assumes that you are familiar with adabust and the trade-off between bias and variance."
1660,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,41.52,60.92,19.4," If not, check out the Quest. The links are in the description below. This Stat Quest is the first part in a series that explains how the gradient boost machine learning algorithm works. Specifically, we'll use this data where we have the height measurements from six people,"
1661,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,60.92,85.12,24.200000000000003," their favorite colors, their genders, and their weights. And we'll walk through step by step the most common way that gradient boost fits a model to this training data. Note, when gradient boost is used to predict a continuous value like weight, we say that we are using gradient boost for regression."
1662,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,85.12,113.68,28.56," Using gradient boost for regression is different from doing linear regression. So while the two methods are related, don't get them confused with each other. Part two in this series will dive deep into the math behind the gradient boost algorithm for regression, walking through it step by step and proving that what we cover today is correct. Part three in this series shows how gradient boost can be used for classification."
1663,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,113.68,140.56,26.879999999999995," Specifically, we'll walk through step by step the most common way gradient boost can classify someone as either loving the movie troll two or not loving troll two. Part four will return to the math behind gradient boost. This time focusing on classification, walking through it step by step. The gradient boost algorithm looks complicated because it was designed to be configured"
1664,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,140.56,163.16,22.599999999999994," in a wide variety of ways. But the reality is that 99% of the time, only one configuration is used to predict continuous values like weight. And one configuration is used to classify samples into different categories. This stack quest focuses on showing you the most common way gradient boost is used"
1665,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,163.16,189.72,26.56," to predict a continuous value like weight. If you are familiar with adabust, then a lot of gradient boost will seem very similar. So let's briefly compare and contrast adabust and gradient boost. If we want to use these measurements to predict weight, then adabust starts by building a very short tree called a stump from the training data."
1666,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,189.72,215.96,26.24000000000001," And then the amount of say that the new stump has on the final output is based on how well it compensated for those previous errors. Then adabust builds the next stump based on errors that the previous stump made. In this example, the new stump did a poor job compensating for the previous stump's errors and its size reflects its reduced amount of say."
1667,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,215.96,244.12,28.16," Then adabust builds another stump based on the errors made by the previous stump. And this stump did a little better than the last stump so it's a little larger. Then adabust continues to make stumps in this fashion until it has made the number of stumps you asked for or it has a perfect fit. In contrast, gradient boost starts by making a single leaf instead of a tree or stump."
1668,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,244.12,271.76,27.639999999999983," This leaf represents an initial guess for the weights for all of the samples. When trying to predict a continuous value like weight, the first guess is the average value. Then gradient boost builds a tree. Like adabust, this tree is based on the errors made by the previous tree. But unlike adabust, this tree is usually larger than a stump."
1669,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,271.76,299.24,27.480000000000015," But said, gradient boost still restricts the size of the tree. In the simple example that we will go through in this stack quest, we will build trees with up to four leaves but no larger. However, in practice, people often set the maximum number of leaves to be between 8 and 32. Thus, like adabust, gradient boost builds fixed size trees based on the previous trees"
1670,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,299.24,325.68,26.44," errors, but unlike adabust, each tree can be larger than a stump. Also, like adabust, gradient boost scales the trees. However, gradient boost scales all trees by the same amount. Then gradient boost builds another tree based on the errors made by the previous tree. And then it scales the tree."
1671,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,325.72,350.88,25.159999999999968," And gradient boost continues to build trees in this fashion until it has made the number of trees you asked for, or additional trees fail to improve the fit. Now that we know the main similarities and differences between gradient boost and adabust, let's see how the most common gradient boost configuration would use this training data to predict weight."
1672,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,350.88,378.52,27.640000000000043," The first thing we do is calculate the average weight. This is the first attempt at predicting everyone's weight. In other words, if we stopped right now, we would predict that everyone weighed 71.2 kilograms. However, gradient boost doesn't stop here. The next thing we do is build a tree based on the errors from the first tree."
1673,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,378.52,407.24,28.71999999999997," The errors that the previous tree made are the differences between the observed weights and the predicted weight, 71.2. So let's start by plugging in 71.2 for the predicted weight. And then plug in the first observed weight and do the math. And save the difference, which is called a pseudo-residual in a new column."
1674,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,407.24,437.08,29.83999999999997," Note, the term pseudo-residual is based on linear regression, where the difference between the observed values and the predicted values results in residuals. The pseudo part of pseudo-residual is a reminder that we are doing gradient boost, not linear regression. And if something I'll talk more about in part 2 of this series, when we go through the math. Now we do the same thing for the remaining weights."
1675,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,437.08,464.28,27.19999999999999," Now we will build a tree using height, favorite color, and gender to predict the residuals. If it seems strange to predict the residuals instead of the original weights, just bear with me and soon all will become clear. So, setting aside the reason why we are building a tree to predict the residuals for the time being, here's the tree."
1676,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,464.36,493.6,29.24000000000001," Remember, in this example, we are only allowing up to four leaves. But when using a larger data set, it is common to allow anywhere from 8 to 32. By restricting the total number of leaves, we get fewer leaves than residuals. As a result, these two rows of data go to the same leaf. So we replace these residuals with their average."
1677,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,493.6,524.6,31.0," Now we can combine the original leaf with the new tree to make a new prediction of an individual's weight from the training data. We start with the initial prediction, 71.2. Then we run the data down the tree, and we get 16.1. We start with the initial prediction, 71.2. Then we run the data down the tree, and we get 16.8."
1678,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,524.6,558.6,34.0," So the predicted weight equals 71.2, plus 16.8, which equals 88, which is the same as the observed weight. Is this awesome? No. The model fits the training data too well. In other words, we have low bias, but probably very high variance. Gradient boost deals with this problem by using a learning rate to scale the contribution from the new tree."
1679,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,558.6,596.6,38.0," The learning rate is a value between 0 and 1. In this case, we'll set the learning rate to 0.1. Now the predicted weight equals 71.2, plus 0.1 times 16.8, which equals 72.9. With the learning rate set to 0.1, the new prediction isn't as good as it was before, but it's a little better than the prediction made with just the original leaf, which predicted that all samples would weigh 71.2."
1680,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,596.6,636.6,40.0," In other words, scaling the tree by the learning rate results in a small step in the right direction. According to the dude that invented gradient boost, Jerome Friedman, empirical evidence shows that taking lots of small steps in the right direction results in better predictions with a testing data set, i.e. lower variance. Bam! So let's build another tree so we can take another small step in the right direction. Just like before, we calculate the pseudo-residules, the difference between the observed weights and our latest predictions."
1681,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,636.6,660.6,24.0, So we plug in the observed weight and the new predicted weight. And we get 15.1. And we save that in the column for pseudo-residules. Then we repeat for all of the other individuals in the training data set. Small bam.
1682,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,660.6,689.6,29.0," Note, these are the original residuals from when our prediction was simply the average overall weight. And these are the residuals after adding the new tree scaled by the learning rate. The new residuals are all smaller than before, so we've taken a small step in the right direction. Double bam! Now let's build a new tree to predict the new residuals."
1683,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,689.6,717.6,28.0," And here's the new tree. Note, in this simple example, the branches are the same as before. However, in practice, the trees can be different each time. Just like before, since multiple samples ended up in these leaves, we just replace the residuals with their averages. Now we combine the new tree with the previous tree and the initial leaf."
1684,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,717.6,745.6,28.0," Note, we scale all of the trees by the learning rate, which we set to 0.1. And add everything together. Now we're ready to make a new prediction from the training data. Just like before, we start with the initial prediction. Then add the scaled amount from the first tree and the scaled amount from the second tree."
1685,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,745.6,785.6,40.0," That gives us 71.2 plus 0.1 times 16.8 plus 0.1 times 15.1, which equals 74.4, which is another small step closer to the observed weight. Now we use the initial leaf, plus the scaled values from the first tree, plus the scaled values from the second tree to calculate new residuals. Remember, these were the residuals from when we just use a single leaf to predict weight."
1686,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,785.6,813.6,28.0," And these were the residuals after we added the first tree to the prediction. And these are the residuals after we added the second tree to the prediction. Each time we add a tree to the prediction, the residuals get smaller. So we've taken another small step towards making good predictions. Now we build another tree to predict the new residuals."
1687,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,813.6,836.6,23.0," And added to the chain of trees that we have already created. And we keep making trees until we reach the maximum specified, or adding additional trees does not significantly reduce the size of the residuals. Bam. Then, when we get some new measurements, we can predict weight."
1688,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,836.6,857.6,21.0," By starting with the initial prediction, then adding the scaled value from the first tree, and the second tree, and the third tree, et cetera, et cetera, et cetera. And once the math is all done, we are left with a predicted weight."
1689,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,857.6,883.6,26.0," In this case, we predicted that this person weighed 70 kilograms. Triple-bound. In summary, when gradient boost is used for regression, we start with a leaf that is the average value of the variable we want to predict. In this case, we wanted to predict weight."
1690,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,883.6,908.6,25.0," Then we add a tree based on the residuals, the difference between the observed values and the predicted values. And we scale the tree's contribution to the final prediction with a learning rate. Then we add another tree based on the new residuals. And we keep adding trees based on the errors made by the previous tree."
1691,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,908.6,926.6,18.0," That's all there is to it. Bam. Tune in for part 2 in this series, when we dive deep into the math behind the gradient boost algorithm for regression, walking through it step by step and proving that it really is this simple."
1692,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,926.6,945.6,19.0," Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider buying one of my original songs, or buying a stat quest T-shirt or hoodie. The links are in the description below."
1693,Gradient Boost Part 1 (of 4): Regression Main Ideas,https://www.youtube.com/watch?v=3CC4N4z3GJc,3CC4N4z3GJc,945.6,952.6,7.0," All right. Until next time, quest on. Thanks for watching."
1694,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,0.0,27.76,27.76," Grating it boost is awesome, Grating it boost is cool Now we're gonna dive into some regression details, Stat Quest Hello, I'm Josh Starmer and welcome to Stat Quest Today we're going to continue our series on gradient boost Specifically, we're going to dive into the algorithmic details of how gradient boost is used for regression"
1695,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,27.84,54.400000000000006,26.56000000000001," Note, this Stat Quest assumes you have already watched the first video in this series, gradient boost part 1, regression main ideas, if not, check out the quest Also, although not required, it might be helpful if you understand gradient descent So check out that quest if you haven't already In this Stat Quest, we are going to walk through the original gradient boost algorithm"
1696,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,54.48,81.2,26.72000000000001," Step by step In order to keep the example from getting out of hand, we are going to use gradient boost to fit a model to a super simple training dataset, which contains height measurements from three people, their favorite color, their gender, and their weight Great, now that we know all about this super simple training dataset, let's walk through the original gradient"
1697,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,81.28,101.44,20.16, descent algorithm step by step We'll start from the top This line describes in an abstract way the training dataset and the method we will use to evaluate how well the model fits the training dataset This first part is just a fancy way to say that we have some data
1698,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,102.8,130.72,27.919999999999987," The X sub-I's refer to each row of measurements that we will use to predict weight And the Y sub-I's refer to the weights measured for each person in the dataset This part just says that the eyes in X sub-I and Y sub-I go from one to N Where N is the number of people in our dataset In this case, N equals three since we only have three rows of data"
1699,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,131.04,160.79999999999998,29.75999999999999," Altogether, this means that we refer to this row of measurements as X sub-I And we will refer to this weight as Y sub-I This row of measurements is X sub-2 and this weight is Y sub-2 This row of measurements is X sub-3 and this weight is Y sub-3 Now we need a differentiable loss function"
1700,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,161.84,182.08,20.24000000000001," In this case, a loss function is just something that evaluates how well we can predict weight The loss function that is most commonly used doing regression with gradient boost is 1.5 times the observed value minus the predicted value Squared"
1701,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,182.08,207.12,25.039999999999992," Now, just for a few minutes, humor me and ignore the 1.5 in front When we remove the 1.5, we end up with the same loss function we use for linear regression For example, if we had height and weight measurements from three people, Then we could fit a line to the data Residuals are the differences between the observed weights"
1702,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,207.76,236.96,29.200000000000017," And the weights that are predicted by the line We can evaluate how well this greenish line fits the data with the sum of the squared residuals Thus, the loss function is just a squared residual If we wanted to compare how well this greenish line fit the data to this pink line Then we would calculate the residuals the difference between the observed and the predicted values for the greenish"
1703,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,237.12,266.08,28.95999999999998," line, square them, and add them up, and get 0.26 for the sum of the squared residuals for the greenish line Then we would do the same thing for the pink line And I would see that 0.26 for the greenish line Is smaller than 6.43 for the pink line So the greenish line is a better fit because it's some of the squared residuals is smaller"
1704,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,267.44,299.84000000000003,32.400000000000034," Note, if I multiplied both sides of the formulas by 1.5 And did the math, then we would get smaller numbers But we would still know that the greenish line fits the data better since its number is still relatively smaller In other words, it doesn't matter if the loss function is, the observed value minus the predicted value squared, or if it's 1.5 times the observed value minus the predicted value, squared"
1705,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,300.72,326.88,26.159999999999968," Since both will tell us that the greenish line has the best fit The reason why people choose this loss function for gradient boost is that when we differentiate it with respect to predicted And use the chain And bring the square down to the front And multiply it by the derivative of minus predicted, which is negative 1"
1706,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,327.84000000000003,350.56,22.71999999999997," Then the two divided by two cancels out And that leaves you with the observed minus the predicted, and multiplied by negative 1 In other words, we are left with the negative residual And that makes the math easier since gradient boost uses the derivative a lot OK, we've got a loss function"
1707,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,351.44,376.4,24.96000000000004," The y sub-is are the observed values And f of x is a function that gives us the predicted values Note, we'll talk more about f of x later We also know that the loss function is differentiable, since we have already taken the derivative Bam, we figured out what the input is for the gradient boost algorithm"
1708,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,377.44,403.04,25.600000000000023," We've got data and we've got a differentiable loss function Note, there are other loss functions to choose from, but this is the most popular one for regression Hooray, now we're ready for step one We start by initializing the model with a constant value And that constant value is determined by this funky-looking thing"
1709,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,404.0,429.28,25.28000000000003," This funky-looking thing is easiest to understand if we start on the right side and work our way left This is just the loss function The y sub-is refers to the observed values And that funky-looking symbol, called gamma, refers to the predicted values The summation means that we add up one loss function for each observed value"
1710,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,429.84,458.32,28.480000000000015," And the arg min over gamma means we need to find a predicted value that minimizes this sum In other words, if we plot the observed weights on a number line Then we want to find the point on the line that minimizes the sum of the squared residuals divided by two Here's a plot of one half of the sum of the squared residuals for potential predicted values"
1711,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,459.76,487.36,27.59999999999997," Note, we could use gradient descent to find the optimal value for predicted But we can also just solve for it because the math isn't that hard The first thing we do is take the derivative of each term with respect to predicted And since we've already shown how to take the derivative of our loss function, we can just plug it in Then we set the sum of the derivatives equal to zero"
1712,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,488.32,514.64,26.319999999999997," And solve And we end up with the average of the observed weights So given this loss function, the value for gamma that minimizes this sum Is the average of the observed weights We have now created the initial predicted value f sub zero of x"
1713,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,515.6,542.4,26.799999999999955," And it equals 73.3 That means that the initial predicted value f sub zero of x is just a leaf The leaf predicts that all samples will weigh 73.3 Bam, we finished step one We initialize the model with a constant value f sub zero of x equals 73.3"
1714,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,543.36,571.84,28.480000000000015," In other words, we created a leaf that predicts all samples will weigh 73.3 Now we can work on step two Step two is huge, but we'll take it one step at a time Step two is a loop where we make all of the trees In generic terms, we will make M trees, but in practice, most people set M to be 100 and make 100 trees"
1715,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,573.36,605.36,32.0," Little M refers to an individual tree, so in little M equals 1, then we're talking about the first tree When little M equals big M, then we're talking about the last tree And when little M is somewhere between 1 and big M, then we're talking about a tree somewhere between 1 and big M Since we are just starting step two, we will set little M equal to 1 Part A of step two looks nasty, but it's not"
1716,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,606.48,626.88,20.399999999999977, This part is just the derivative of the loss function With respect to the predicted value And we've already calculated this This big minus sign tells us to multiply the derivative by negative 1 And that leaves us with the observed value minus the predicted value
1717,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,627.84,667.12,39.27999999999997," In other words, this nasty looking thing is just a residual Now we plug F sub M minus 1 of X in for predicted And since M equals 1, that means we plug in F sub 0 of X for F sub M minus 1 of X And since F sub 0 of X is just the leaf set to 73.3, we plug in 73.3 Now we can compute R sub I comma M, where R is short for residual, I is the sample number, and M is the tree that we're trying to build"
1718,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,668.64,693.9200000000001,25.280000000000086," This tells us to calculate residuals for all three samples in the dataset So we'll start with R sub 1 comma 1, the residual for the first sample and the first tree So we plug in the observed weight for the first sample And get 14.7 We'll keep track of R sub 1 comma 1 by adding it to the dataset"
1719,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,695.5200000000001,728.16,32.63999999999987," Now we calculate R sub 2 comma 1 And we calculate R sub 3 comma 1 But a little bit more Hey, we finished part A of step 2 by calculating a residual for each sample Note, before we move on, I just want to point out that this derivative is the gradient that gradient boost is named after"
1720,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,729.6,756.1600000000001,26.56000000000006," Little bam I also want to point out that R sub I comma M values are technically called pseudo residuals When we use this loss function, we end up calculating normal residuals But if we used another loss function, this time we're not multiplying by 1 half Then we would end up with something similar to a residual, but not quite"
1721,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,757.6800000000001,784.24,26.559999999999945," In other words, we'd end up with a pseudo residual And that's why the R sub I comma M's are called pseudo residuals Okay, now let's do part B All this is saying is that we will build a regression tree to predict the residuals instead of the weights So we will use height, favorite color, and gender to predict the residuals"
1722,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,785.44,812.88,27.43999999999994," Here's the new tree Yes, I know this is just a stump and gradient boost almost always uses larger trees However, in order to demonstrate the details of the gradient boost algorithm, we need at least one leaf with more than one sample in it And when you only have three samples, then you can't have more than two leaves So we're stuck using stumps, even though they are not typically used with gradient boost"
1723,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,813.2,842.88,29.67999999999995," The residual for the third sample, x of 3, goes to the leaf on the left And the residual for samples x of 1 and x of 2 go to the leaf on the right So we have a regression tree fit to the residuals Now we need to create terminal regions r sub j comma m This part is super easy because the leaves are the terminal regions r sub j comma m"
1724,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,844.32,871.84,27.519999999999985," Note this little m is the index for the tree we just made since this is the first tree m equals 1 And this little j is the index for each leaf in the tree Since this tree has two leaves j sub m equals 2 So in this example, we'll let this leaf be r sub 1 comma 1 And this leaf be r sub 2 comma 1"
1725,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,873.0400000000001,897.2,24.159999999999968," Note it doesn't matter which leaf gets which label However, once we give a leaf a label, we need to keep track of it Hooray, we finished part b of step 2 by fitting our regression tree to the residuals and labeling the leaves Now let's do part c In this part, we determine the output values for each leaf"
1726,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,898.6400000000001,931.2,32.559999999999945," Specifically, since two residuals ended up in this leaf, it's unclear what its output value should be So for each leaf in the new tree, we compute an output value, gamma sub j comma m The output value for each leaf is the value for gamma that minimizes this summation Note this minimization is like what we did in step 1 One small difference is that now we are taking the previous prediction into account"
1727,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,932.72,962.16,29.43999999999994," While before, since we were just starting out, there was no previous prediction The other difference is that this summation is picky about which samples it includes While before, the summation included all of the samples Specifically, the x sub i and r sub i comma j means that Since only one sample, x sub 3 goes to leaf r sub 1 comma 1"
1728,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,963.04,998.24,35.200000000000045," Then only x sub 3 is used to calculate the output value for r sub 1 comma 1 And since only two samples, x sub 1 and x sub 2 go to leaf r sub 1 comma 2 Then only x sub 1 and x sub 2 are used to calculate the output value for r sub 1 comma 2 Let's start by calculating the output value for the leaf on the left r sub 1 comma 1 That means j equals 1 since this is the first leaf and m equals 1 since this is the first tree"
1729,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,999.52,1021.12,21.600000000000023," Now let's replace the generic loss function With the actual loss function that we decided to use And let's expand the summation into individual terms Since only x sub 3 goes to r sub 1 comma 1, Expanding means we remove the big sigma and swap the eyes with threes"
1730,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1022.8,1048.32,25.519999999999868," Now we plug in the value for y sub 3 the observed value And the most recent predicted value for x sub 3 Since m equals 1, the most recent prediction was f sub 0 of x Which predicted that all samples weighed 73.3 So we plug in 73.3 for f sub m minus 1 of x sub 3 and"
1731,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1049.76,1075.52,25.75999999999999," Simplify what's inside the parentheses Now we need to find the value for gamma that minimizes this equation Just like step one we can try different values for gamma or solve it analytically Since the math is easy, we'll solve for it First we take the derivative of the loss function with respect to gamma just like we did at the very start"
1732,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1076.0800000000002,1108.8,32.7199999999998, We use the change and get this Now we set the derivative equal to 0 and Solve the value for gamma that minimizes this equation is negative 17.3 and That means gamma sub 1 comma 1 equals negative 17.3 And ultimately the leaf r sub 1 comma 1 has an output value of negative 17.3
1733,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1110.48,1132.56,22.079999999999927, Now let's solve for the output value for r sub 2 comma 1 That means j equals 2 since this is the second leaf and m equals 1 since this is still the first tree Now plug in the loss function Expand the summation Plug in the observed weights
1734,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1134.48,1163.92,29.44000000000005, And plug in 73.3 for f sub m minus 1 of x sub 1 and f sub m minus 1 of x sub 2 Now simplify what's inside the parentheses And take the derivative with respect to gamma using The chain rule and that gives us this derivative Which we set equal to 0 and then we solve
1735,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1167.12,1194.3200000000002,27.200000000000276, We end up with the average of the residuals that ended up in leaf r sub 2 comma 1 And that equals 8.7 So the value for gamma that minimizes this equation is 8.7 And that means gamma sub 2 comma 1 equals 8.7 And ultimately the leaf r sub 2 comma 1 has an output value of 8.7
1736,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1195.36,1228.24,32.87999999999988," One last little observation before we move on We just saw that the output value for this leaf r sub 2 comma 1 is the average of the residuals that ended up here Given our choice of loss function, the output values are always the average of the residuals that end up in the same leaf Even if only one residual ends up in a leaf, the output value is still the average since Negative 17.3 divided by 1 equals negative 17.3"
1737,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1229.2,1262.56,33.3599999999999," Hooray, we finished part c of step 2 by computing gamma values or output values for each leaf Now let's do part d And part d we make a new prediction for each sample Since this is our first pass through step 2 and am equals 1, this new prediction will be called f sub 1 of x The new prediction f sub 1 of x is based on the last prediction we made f sub 0 of x"
1738,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1264.16,1290.96,26.800000000000185, And the tree we just finished making Note this summation is there just in case a single sample ends up in multiple leaves The summation says we should add up the output values gamma sub j comma ms for all the leaves r sub j comma m That a sample x can be found in The last thing in this equation is this Greek character new
1739,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1292.4,1320.72,28.319999999999936," New is the learning rate and is a value between 1 and 0 A small learning rate reduces the effect each tree has on the final prediction and this improves accuracy in the long run In this example, we'll set new to 0.1 Hooray, we've created f sub 1 of x Now we will use f sub 1 of x to make new predictions for each sample"
1740,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1321.68,1344.16,22.480000000000015, We'll start with the first sample x sub 1 The new prediction for x sub 1 starts with the last prediction f sub 0 of x Which is 73.3 plus 0.1 times the output value from the new tree Which is 8.7 because x sub 1's height is greater than 1.55
1741,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1345.52,1359.92,14.400000000000093, Now just do the math The new prediction for the first sample is 74.2 Which is slightly closer to the observed weight 88 Then the first prediction 73.3 Bam
1742,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1361.2,1385.6,24.40000000000009, Now let's make a new prediction for the second sample x sub 2 B D B B B B B B Boo b herd The new prediction for x sub 2 is also 74.2 Which is an improvement over the first prediction 73.3 Now let's make a new prediction for the third sample x sub 3
1743,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1389.3000000000002,1410.3200000000002,21.019999999999985, The new predictioniyoruz hit 123 hooray! We made it through 1 iteration of step 2 we started by setting m to 1 then we solved for the negative gradient
1744,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1411.6,1426.0,14.399999999999864, plugged in the observed values plugged in the latest predictions and that gave us residuals then we fit our regression tree to the residuals and computed the output values
1745,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1426.0,1446.4,20.399999999999864, gamma subj comma m for each leaf lastly we made new predictions for each sample f sub 1 of x based on the previous prediction f sub 0 of x the learning rate new and the output values gamma subj comma m
1746,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1446.8,1464.8,18.0, from the new tree now we set m equals 2 and do everything over again a calculate new residuals for the new predictions b create a new tree c calculate output values
1747,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1466.08,1484.64,18.560000000000173, and d make new predictions at the end of the second round m equals 2 and the new predictions f sub 2 of x are based on the predictions made by f sub 1 of x and the learning rate times the output values from the new is tree
1748,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1486.72,1503.76,17.039999999999964, now in the interest of time let's assume big m equals 2 so that we are done with step 2 note in practice big m equals 100 or more now we are ready for gradient boosts third and final step
1749,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1504.88,1520.16,15.279999999999973, if big m equals 2 then f sub 2 of x is the output from the gradient boost algorithm holy smokes we made it through this whole thing that's crazy now if we receive some new data
1750,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1520.88,1535.6,14.720000000000027, we could use f sub 2 of x to predict the weight the predicted weight equals 73.3 b b b b b b plus 0.1 times negative 17.3
1751,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1536.16,1551.2,15.039999999999964, b b b b b b b b plus 0.1 times negative 15.6 which equals 70 gradient boost predicts this person weighs 70 kilograms tripply bam
1752,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1553.04,1574.48,21.44000000000005, note before we go I want to remind you that gradient boost usually uses trees larger than stumps I only use stumps in this tutorial because our training data set was so darn small also be sure to watch part 3 of this exciting series on gradient boost next time we'll talk about classification
1753,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1575.68,1595.2,19.519999999999985, hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stat quest well consider getting one or two of my original songs are buying a t-shirt or a hoodie or a onesie or something anyways the links are below
1754,Gradient Boost Part 2 (of 4): Regression Details,https://www.youtube.com/watch?v=2xudPOBz-vs,2xudPOBz-vs,1595.2,1598.24,3.039999999999964, all right until next time quest on
1755,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,0.0,20.84,20.84," Last night I heard a dream about gradient boost and it was crazy. I was using it to classify things. And my memory is clear not hazy. Stat Quest. Hello, I'm Josh Stormer and welcome to Stat Quest."
1756,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,20.84,45.120000000000005,24.280000000000005," Today we're going to do part 3 in our series on gradient boost. This time we'll focus on the main ideas of how gradient boost can be used for classification. Note, this Stat Quest assumes you have already watched gradient boost part 1, regression main ideas. If not, check out the Quest. In addition, when gradient boost is used for classification, it has a lot in common with"
1757,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,45.120000000000005,71.44,26.319999999999997," logistic regression. So if you're not already familiar with logistic regression, check out the quests. In this Stat Quest, we will use this training data, where we have collected popcorn preference from 6 people, their age, their favorite color, and whether or not they love the movie troll too. And walk through step by step, the most common way that gradient boost fits a model to"
1758,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,71.44,89.47999999999999,18.039999999999992," this training data. Just like in part 1 of this series, we start with a leaf that represents an initial prediction for every individual. When we use gradient boost for classification, the initial prediction for every individual is the log of the odds."
1759,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,89.47999999999999,121.88,32.400000000000006," I like to think of the log of the odds as the logistic regression equivalent of the average. So let's calculate the overall log of the odds that someone loves troll too. Since four people in the training data set love troll too, and two people do not, then the log of the odds that someone loves troll too is the log of 4 divided by 2, which equals 0.7, which we will put into our initial leaf."
1760,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,121.88,140.48000000000002,18.600000000000023," So this is the initial prediction. How do we use it for classification? Just like with logistic regression, the easiest way to use the log of the odds for classification is to convert it to a probability. And we do that with the logistic function."
1761,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,140.51999999999998,165.12,24.600000000000023," The probability of loving troll too equals e to the log odds divided by 1 plus e to the log odds. So we plug in the log of the odds into the logistic function. Do the math, and we get 0.7 as the probability of loving troll too. And let's save that up here for now."
1762,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,165.16,194.68,29.52000000000001," Note, these two numbers, the log of 4 divided by 2, and the probability are the same only because I'm rounding. If I allowed 4 digits past the decimal place, then the log of 4 divided by 2 would equal 0.6931 and the probability would equal 0.6667. Since the probability of loving troll too is greater than 0.5, we can classify everyone in"
1763,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,194.68,223.36,28.680000000000007," the training data set as someone who loves troll too. Note, while 0.5 is a very common threshold for making classification decisions based on probability, we could have just as easily used a different value. For more details, check out the stat quest, ROC and AUC clearly explained. Now, classifying everyone in the training data set as someone who loves troll too is pretty"
1764,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,223.44000000000003,245.56,22.119999999999976," lame because two of the people do not love the movie. We can measure how bad the initial prediction is by calculating pseudo-residules, the difference between the observed and the predicted values. Although the math is easy, I think it's easier to grasp what's going on if we draw the residuals on a graph."
1765,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,245.56,270.48,24.920000000000016, The y-axis is the probability of loving troll too. The predicted probability of loving troll too is 0.7. The red dots with the probability of loving troll too equal to 0 represent the two people that do not love troll too. And the blue dots with the probability of loving troll too equal to 1 represent the four
1766,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,270.48,296.12,25.639999999999983," people that love troll too. In other words, the red and blue dots are the observed values. And the dotted line is the predicted value. So, for this sample, we plug in one for the observed value and 0.7 for the predicted value and we get 0.3."
1767,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,296.12,316.98,20.85999999999996," And we save the residual in a new column. Then we calculate the rest of the residuals. Ba-ba-ba-ba-ba-ba, ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-ba-εται. OKAY!!! We've calculated the residuals for the leaf's initial prediction."
1768,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,316.98,342.56,25.58000000000004," Now we build a tree using Liekspop chord, age and favored color to predict the residuals. Here's the tree. Note, just like when we used gradient boost for regression, we are limiting the number of leaves that we will allow in the tree. In this simple example, we are limiting the number of leaves to 3."
1769,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,342.56,370.4,27.83999999999997," In practice, people often set the maximum number of leaves to be between 8 and 32. Now let's calculate the output values for the leaves. Note, these 3 rows of data go to the same leaf. These 2 rows of data go to the same leaf. Lastly, this row of data goes to its own leaf."
1770,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,370.4,395.0,24.600000000000023," When we used gradient boost for regression, a leaf with a single residual had an output value equal to that residual. In contrast, when we use gradient boost for classification, the situation is a little more complex. This is because the predictions are in terms of the log of the odds. And this leaf is derived from a probability."
1771,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,395.0,414.92,19.91999999999996," So we can't just add them together and get a new log of the odds prediction without some sort of transformation. When we use gradient boost for classification, the most common transformation is the following formula. The numerator is the sum of all the residuals in the leaf."
1772,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,414.92,442.36,27.44000000000005," And the denominator is the sum of the previously predicted probabilities for each residual times 1 minus the same predicted probability. Note, the derivation of this formula is quite technical, so I'm saving it for part 4 of this series when we get into the nitty-gritty details of gradient boost for classification. So for now, let's just use the formula to calculate the output value for this leaf."
1773,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,442.36,463.28,20.920000000000016," Since there's only one residual in this leaf, we can ignore these summation signs for now. So we plug in the residual from the leaf. And, since we are building the first tree, the previous probability refers to the probability from the initial leaf. So we plug that in."
1774,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,463.28,492.68,29.399999999999977," We do the math, and we end up with negative 3.3 as the new output value for this leaf. Now we need to calculate the output value for this leaf. Since we have two residuals in the leaf, we'll add them together in the numerator. And in the denominator, we just add up the previous probability times 1 minus the previous probability for each residual."
1775,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,492.68,513.6800000000001,21.000000000000057," So we plug in the previous probability for each residual. Note, for now, the previous probabilities are the same for all of the residuals, but this will change when we build the next tree. Now do the math. And the output value for this leaf is negative 1."
1776,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,513.6800000000001,539.8,26.11999999999989, Now let's determine the output value for this leaf. We plug the residuals into the formula and the previous probabilities and do the math. And the output value for this leaf is 1.4. Hooray! We've calculated the output values for all three leaves in the tree.
1777,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,539.8,566.0,26.200000000000045," Now we are ready to update our predictions by combining the initial leaf with the new tree. Note, just like before, the new tree is scaled by a learning rate. This example uses a relatively large learning rate for illustrative purposes. However, 0.1 is more common. Now let's calculate the log of the odds prediction for this person."
1778,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,566.0,596.36,30.360000000000014," The log of the odds prediction is the previous prediction, 0.7, plus the output value from the tree scaled by the learning rate, 0.8 times 1.4. And the new log of the odds prediction equals 1.8. Now we can convert the new log odds prediction into a probability. And the new predicted probability equals 0.9."
1779,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,596.36,626.04,29.67999999999995," So we are taking a small step in the right direction since this person loves troll 2. We save the new predicted probability here. Now we calculate the new log of the odds prediction for the second person. The log of the odds prediction is the previous prediction, 0.7, plus the output value from the tree scaled by the learning rate, 0.8 times negative 1."
1780,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,626.04,652.16,26.120000000000005," Which gives us negative 0.1 for the new prediction. Now we can convert the log of the odds prediction into a probability. And save the new predicted probability, 0.5, here. Note, this new predicted probability is worse than before. And this is one reason why we build a lot of trees and not just one."
1781,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,652.16,681.1999999999999,29.039999999999964," Then we calculate the predicted probabilities for the remaining people. And now just like before we calculate the new residuals. And just like before, residuals are the difference between the observed and predicted probabilities. And just like before, we can plot the observed probabilities on a graph. However, now everyone has a different predicted probability."
1782,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,681.1999999999999,708.84,27.6400000000001," So to calculate the residual for the first person, we plot the predicted probability and the residual is the difference between the observed and predicted probabilities. And we save that value here. Now we calculate the residual for the second person. We plot the predicted probability and the residual is the difference."
1783,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,708.84,728.48,19.639999999999983," And we save that value here. And then we just do the same thing for all the remaining people. Bam! Now that we have the residuals, we can build a new tree. And then we need to calculate the output values for each leaf."
1784,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,728.48,752.4,23.91999999999996," Let's start with this leaf. Note, only the second person goes to this leaf. So we plug in the residual into the formula for the output values. Then we plug in the last predicted probability. Do the math, and the output value for this leaf is 2."
1785,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,752.4,777.56,25.159999999999968," Now let's calculate the output value for this leaf. Note, only the third person goes to this leaf. So we plug the residual into the formula for the output values. Then we plug in the last predicted probability. Do the math, and the output value for this leaf is negative 2."
1786,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,777.56,805.6,28.04000000000008," Lastly, let's calculate the output value for this leaf. Note, a bunch of people go to this leaf. So we plug the residuals into the formula for the output values. And we plug in the predicted probability for each individual in the leaf. Now do the math, and the output value for this leaf is 0.6."
1787,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,805.6,827.04,21.43999999999994," Bam! Now that we've calculated all of the output values for this tree, we can combine it with everything else we've done so far. We started with just a leaf, which made one prediction for every individual. Then we built a tree based on the residuals, the difference between the observed values"
1788,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,827.12,847.68,20.559999999999945," and the single value predicted by the leaf. Then we calculated the output values for each leaf, and we scaled it with a learning rate. Then we built another tree based on the new residuals, the difference between the observed values and the values predicted by the leaf and the first tree."
1789,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,847.68,867.28,19.600000000000023," Then we calculated the output values for each leaf, and we scaled this new tree with the learning rate as well. This process repeats until we have made the maximum number of trees specified or the residuals get super small. Double bam!"
1790,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,868.48,887.68,19.19999999999993," Now, for the sake of keeping the example relatively simple, imagine that we configured gradient boost to just make these two trees. And we needed to classify a new person as someone who loves troll too, or does not love troll too. The prediction starts with the leaf."
1791,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,889.04,905.04,16.0," Then we run the data down the first tree. Pippu, Pippu, Pippu, Pippu, and we add the scaled output value. Then we run the data down the second tree. Pippu, Pippu, Pippu, and then we add the scaled output value."
1792,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,906.48,933.2,26.720000000000027, Now we just do the math and get 2.3 as the log of the odds prediction that this person loves troll too. Now we need to convert this log of the odds into a probability. So we plug the log of the odds into the logistic function. Do the math and the predicted probability that this individual will love troll too
1793,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,933.2,951.52,18.320000000000164, is 0.9. Since we are using 0.5 as our threshold for deciding how to classify people and 0.9 is greater than 0.5. We will classify this person as someone who loves troll too. Triple BAM!
1794,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,953.52,977.68,24.15999999999985," Note, before we go, I want to remind you that gradient boost usually uses trees with between 8 and 32 leaves. We use small trees in this stack quest because our training data set was super small. Also, be sure to watch part 4 of this exciting series on gradient boost. Next time we'll dive deep into the math of how gradient boost is used for classification."
1795,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,979.2,1001.04,21.840000000000032," We'll derive the equation used to update the leaves and that will make you feel totally awesome. Mega BAM! Hey, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, well, consider buying one or two of my original songs"
1796,Gradient Boost Part 3 (of 4): Classification,https://www.youtube.com/watch?v=jxuNLH5dXCs,jxuNLH5dXCs,1001.04,1008.72,7.67999999999995," or buying a T-shirt or a hoodie. The links to do this are in the description below. Alright, until next time, quest on!"
1797,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,0.0,17.92,17.92," Gradient Boost for Classification seems scary. But it's not. Stat Quest. Hello, I'm Josh Stormer and welcome to Stat Quest. Today we're going to do Gradient Boost Part 4."
1798,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,17.92,37.12,19.2," We're going to talk about how Gradient Boost is used for classification. And we're going to dive deep into the details. Note, this Stat Quest assumes you've already watched the first three parts in the series. If not, check out the quests. Also, it's important that you have a pretty good understanding of the roles that the"
1799,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,37.12,58.48,21.360000000000007," log odds and the log likelihood play in logistic regression. So if you haven't already, check out these quests. In this Stat Quest, we will walk through the original Gradient Boost algorithm for classification, step by step. Just like in Part 2 of this series, we will use an incredibly small training set for the examples."
1800,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,59.44,82.4,22.960000000000008," The small size will help us focus on the algorithm's details, but it will mean using stumps instead of trees. However, by now you know that in practice, Gradient Boost usually uses trees with between eight and 32 leaves. Now we'll describe our training dataset. We have whether or not three people like popcorn,"
1801,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,83.28,99.28,16.0," their age, favorite color, and whether or not they love the movie, troll 2. Great. Now that we know all about the training dataset, let's go through the algorithm step by step. We'll start from the top."
1802,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,100.4,130.72,30.319999999999997," From Part 2 in this series, we know that this refers to the training dataset. Just to remind you, Exobie refers to a row of measurements that we will use to predict if someone loves troll 2. And why so buy refers to whether or not someone loves troll 2. Now we need a differentiable loss function that will work for classification. I think the easiest way to understand the most commonly used loss function for classification"
1803,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,130.72,153.12,22.40000000000001, is to show how it works on a graph. The Y-axis is the probability of loving troll 2. The red dot with the probability of loving troll 2 equal to zero represents the one person that does not love troll 2. And the blue dots with the probability of loving troll 2 equal to one
1804,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,153.76000000000002,183.44,29.67999999999998," represent the two people that love troll 2. In other words, the red and blue dots are the observed values. And we can draw dotted line to represent the predicted probability that someone loves troll 2. In this example, I've set the predicted probability to zero point six seven. Now, just like we do for logistic regression, we can calculate the log likelihood of the data given the"
1805,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,183.44,213.76,30.319999999999997," predicted probability. The log likelihood of the observed data given the prediction is this nasty looking summation. The P's refer to the predicted probability, which is 0.67 in this example. And the Y-sub-I's refer to the observed values for love's troll 2. For the two people who love troll 2, Y-sub-I equals one, which means that this term will be zero."
1806,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,214.72,236.32,21.599999999999994," Leaving just the log of P. In contrast, for the one person who does not love troll 2, Y-sub-I equals zero, which means that this term will be zero. Leaving just the log of one minus P. Now, let's use the summation to calculate the log likelihood of all three observed values."
1807,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,237.76,268.88,31.120000000000005," We'll start by calculating the log likelihood for the first person. Because this person loves troll 2, Y-sub-I equals one. Then we plug in 0.67 for the predicted probability P. 1 minus 1 equals zero, and now we do the multiplication. The log likelihood for the first person, given the predicted probability, is the log of 0.67."
1808,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,269.2,302.88,33.68000000000001," Now let's calculate the log likelihood for the second person. We plug in the observed value, 1 for Y-sub-2, and plug in 0.67 for the predicted probability P. 1 minus 1 equals zero, and now we do the multiplication. And we get the log of 0.67 since the predicted probability was the same. Now let's calculate the log likelihood for the third person."
1809,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,304.16,327.04,22.88000000000005," We plug in the observed value, zero for Y-sub-3, since this person does not love the movie. Plug in 0.67 for the predicted probability P. 1 minus 0 equals 1, and now we do the multiplication. And we get the log of 1 minus 0.67."
1810,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,328.72,351.84000000000003,23.12000000000006," Note, the better the prediction, the larger the log likelihood, and this is why, when doing logistic regression, the goal is to maximize the log likelihood. That means that if we want to use the log likelihood as a loss function, where smaller values represent better fitting models, then we need to multiply the log likelihood by negative 1."
1811,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,353.52000000000004,376.72,23.19999999999999," So we'll put this subtle, but very important minus sign in front of everything. And since a loss function sometimes only deals with 1 sample at a time, we can get rid of the summation. And to make it easier to read, we'll replace Y with observed. Now we need to transform this equation, the negative log likelihood,"
1812,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,376.72,406.48,29.75999999999999," so that it is a function of the predicted log odds instead of the predicted probability P. And we need to simplify it. Since this will require a lot of steps, we'll move the negative log likelihood to the top left hand corner to give us space to work. The first thing we do is distribute the minus sign through the equation. Now we multiply the negative 1 minus observed by the log of 1 minus P."
1813,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,407.92,436.56,28.639999999999983," Now we combine this with this and get this. And since the log of P minus the log of 1 minus P equals the log of P divided by the log of 1 minus P and that equals the log of P divided by 1 minus P, and that equals the log of the odds. So we replace the log of P minus the log of 1 minus P with the log of the odds."
1814,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,438.24,464.64,26.400000000000038," In other words, we converted the log of P minus the log of 1 minus P into a function of the log of the odds. Note, the relationship between probability P and the log of the odds is derived in the log of the odds. So check that out if you want more details. Now we need to convert the log of 1 minus P into a function of the log of the odds."
1815,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,466.16,498.64,32.48000000000002," The log of 1 minus P equals the log of 1 minus E to the log odds divided by 1 plus E to the log odds. Note, this relationship between probability P and the log of the odds is derived in the logistic regression stack quest on estimating parameters with maximum likelihood. Now we replace 1 with this fraction. Do the subtraction and we get the log of 1 divided by 1 plus E to the log odds."
1816,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,499.92,524.16,24.239999999999952," Now convert the division into subtraction. And since the log of 1 equals 0, we can remove it. And that leaves us with the negative log of 1 plus E to the log odds. Thus, the log of 1 minus P, which is a function of the predicted probability, P, can be transformed into a function of the predicted log odds."
1817,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,525.68,551.12,25.44000000000005," So let's plug that in. Note, this sign changed from negative to positive because we replaced the log of 1 minus P with the negative log of 1 plus E to the log odds. Whoay! We converted the negative log likelihood of the data, which is a function of the predicted probability, P, into a function of the predicted log odds."
1818,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,552.1600000000001,577.36,25.19999999999993, Bam! So this will be the loss function. Now we just need to show that it is differentiable. So let's take the derivative of the loss function with respect to the predicted log odds. The derivative of the first part with respect to the predicted log odds is super easy. It's just the negative observed value.
1819,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,578.64,608.0,29.360000000000014," The derivative of the second part is also super easy if you know how to use the chain. Ooh! The derivative of the log of something is one over that something times the derivative of that something. And this multiplication can be rewritten as a single fraction. Note, earlier we saw that we can substitute the predicted probability, P, with this fraction."
1820,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,609.1199999999999,635.1199999999999,26.0," But we can also swap the predicted probability, P, back in. And that means that the derivative of the loss function can be a function of the predicted log odds, or a function of the predicted probability, P. As we will soon see, sometimes it's easier to use the function of the log odds, and sometimes it's easier to use the function of the predicted probability, P."
1821,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,636.4799999999999,661.84,25.360000000000127," In summary, the input data is the training data set, and this fairly nasty looking thing, which is just a transformation of the negative log likelihood, is the differentiable loss function. And the derivative can be a function of the predicted log odds, or a function of the predicted probability, P. Hooray! Now we're ready for step one."
1822,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,663.2,689.6,26.399999999999977," Just like when we used gradient boost for regression, we need to come up with the initial prediction. And just like before, we'll use this funky looking equation to find the optimal initial prediction. Remember, this is just the loss function. The YI refers to the observed values, and that funky symbol, called gamma, refers to a log odds value."
1823,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,690.8,723.0400000000001,32.24000000000012," In theory, we could go ahead and replace the log odds with gamma, but it's actually easier to see what's going on if we leave the log odds end and remember that it represents gamma. The summation means that we add up one loss function for each observed value. And the arg min over gamma means we need to find a log odds value that minimizes this sum. The first thing we do is take the derivative of each term with respect to the log odds."
1824,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,723.36,760.4,37.03999999999997," Now, to make the next step super easy, let's replace the log odds with the predicted probability, P. And set the sum of the derivatives equal to zero and solve. And we end up with two divided by three for the initial predicted probability, P, because two people love troll two, and there are three people in the training dataset. Now we can convert the predicted probability into the predicted log odds."
1825,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,761.9200000000001,795.0400000000001,33.120000000000005," So we plug in the predicted probability and solve. And the predicted log odds is the log of two divided by one, which makes sense because two people love troll two and one person does not. So given this loss function, the log odds value for gamma that minimizes this sum is the predicted log odds of loving troll two based on the observed yes, no values."
1826,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,796.32,822.4,26.079999999999927," Ray, we've created the initial predicted log odds f sub zero of x. It equals 0.69. So this is the initial leaf f sub zero of x. Bam, we finished step one. We initialize the model with a constant value f sub zero of x equals the log of two divided by one,"
1827,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,822.4,848.16,25.75999999999999," which equals 0.69. In other words, we created a leaf that predicts the log odds that someone will love troll two and it equals 0.69. Now we can work on step two. Just like when we used gradient boost for regression, this is when we'll build the trees. So we'll start by setting little m equal to 1 and go from here."
1828,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,849.6,875.76,26.159999999999968," In part a, we calculate pseudo residuals with this nasty looking thing. This is just the derivative of the loss function, with respect to the predicted log odds. And we've already calculated this. This big minus sign tells us to multiply the derivative by negative one. And that leaves us with this equation for calculating pseudo residuals."
1829,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,876.96,907.36,30.399999999999977," Note, as we have seen before, we can replace this term with the predicted probability p. So we can think of the pseudo residuals as the observed probability minus the predicted probability. And the observed minus the predicted results in a pseudo residual. This part says to plug in the most recent predicted log odds. So we plug in f sub zero of x, the most recent predicted log odds."
1830,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,908.64,939.36,30.719999999999917," Then do the math to convert the predicted log odds into the predicted probability p. Now we can compute the pseudo residuals for each sample r sub i comma m, where i is the sample number, and m is the tree that we're building. So we'll start with r sub 1 comma 1, the residual for the first sample and the first tree. So we plug in the observed weight for the first sample. And we get 0.33."
1831,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,940.32,966.16,25.83999999999992," We'll keep track of r sub 1 comma 1 by adding it to the dataset. Now we'll calculate the other two residuals. We've finished part a of step two by calculating residual for each sample. Now we're ready for part b, where we will build a regression tree. We will build a regression tree using"
1832,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,966.16,989.12,22.96000000000004," likes popcorn, age, and favored color to predict the residuals. Here's the new tree. So we have a regression tree fit to the residuals. Now we need to create terminal regions r sub j comma m. So in this example, we'll name this leaf r sub 1 comma 1"
1833,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,990.0,1017.04,27.039999999999964," and name this leaf r sub 2 comma 1. Hooray! We've finished part b of step two by fitting a regression tree to the residuals and labelling the leaves. Now let's do part c. This is when we calculate the output values for the new tree. So for each leaf in the new tree, we compute an output value, gamma sub j comma m."
1834,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1017.68,1050.32,32.639999999999986," The output value for each leaf is the value for gamma that minimizes this summation. The x sub i in r sub i comma j means that, since only the first row of data, x sub 1 goes to leaf r sub 1 comma 1, then only x sub 1 is used to calculate the output value for r sub 1 comma 1. And since only two samples, x sub 2 and x sub 3 go to leaf r sub 2 comma 1,"
1835,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1051.12,1082.72,31.600000000000136," then only x sub 2 and x sub 3 are used to calculate the output value for r sub 2 comma 1. Let's start by calculating the output value for the leaf on the left, r sub 1 comma 1. That means that j equals 1 since this is the first leaf and am equals 1 since this is the first tree. Remember, this is just the loss function. So we can replace the generic form with the actual loss function that we are using."
1836,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1084.0,1114.16,30.15999999999985," Note, to keep the length of the formula from getting out of hand, I'm returning to using y sub i to refer to the observed values. Since only x sub 1 goes to r sub 1 comma 1, we can remove the big sigma and swap the i's with 1's. Now let's solve for the optimal value for gamma. In theory, we could take the derivative of this function with respect to gamma and then solve for gamma,"
1837,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1115.12,1142.2399999999998,27.11999999999989," but that would turn into a huge mess. So we will take a different approach than when we used gradient boost for regression. Let's start by moving the equation up here to give us room to work. And even though we want to minimize gamma, let's remember that we are working with the loss function. Since taking the derivative of the loss function with respect to gamma and then solving for gamma is hard,"
1838,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1143.2,1171.2,28.0," we can approximate the loss function with a second order Taylor polynomial. Why this second order Taylor polynomial is a good approximation, is something we can talk about in a future stat quest. For now, just take my word for it. Now we can take the derivative of this function with respect to gamma. Since gamma is not part of this term, the derivative with respect to gamma is zero,"
1839,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1171.28,1202.96,31.680000000000064," so we can omit it. The derivative of this with respect to gamma is super easy. We can treat this the derivative of the loss function with respect to the predicted log odds as a constant. And the derivative of a constant times gamma is the constant. Similarly, the derivative of this with respect to gamma is super easy. All this stuff, one half times the second derivative of the loss function with respect to the predicted log odds,"
1840,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1203.44,1227.52,24.079999999999927," can be treated like a constant. And we get this, because the square on the gamma comes down and cancels the one half. Using the approximation of the loss function made taking the derivative with respect to gamma super easy. Now let's move it up to give ourselves a little more room. Set it equal to zero and solve for gamma."
1841,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1228.72,1253.76,25.039999999999964," First, we'll subtract this term from both sides. Then we'll divide both sides by this term. Bam, we solve for gamma. And now, just because I'm into moving equations to the top of the screen, let's move this one up. Gamma equals negative one times the derivative of the loss function,"
1842,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1254.56,1281.2,26.6400000000001," divided by the second derivative of the loss function. Since we have already solved for this, we can just plug it in and we can replace this with the predicted probability P. And the observed value minus the predicted probability P is just the residual. Now we need to take the second derivative of the loss function to figure out what goes in the denominator."
1843,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1282.48,1307.4399999999998,24.95999999999981," The second derivative of the loss function equals the derivative of the first derivative of the loss function. So we can plug in the first derivative of the loss function. Now, to make taking the derivative a little more obvious, let's rewrite this fraction as multiplication. Now we need to take the derivative of this with respect to the log odds."
1844,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1308.64,1339.28,30.6400000000001," The derivative of negative observed is zero, so we can ignore this part. This part, however, is a little more involved. We'll need to use the product rule. The product rule says that the derivative of a times b equals the derivative of a times b plus a times the derivative of b. So we start with the derivative of the first part by using the chain."
1845,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1341.28,1358.48,17.200000000000045," And that gives us this derivative. Then we multiply by the second part. Then we add the first part times the derivative of the second part. Bam! Now, this long thing is the second derivative of the loss function."
1846,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1359.76,1380.56,20.799999999999955," Technically, we could leave this as it is and move on, but with a little bit of algebra, we can get something way, way easier to work with. So let's move this to the top of the screen so that we have room to work. The first thing we do is rewrite this as a fraction. Then we rewrite this as a fraction."
1847,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1381.44,1405.04,23.59999999999991," Then we multiply the top and bottom of the second term by 1 plus e to the log odds. And now we can add these terms together. These two parts in the numerator cancel each other out. And that leaves us with this. Note, I also split the denominator into two terms so that the next steps make more sense."
1848,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1405.6,1442.96,37.36000000000013," Now we'll multiply the numerator by 1, which seems silly, but is the key to reducing the whole thing into two super simple terms. By multiplying the numerator by 1, we can easily see how the single term separates into two terms multiplied together. At this point, you may recognize the first term. It converts the predicted log odds, too, the predicted probability, P. The second term should also remind you of something we have seen earlier in this stat quest."
1849,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1444.4,1475.2,30.799999999999955," Earlier, we saw that the log of 1 minus P equals this. And that means that 1 minus P equals this, which means the second term is just 1 minus the predicted probability P. So at long last, we see that the second derivative of the loss function is equal to P times 1 minus P. And that brings us back to gamma."
1850,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1476.16,1508.3200000000002,32.16000000000031," Earlier, we saw that gamma equals the residual divided by the second derivative of the loss function. And now we can replace the second derivative of the loss function with P times 1 minus P. Bam! I know it seems like a long time ago, but remember, we were trying to find out the output value for this leaf. In other words, we were trying to find the value for gamma that when added to the most recent predicted log odds,"
1851,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1508.4,1537.2,28.800000000000185," minimized the loss function. Then we did a ton of math and discovered that gamma sub 1 comma 1 is the residual divided by P times 1 minus P. Now we can plug in the residual and the most recent predicted probability P for this sample. In this case, the predicted probability for this sample is derived from f sub 0 of x."
1852,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1537.36,1558.24,20.87999999999988," The most recent log odds prediction. Now we just do the math. And the output value for leaf r sub 1 comma 1 is 1.5. Now let's calculate the output value for the other leaf, r sub 2 comma 1. That means we're calculating gamma sub 2 comma 1."
1853,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1559.52,1594.96,35.440000000000055," Since samples x2 and x3 go to leaf r sub 2 comma 1, then we will need a loss function for x sub 2 and a loss function for x sub 3. Now, just like before, we can approximate the loss function with second order Taylor polynomials. Here's the second order Taylor polynomial approximation of the loss function for sample x sub 2. And here's the second order Taylor polynomial approximation of the loss function for x sub 3."
1854,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1596.4,1618.4,22.0," If we add these two loss functions together, we get this. And we can approximate that sum by adding together the two Taylor polynomials. We'll start by adding the first terms in the Taylor polynomials. Then add the second terms. And then add the third terms."
1855,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1620.24,1654.88,34.63999999999987," Note, we can pull these two gammas out of the second terms. And we can pull the one halves and the gamma squares out of the third terms. Now let's move everything to the top so we have some space to determine the optimal value for gamma. The first step in finding the optimal value for gamma is to take the derivative of the sum of the two approximate loss functions with respect to gamma. The derivative of this part is zero since gamma is not involved at all."
1856,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1656.48,1682.24,25.75999999999999," For the second term, since everything between the square brackets is like a constant with respect to gamma, the derivative is everything between the square brackets. And for the third term, the derivative of this with respect to gamma means the square comes down and cancels out the one half, and that leaves us with everything between the square brackets times gamma."
1857,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1683.52,1707.44,23.920000000000076," So this is the derivative of the sum of the approximate loss functions with respect to gamma. Move it to the top so we have some room to work. Set it equal to zero, now solve for gamma. First, we subtract this square bracket stuff from both sides. Now we divide both sides by this square bracket stuff."
1858,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1708.72,1725.6799999999998,16.95999999999981," And this is the solution for gamma. Now all we need to do is simplify it. So we move it to the top to give us a little room to work. In the numerator, we have two separate derivatives of the loss function, one for x2 and one for x3."
1859,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1726.8,1754.88,28.080000000000155," And since we already know that the derivative of the loss function is this, we can plug it in for x2 and for x3. And since e to the log odds divided by one plus e to the log odds equals p, we can plug in p sub 2, the predicted probability for x sub 2, and we can plug in p sub 3, the predicted probability for x sub 3."
1860,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1756.4,1781.6,25.200000000000045," Now we multiply the negative one through the numerator, and that leaves us with the observed minus the predicted probabilities for x sub 2 and x sub 3. In other words, we are left with the sum of the residuals in the numerator, similarly, we have the sum of two second derivatives in the denominator, one for x sub 2 and one for x sub 3."
1861,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1783.04,1807.04,24.0," And since we already know that the second derivative of the loss function equals p times one minus p, we can plug in p sub 2 for the predicted probability for x sub 2, and we can plug in p sub 3, the predicted probability for x sub 3. Now we just tidy everything up. Double bam."
1862,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1808.8,1842.0,33.200000000000045," At long last, we see that gamma is equal to the sum of the residuals, divided by the sum of p times one minus p for each sample in the leaf. Okay, I know it seems like a long time ago, but we were trying to find the output value for this leaf. So we did a ton of math and discovered that gamma sub 2 comma 1 is, the sum of the residuals divided by the sum of p times one minus p for each sample that ended up in the leaf."
1863,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1843.36,1864.96,21.600000000000136," Now we can plug in the residuals. Now we need to plug in the most recent predicted probabilities, p 2 and p 3 for x 2 and x 3. Just like before, since we are building the first tree, the predicted probability for these samples is derived from f sub 0 of x, the most recent log odds prediction."
1864,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1866.56,1890.64,24.080000000000155," Note, since we are just starting out, the predicted probabilities are the same for all of the samples. However, after we build the first tree, they can be different. Now just do the math. And the output value for leaf r sub 2 comma 1 is negative 0.77. Hooray, we made it through step 2 part c."
1865,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1891.84,1910.64,18.799999999999955," We calculated output values for each leaf in the tree. Now let's do part d. In part d, we make a new prediction for each sample. Since this is our first pass through step 2 and m equals 1, this new prediction will be called f sub 1 of x."
1866,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1911.92,1944.32,32.40000000000009," The new prediction, f sub 1 of x, is based on the last prediction we made, f sub 0 of x. Plus the learning rate, new, times, the output values from the first tree we made. Note, this summation is their just in case a single sample ends up in multiple leaves. Also note, we've set the learning rate, new to 0.8, which is relatively large. For more details about this, check out the stat quest on the main ideas."
1867,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1945.84,1976.8,30.95999999999981," Hooray, we've created f sub 1 of x. Now we will use f sub 1 of x to make new predictions for each sample. We'll start with the first sample, x sub 1. The new prediction for x sub 1 starts with the last prediction, f sub 0 of x, which is 0.69. Plus 0.8 times the output value from the new tree, which is 1.5 because x sub 1 likes popcorn."
1868,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,1977.52,2000.0,22.480000000000015," By the way, I love popcorn too. Now just do the math. The new log odds prediction for the first sample is 1.89, which is a better prediction than before, because the odds are more in favor that this person will love troll too. Now we'll calculate the new predicted log odds for the second sample, x sub 2."
1869,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2001.28,2028.72,27.44000000000005," The new log odds prediction for the second sample is 0.07, which is worse than before, but that's also why we build more than one tree. Now we'll calculate the new predicted log odds for the third sample, x sub 3. The new log odds prediction for the third sample is 0.07, which is better than before. Hooray, we made it through one iteration of step 2."
1870,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2030.0,2052.56,22.559999999999945," We started by setting little m equal to 1. Then we calculated pseudo-residules, by plugging in the observed values, and the latest predictions, and that gave us residuals. Then we fit a regression tree to the residuals, and computed the output values, gamma sub j comma m, for each leaf."
1871,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2054.16,2084.64,30.480000000000015," Lastly, we made new predictions for each sample, f sub 1 of x, based on the previous prediction, f sub 0 of x, the learning rate, new, and the output values, gamma sub j comma m from the new tree. Now we set little m equal to 2, and do everything over again. A, calculate new residuals for the new predictions, b, create a new regression tree,"
1872,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2085.92,2115.28,29.360000000000127," c, calculate output values, and d make new predictions. At the end of the second round, little m equals 2, and the new predictions, f sub 2 of x, are based on, the predictions made by f sub 1 of x, and the learning rate times the output values from the newest tree. Now, in the interest of time, let's assume big m equals 2, so that we are done with step 2."
1873,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2115.6000000000004,2154.32,38.7199999999998," Note, in practice, m equals 100 or more. Now we are ready for gradient boosts third and final step. If big m equals 2, then f sub 2 of x is the output from the gradient boost algorithm. Now, if we receive some new data, we would use f sub 2 of x to predict whether this person loves trow2. The predicted log odds that this person will love trow2 equals 3.4."
1874,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2155.52,2185.28,29.76000000000022," The predicted probability that this person will love trow2 equals 0.97. If we use a threshold of 0.5 for deciding if someone loves trow2, then since 0.97 is greater than 0.5, this person loves trow2. Triple B. Holy freaking smokes, we made it through this whole algorithm. I can't believe it."
1875,Gradient Boost Part 4 (of 4): Classification Details,https://www.youtube.com/watch?v=StWY5QWMXCw,StWY5QWMXCw,2185.28,2211.2,25.919999999999614," We've done it for regression, and now we've done it for classification. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, well, consider buying one or two of my original songs or getting a T-shirt or a hoodie. The links to do this are in the description below. All right, until next time, quest on."
1876,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,0.0,24.84,24.84," True to I want to watch you, but I am so afraid that quest. Hello, I'm Josh Starmer and welcome to StacQuest. Today we're going to talk about Troll 2 and it's going to be clearly explained. This StacQuest is sponsored by Lightning and Grid.ai. Check them out."
1877,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,24.84,43.04,18.2," They are awesome. For more details, follow the links in the pinned comment below. Note, this StacQuest assumes that you are already familiar with BAM. If not, check out the quest. The link is in the description below."
1878,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,43.04,72.08,29.04," If you've watched enough StacQuest, chances are you've seen references to the movie Troll 2. For example, in the StacQuest on expected values, Statsquatch makes a bet that the next person we meet has heard of the movie Troll 2. And references to the movie Troll 2 appear in a bunch of other StacQuest as well. So now it's time for Troll 2 to be clearly explained."
1879,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,72.08,94.92,22.840000000000003," First of all, there is no consensus on when Troll 2 came out. The Wikipedia article says 1990. His streaming platform, Tubby, says it came out in 1991. And Rotten Tomatoes says it came out in 1992. And while we're on the Rotten Tomatoes page, it's also worth noting that the critics'"
1880,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,94.92,113.6,18.679999999999996," consensus is just three words. Oh my god. BAM. Another confusing thing about Troll 2 is its name. Depending on the poster, sometimes it's Troll 2, and sometimes it's Troll squared."
1881,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,113.6,132.04,18.44," However, one thing is certain about Troll 2 slash Troll squared. It is not a sequel. I mean, sure, there's a movie called Troll. That pretty much everyone agrees came out in 1986. But Troll, the movie, is about a Troll."
1882,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,132.07999999999998,153.0,20.920000000000016," Troll 2 slash Troll squared slash whatever you want to call it had nothing to do with the earlier movie and is not about Trolls. That's right. At no point during the 94 minutes spent watching Troll 2, will you hear or see a single reference to a Troll or multiple Trolls?"
1883,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,153.0,173.12,20.120000000000005," Oh no, it's that you're headed terminology alert. Although Troll 2 is called Troll 2, the monsters in the movie are all goblins. In fact, the original title for the movie was goblins. However, they changed the name when they thought more people would go see a movie called Troll 2."
1884,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,173.12,189.76,16.639999999999986," Double BAM. Troll 2 is all about watching people eat, including this love struck man enjoying a green pudding. A child protecting his family by eating a massive baloney sandwich and someone buried in popcorn."
1885,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,189.76,213.08,23.32000000000002," We also see a whole lot of this guy who goes by Grandpa's death. Sometimes you see Grandpa's face floating in air and sometimes you see an extreme close-up of Grandpa's death. Anyway, I don't want to give away too much of the movie, but it involves a happy, yet chaoticly arranged family visiting the countryside."
1886,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,213.08,234.84,21.75999999999999," And they are greeted by a family that stands in order from shortest to tallest. To appreciate the differences between these two families, it's good to see them both at the same time. On the left, we have the happy family that has no idea how to pose for a picture. And on the right, we have a family that can quickly arrange themselves in a photogenic way."
1887,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,234.84,254.04,19.19999999999999," Small BAM. Anyway, this kid knows there's something funny going on. And ends up having to fight this creepy-looking woman with lots of jewelry. This lady with interesting glasses, and this old dude with a wide-bremmed hat. Does the kid win all three fights?"
1888,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,254.04,272.36,18.319999999999965," Well, you'll just have to watch it to find out. But before you do that, let me warn you. Troll 2 is consistently voted the worst movie ever made. And that's how it caught my attention when I decided that stat quest needed a new example dataset."
1889,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,272.36,288.16,15.800000000000011," But is Troll 2 really that bad? I'll let you decide. Triple BAM. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
1890,"Troll 2, Clearly Explained!!!",https://www.youtube.com/watch?v=W4sYsXGeCls,W4sYsXGeCls,288.16,305.24,17.08000000000004," And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time, quest on."
1891,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,0.0,22.88,22.88," XG Boost, it's extreme and it's great in Boost. StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about XG Boost Part 1. We're going to talk about XG Boost trees and how they're used for regression."
1892,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,22.88,44.400000000000006,21.520000000000007," Note, this StatQuest assumes that you are already from Meyer with at least the main ideas of how gradient Boost does regression. And you should be familiar with at least the main ideas behind regularization. If not, check out the quests. The links are in the description below."
1893,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,44.400000000000006,69.0,24.599999999999994," XG Boost is extreme and that means it's a big machine learning algorithm with lots of parts. The good news is that each part is pretty simple and easy to understand and we'll go through them one step at a time. Actually, I'm assuming that you are already familiar with gradient Boost and regularization."
1894,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,69.0,99.4,30.40000000000001," So we'll start by learning about XG Boost's unique regression trees. Because this is a big topic, we'll spend three whole stat quests on it. In this stat quest, Part 1, we'll build our intuition about how XG Boost does regression with its unique trees. In Part 2, we'll build our intuition about how XG Boost does classification."
1895,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,99.4,126.84,27.44," And in Part 3, we'll dive into the mathematical details and show you how regression and classification are related and why creating unique trees makes so much sense. Note, XG Boost was designed to be used with large, complicated data sets. However, to keep the examples from getting out of hand, we'll use this super simple training data."
1896,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,126.84,151.44,24.599999999999994," On the X-axis, we have different drug dosages. And on the Y-axis, we've measured drug effectiveness. These two observations have relatively large positive values for drug effectiveness, and that means that the drug was helpful. These two observations have relatively large negative values for drug effectiveness, and that"
1897,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,151.44,182.92,31.480000000000015," means that the drug did more harm than good. The very first step in fitting XG Boost to the training data is to make an initial prediction. This prediction can be anything, but by default, it is 0.5, regardless of whether you're using XG Boost for regression or classification. The prediction, 0.5 corresponds to this thick, black horizontal line."
1898,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,182.92,215.96,33.03999999999999," And the residuals, the differences between the observed and predicted values, show is how good the initial prediction is. Now, just like on-extreme gradient boost, XG Boost fits a regression tree to the residuals. However, unlike on-extreme gradient boost, which typically uses regular off-the-shelf regression trees, XG Boost uses a unique regression tree that I call an XG Boost tree."
1899,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,215.96,240.76,24.799999999999983," So let's talk about how to build an XG Boost tree for regression. Note, there are many ways to build XG Boost trees. This video focuses on the most common way to build them for regression. Each tree starts out as a single leaf. And all of the residuals go to the leaf."
1900,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,240.76,269.96000000000004,29.200000000000045," Now we calculate a quality score, or similarity score, for the residuals. The similarity score equals the sum of the residuals squared over the number of residuals plus lambda. Note, lambda is a regularization parameter, and we'll talk more about that later. For now, let lambda equals 0."
1901,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,269.96000000000004,302.08000000000004,32.120000000000005," Now we plug the four residuals into the numerator, and since there are four residuals in the leaf, we put a four in the denominator. Note, because we do not square the residuals before we add them together in the numerator, 7.5, and negative 7.5 cancel each other out. In other words, when we add this residual to this residual, they cancel each other out."
1902,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,302.12,334.92,32.799999999999955," Likewise, 6.5 cancels out most of negative 10.5, leaving negative 4 squared in the numerator. Thus, the similarity score for the residuals in the root equals 4. So let's put similarity equals 4 up here so we can keep track of it. Now the question is whether or not we can do a better job clustering similar residuals if we split them into two groups."
1903,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,334.92,367.76,32.84000000000003," To answer this, we first focus on the two observations with the lowest dosages. Their average dosage is 15, and that corresponds to this dotted red line. So we split the observations into two groups based on whether or not the dosage is less than 15. The observation on the far left is the only one with dosage less than 15. So its residual goes to the leaf on the left."
1904,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,367.76,397.04,29.28000000000003," All of the other residuals go to the leaf on the right. Now we calculate the similarity score for the leaf on the left by plugging the one residual into the numerator. And since only one residual went to the leaf on the left, the number of residuals equals 1. And just like before, we set lambda equal to 0 and the similarity score for the leaf on the left"
1905,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,397.04,428.48,31.44," equals 110.25. So let's put similarity equals 110.25 under the leaf so we can keep track of it. We can calculate the similarity score for the residuals that go to the leaf on the right. We plug in the sum of residuals squared into the numerator. And since there are three residuals in the leaf on the right, we plug three into the denominator"
1906,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,428.48,461.76,33.27999999999997," and just like before, let's let lambda equal 0. Note, like we saw earlier, because we do not square the residuals before we add the them together, 7.5 and negative 7.5 cancel each other out. Leaving only one residual, 6.5 in the numerator. Thus the similarity score for the residuals in the leaf on the right equals 14.08."
1907,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,461.8,490.4,28.59999999999997," So let's put similarity equals 14.08 under the leaf so we can keep track of it. Now that we have calculated similarity scores for each node, we see that when the residuals in a node are very different, they cancel each other out and the similarity score is relatively small. In contrast, when the residuals are similar or there is just one of them, they do not cancel"
1908,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,490.4,516.64,26.24000000000001," out and the similarity score is relatively large. Now we need to quantify how much better the leaves cluster similar residuals than the root. We do this by calculating the gain of splitting the residuals into two groups. Gain is equal to the similarity score for the leaf on the left, plus the similarity score"
1909,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,516.64,553.32,36.680000000000064," for the leaf on the right minus the similarity score for the root. Plugging in the numbers gives us 120.33 small bam. Now that we have calculated the gain for the threshold of dosage less than 15, we can compare it to the gain calculated for other thresholds. So we shift the threshold over so that it is the average of the next two observations"
1910,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,553.32,589.08,35.75999999999999," and build a simple tree that divides the observations using the new threshold dosage less than 22.5. Now we calculate the similarity scores for the leaves and calculate the gain. The gain for dosage less than 22.5 is 4. Since the gain for dosage less than 22.5 is less than the gain for dosage less than 15, dosage"
1911,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,589.08,622.92,33.83999999999992, less than 15 is better at splitting the residuals into clusters of similar values. Now we shift the threshold over so that it is the average of the last two observations and build a simple tree that divides the observations using the new threshold dosage less than 30. Then we calculate the similarity scores for the leaves and the gain.
1912,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,622.92,649.84,26.920000000000076," The gain for dosage less than 30 equals 56.33. Again, since the gain for dosage less than 30 is less than the gain for dosage less than 15, dosage less than 15 is better at splitting the observations. And since we can't shift the threshold over any further to the right, we are done comparing different thresholds."
1913,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,649.84,674.52,24.67999999999995," And we will use the threshold that gave us the largest gain, dosage less than 15, for the first branch in the tree. Now, since there is only one residual in the leaf on the left, we can't split it any further. However, we can split the three residuals in the leaf on the right."
1914,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,674.52,703.2,28.680000000000064," So we start with these two observations and their average dosage is 22.5, which corresponds to this dotted green line. So the first threshold that we try is dosage less than 22.5. Now, just like before, we calculate the similarity scores for the leaves. Note, we calculated the similarity score for this node when we figured out how to split"
1915,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,703.2,737.52,34.319999999999936, the root. So now we calculate the gain. And we get gain equals 28.17 for when the threshold is dosage less than 22.5. Now we shift the threshold over so that it is the average of the last two observations. Calculate the similarity scores for the leaves and the gain.
1916,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,737.52,764.0,26.480000000000015," And we get gain equals 140.17, which is much larger than 28.17 when the threshold was dosage less than 22.5. So we will use dosage less than 30 as the threshold for this branch. Note, to keep this example from getting out of hand, I've limited the tree depth to two levels."
1917,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,764.0,787.92,23.91999999999996," And this means we will not split this leaf any further and we are done building this tree. However, the default is to allow up to six levels. Small bam. Now we need to talk about how to prune this tree. We prune an XG boost tree based on its gain values."
1918,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,787.92,807.26,19.340000000000032," We start by picking a number. For example, 130. Oh no, it's the dreaded terminology alert. XG boost calls this number gamma. We then calculate the difference between the gain associated with the lowest branch in the"
1919,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,807.26,840.6400000000001,33.38000000000011," tree and the value for gamma. If the difference between the gain and gamma is negative, we will remove the branch. And if the difference between the gain and gamma is positive, we will not remove the branch. In this case, when we plug in the gain and the value for gamma, 130, we get a positive number, so we will not remove this branch and we are done pruning."
1920,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,840.68,873.48,32.80000000000007," So, the gain for the root, 120.3 is less than 130, the value for gamma, so the difference will be negative. However, because we did not remove the first branch, we will not remove the root. In contrast, if we set gamma equal to 150, then we would remove this branch because 140.17 minus 150 equals a negative number."
1921,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,873.48,905.68,32.19999999999993," So, let's remove this branch. Now we subtract gamma from the gain for the root. Since 120.33 minus 150 equals a negative number, we will remove the root. And all we would be left with is the original prediction, which is pretty extreme pruning. So, while this wasn't the most nuanced example of how an XG boost tree is pruned,"
1922,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,905.68,935.08,29.399999999999977," I hope you get the idea. Now let's go back to the original residuals and build a tree just like before. Only this time, when we calculate similarity scores, we will set lambda equal to 1. Remember, lambda is a regularization parameter, which means that it is intended to reduce the prediction sensitivity to individual observations."
1923,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,935.08,965.4,30.32000000000005," Now the similarity score for the root is 3.2, which is 8 tenths of what we got when lambda equal to 0. When we calculate the similarity score for the leaf on the left, we get 55.12, which is half of what we got when lambda equal to 0. And when we calculate the similarity score for the leaf on the right, we get 10.56, which"
1924,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,965.4,995.48,30.08000000000004," is 3 quarters of what we got when lambda equal to 0. So, one thing we see is that when lambda is greater than 0, the similarity scores are smaller. And the amount of decrease is inversely proportional to the number of residuals in the node. In other words, the leaf on the left had only one residual, and it had the largest decrease in similarity score 50%."
1925,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,995.48,1031.76,36.27999999999997," In contrast, the root had all four residuals in the smallest decrease, 20%. Now when we calculate the gain, we get 66, which is a lot less than 120.33, the value we got when lambda equal 0. Similarly, when lambda equal 1, the gain for the next branch is smaller than before. Now, just for comparison, these were the gain values when lambda equal 0."
1926,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1031.76,1060.68,28.920000000000076," When we first talked about pruning trees, we set gamma equal to 130. And because, for the lowest branch in the first tree, gain minus gamma equal to positive number, so we did not prune it all. Now, with lambda equal 1, the values for gain are both less than 130. So we would prune the whole tree away."
1927,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1060.68,1088.52,27.83999999999992," So when lambda is greater than 0, it is easier to prune leaves because the values for gain are smaller. Note, before we move on, I want to illustrate one last feature of lambda. For this example, imagine we split this node into two leaves. Now let's calculate the similarity scores with lambda equal to 1."
1928,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1088.52,1120.16,31.639999999999876," For the branch we get 65.3. For the left leaf we get 21.12. And for the right leaf we get 28.12. That means the gain is negative 16.06. Now when we decide if we should prune this branch, we plug in the gain and we plug in a value"
1929,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1120.16,1144.64,24.480000000000015," for gamma. Note, if we set gamma equal to 0, then we will get a negative number. And we will prune this branch, even though gamma equal 0. In other words, setting gamma equal to 0 does not turn off pruning. Dang!"
1930,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1144.64,1169.16,24.52000000000021," On the other hand, by setting lambda equal to 1, lambda did what it was supposed to do. It prevented overfitting the training data. Awesome! For now, regardless of lambda and gamma, let's assume that this is the tree we are working with and determine the output values for the leaves."
1931,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1169.16,1197.88,28.720000000000027," The output value equals the sum of the residuals divided by the number of residuals plus lambda. Note, the output value equation is like the similarity score, except we do not square the sum of the residuals. So for this leaf, we plug in the residual negative 10.5, the number of residuals in the leaf"
1932,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1197.88,1228.36,30.47999999999979," 1, and the value for the regularization parameter, lambda. If lambda equal 0, then there is no regularization and the output value equals negative 10.5. On the other hand, if lambda equals 1, the output value equals negative 5.25. In other words, when lambda is greater than 0, then it will reduce the amount that this"
1933,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1228.36,1250.52,22.16000000000008," individual observation adds to the overall prediction. Thus, lambda, the regularization parameter, will reduce the prediction sensitivity to this individual observation. For now, we'll keep things simple and let lambda equal 0, because this is the default value."
1934,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1250.52,1277.2800000000002,26.76000000000022," And put negative 10.5 under the leaf so we will remember it. Now let's calculate the output value for this leaf. When lambda equals 0, the output value is 7. In other words, when lambda equals 0, the output value for a leaf is simply the average of the residuals in that leaf."
1935,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1277.2800000000002,1305.64,28.3599999999999," So we'll put the output value under the leaf so we will remember it. Lastly, when lambda equals 0, the output value for this leaf is negative 7.5. Now, at long last, the first tree is complete. Double BAM. Since we have built our first tree, we can make new predictions."
1936,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1305.64,1330.76,25.11999999999989," And just like on extreme gradient boost, XG boost makes new predictions by starting with the initial prediction and adding the output of the tree scaled by a learning rate. Oh no, it's another dreaded terminology alert. XG boost calls the learning rate EDA. And the default value is 0.3."
1937,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1330.76,1365.9599999999998,35.19999999999982," So that's what we'll use. Thus, the new predicted value for this observation with dosage equal to 10 is the original prediction 0.5 plus the learning rate EDA 0.3 times the output value negative 10.5. And that gives us negative 2.65. So if the original prediction was 0.5, then this was the original residual."
1938,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1366.0,1389.24,23.24000000000001," The new prediction is negative 2.65. And we see that the new residual is smaller than before. So we've taken a small step in the right direction. Similarly, the new prediction for this observation with dosage equal to 20 is BPP PPPPPP 2.6."
1939,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1389.24,1415.84,26.59999999999991," And the new residual is smaller than before. So we've taken another small step in the right direction. Likewise, the new predictions for the remaining observations have smaller residuals than before, suggesting each small step was in the right direction. Now we build another tree based on the new residuals and make new predictions that give"
1940,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1415.84,1434.9199999999998,19.079999999999927, us even smaller residuals. And then build another tree based on the newest residuals. And we keep building trees until the residuals are super small or we have reached the maximum number. Simple BAM.
1941,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1434.9199999999998,1463.72,28.800000000000185," In summary, when building XG Boost trees from regression, we calculate similarity scores, and gain to determine how to split the data. And we prune the tree by calculating the differences between gain values and a user defined tree complexity parameter, gamma. If the difference is positive, then we do not prune."
1942,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1463.72,1485.0800000000002,21.360000000000127," If it's negative, then we prune. For example, if we subtract gamma from this gain and get a negative value, we will prune. Otherwise, we're done. If we prune, then we will subtract gamma from the next game value and work our way up the tree."
1943,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1485.0800000000002,1506.28,21.199999999999815," Then we calculate the output values for the remaining leaves. And lastly, lambda is a regularization parameter, and when lambda is greater than zero, it results in more pruning by shrinking the similarity scores, and it results in smaller output values for the leaves. BAM."
1944,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1506.28,1527.4,21.11999999999989," Tune in next time for XG Boost Part 2, when we give an overview of how XG Boost trees are built for classification, it's going to be totally awesome. BRAE! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
1945,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1527.4,1543.4399999999998,16.039999999999964," And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below. All right."
1946,XGBoost Part 1 (of 4): Regression,https://www.youtube.com/watch?v=OtD8wVaFm6E,OtD8wVaFm6E,1543.4399999999998,1545.52,2.0800000000001546," Until next time, quest on."
1947,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,0.0,31.68,31.68," Classification, it's not a vacation, it's not a sensation, but it's cool. Step Quest. Hello, I'm Josh Starman, welcome to Step Quest. Today we're going to talk about XG Boost Part 2, XG Boost Trades for classification. Note, this step Quest assumes that you're already familiar with the main ideas of how"
1948,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,31.68,59.84,28.160000000000004," XG Boost does regression, and at least the main ideas of how gradient boost is used for classification. If not, check out the quests, the links are in the description below. XG Boost is EXTRAIN! And that means it's a big machine learning algorithm with lots of parts. The good news is that each part is pretty simple and easy to understand, and we'll go through"
1949,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,59.84,90.4,30.56," them one step at a time. In part one in this series, we provided an overview of how XG Boost Trades are built for regression. In this video, part two, we'll give an overview of how XG Boost Trades are built for classification. In part three, we'll dive into the mathematical details to show you how regression and classification are related and why creating unique trees makes so much sense."
1950,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,90.4,124.44,34.03999999999999," Note, XG Boost was designed to be used with large, complicated data sets. However, to keep the examples from getting out of hand, we will use this super simple training data consisting of four different drug dosages. The green dots indicate the drug was effective, and the red dots indicate that the drug was not effective. The very first step in fitting XG Boost to training data is to make an initial prediction."
1951,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,124.44,151.4,26.960000000000008," This prediction can be anything, for example, the probability of observing an effective dosage in the training data, but by default it is 0.5, regardless of whether you're using XG Boost for regression or classification. In other words, regardless of the dosage, the default prediction is that there is a 50% chance the drug is effective."
1952,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,151.4,180.76,29.360000000000014," We can illustrate the initial prediction by adding a Y-axis to our graph to represent the probability that the drug is effective, and drawing a thick black line at 0.5 to represent a 50% chance that the drug is effective. Since these two green dots represent effective dosages, we will move them to the top of the graph, where the probability that the drug is effective is 1."
1953,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,180.76,207.88,27.119999999999976," These two red dots represent ineffective dosages, so we will leave them at the bottom of the graph, where the probability that the drug is effective is 0. The residuals, the differences between the observed and predicted values, show us how good the initial prediction is. Now, just like we did for regression, we fit an XG Boost tree to the residuals."
1954,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,207.88,239.04,31.160000000000025," However, since we are using XG Boost for classification, we have a new formula for the similarity scores. Even though the numerator looks fancy, it's just the sum of the residuals squared. In other words, the numerator for classification is the same as the numerator for regression. And just like for regression, the denominator contains lambda, the regularization parameter."
1955,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,240.48,273.12,32.639999999999986," However, the rest of the denominator is different. The good news is that we already saw something just like this in regular, unexterian gradient boost. It's just the sum for each observation of the previously predicted probability times 1 minus the previously predicted probability. Note, although this formula is different from what XG Boost uses for regression, it is very closely related,"
1956,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,273.12,299.6,26.480000000000015," and we will show you why in part 3 when we get into the nitty-gritty details. Now let's build a tree. Just like for regression, each tree starts out as a single leaf, and all of the residuals go to the leaf. Now we need to calculate a similarity score for the leaf. And that means we plug all four residuals into the numerator."
1957,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,299.92,327.84000000000003,27.920000000000016," Note, because we do not square the residuals before we add them together, they will cancel each other out. And we will end up with zero in the numerator, and that makes the similarity score equal to zero. And that's a little bit of a bummer, since it doesn't give us a chance to talk about the denominator, which is the interesting part. However, don't freak out, we'll get to it soon."
1958,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,328.79999999999995,358.48,29.680000000000007," For now, let's just put similarity equals zero up here so we can keep track of it. Now we need to decide if we can do a better job clustering similar residuals if we split them into two groups. We'll start with this threshold, dosage less than 15. Note, we chose the threshold dosage less than 15, because 15 is the average value between the last two observations."
1959,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,360.0,391.36,31.360000000000014," Thus, the three residuals with dosages less than 15 go to the leaf on the left, and the one residual with dosage greater than 15 goes to the leaf on the right. To calculate the similarity score for the three residuals that ended up in the leaf on the left, we plug the three residuals into the numerator, and since we are building the first tree, the previous probability refers to the prediction from the initial leaf."
1960,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,392.64,419.7600000000001,27.120000000000005," So we plug in 0.5 for each residual that ended up in the leaf. Now, just to keep things simple, we'll let land a equal zero. However, you know from part one that land a reduces the similarity score, which ultimately makes leaves easier to prune. Now, notice that these two residuals in the numerator cancel each other out,"
1961,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,420.88,449.68,28.80000000000001," leaving us with just one residual in the numerator. And when we do the math, we get 0.33. So let's put similarity equals 0.33 under this leaf so we can keep track of it. The similarity score for the leaf on the right is... 1 when land a equal zero."
1962,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,451.36,477.68,26.319999999999997," So let's put similarity equals 1 under this leaf to keep track of it. Now we can calculate the gain just like we did when we used xg boost for regression. We plug in the similarity scores, and get 1.33. So when we split the observations based on the threshold,"
1963,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,477.68,510.32,32.639999999999986," dosage less than 15, gain equals 1.33. Since I'm such a nice guy, I'm going to tell you that no other threshold gives us a larger gain value. And that means dosage less than 15 will be the first branch in our tree. Now we can focus on splitting these residuals into two leaves. Note, we can tell just by looking at the data that this threshold dosage less than 5"
1964,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,511.28,531.84,20.56," has a higher gain than this threshold dosage less than 10. This is because when the threshold is dosage less than 10, these two residuals will cancel each other out. And the similarity score for this leaf will be zero. So when we calculate the gain, we get..."
1965,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,536.4,558.88,22.480000000000015," 0.66. Now let's compare that to the gain we get when the threshold is dosage less than 5. These are the similarity scores, and when we plug them into the equation for the gain, we get 2.66."
1966,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,560.16,580.96,20.800000000000068," And since 2.66 is greater than 0.66, we'll use dosage less than 5 as the threshold for this branch. Now, since I'm limiting the trees to two levels, we will not split this leaf any further, and we are done building this tree. Bam!"
1967,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,582.6400000000001,614.48,31.83999999999992," Note, we stop growing this tree because we limited the number of levels to two. However, XG Boost also has a threshold for the minimum number of residuals in each leaf. Warning, it's time for some tedious detail and terminology. The minimum number of residuals in each leaf is determined by calculating something called cover. Cover is defined as the denominator of the similarity score minus lambda."
1968,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,615.6,646.0,30.399999999999977," In other words, when we are using XG Boost for classification, cover is equal to the sum of the previous probability times 1 minus the previous probability for each residual that's in the leaf. In contrast, when XG Boost is used for regression and we are using this formula for the similarity score, then cover is equal to the number of residuals in a leaf."
1969,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,646.56,679.0400000000001,32.48000000000013," By default, the minimum value for cover is 1. Thus, by default, when we use XG Boost for regression, we can have as few as one residual per leaf. In other words, when we use XG Boost for regression and use the default minimum value for cover, cover has no effect on how we grow the tree. In contrast, things are way more complicated when we use XG Boost for classification,"
1970,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,679.6,709.76,30.159999999999968," because cover depends on the previously predicted probability of each residual in a leaf. For example, the cover for this leaf is the previously predicted probability for this observation, which was 0.5 times 1 minus the previously predicted probability, which is 0.25. And since the default value for the minimum cover is 1,"
1971,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,709.76,738.8800000000001,29.12000000000012," XG Boost would not allow this leaf. Likewise, the cover for this leaf is equal to 0.5. So, by default, XG Boost would not allow this leaf either. Since these leaves are not allowed, let's remove them and go back to this leaf. Because the previously predicted probability is the same for all three of these residuals,"
1972,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,740.0,765.9200000000001,25.920000000000076," cover is just three times the cover for one of the residuals, and that means cover equals 0.75 So, XG Boost would not allow this leaf either. Ultimately, if we use the default minimum value for cover 1, then we would be left with the root, and XG Boost requires trees to be larger than just the root."
1973,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,767.12,787.52,20.399999999999977," So, in order to prevent this from being the worst example ever, let's set the minimum value for cover equal to 0. And that means setting the men-child weight parameter equal to 0. Small bam. Now we can talk about how to prune the tree."
1974,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,789.1999999999999,814.88,25.680000000000064," Just like we did in part 1, we prune by calculating the difference between the gain associated with the lowest branch and a number we pick for gamma. For example, if we plugged in the gain and set gamma equal to 2, then we would not prune because the difference is a positive number. In contrast, if we set gamma equal to 3,"
1975,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,815.76,836.5600000000001,20.800000000000068, then we would prune because the difference is a negative number. And we would also prune this branch because 1.33 minus 3 equals a negative number. And all we would be left with is the original prediction. Small bam.
1976,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,837.6,861.2,23.600000000000023," Now, going back to the original tree, remember from part 1 that lambda, the regularization parameter, reduces the similarity scores, and that lower similarity scores result in lower values for gain. For example, if we set lambda equal to 1, then we would get these lower values for gain."
1977,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,862.4000000000001,885.0400000000001,22.639999999999983," And that means a lower value for gamma will result in a negative difference and cause us to prune branches. In other words, values for lambda greater than 0 reduce the sensitivity of the tree to individual observations by pruning and combining them with other observations. Bam."
1978,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,886.72,908.88,22.159999999999968," For now, regardless of lambda and gamma, let's assume that this is the tree we are working with and determine the output values for the leaves. For classification, the output value for a leaf is the sum of the residuals divided by the sum of the previous probability"
1979,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,908.88,931.6,22.720000000000027," times 1 minus the previous probability for each residual in the leaf plus lambda. Note, with the exception of lambda, the regularization parameter, this is the same formula we used for un-extreme gradient boost. So, for this leaf, we plug in the residual,"
1980,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,931.6,952.88,21.279999999999973," negative 0.5, and the previously predicted probability and the value for the regularization parameter, lambda. If lambda equals 0, then there is no regularization and the output value equals negative 2. On the other hand, if lambda equals 1,"
1981,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,954.0,972.0,18.0," then the output value equals negative 0.4, which is closer to 0 than negative 2, when lambda equals 0. In other words, when lambda is greater than 0, then it reduces the amount that this single observation adds to the new prediction."
1982,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,973.68,992.24,18.56000000000006," Thus, lambda, the regularization parameter, reduces the prediction sensitivity to isolated observations. For now, we'll let lambda equals 0, because this is the default value, and put negative 2 under the leaf, so we will remember it."
1983,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,993.6,1021.04,27.43999999999994," Similarly, when lambda equals 0, the output value for this leaf, is 2. Note, if lambda equals 1, then we get 0.67, which is closer to 0, but the effect of lambda is smaller this time, because there are two observations in this leaf."
1984,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1022.4,1039.28,16.879999999999995," But like I said, we'll let lambda equals 0, since that is the default value, and put 2 under the leaf, so we will remember it. Lastly, when lambda equals 0, the output value for this leaf is negative 2."
1985,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1040.88,1057.28,16.40000000000009," Hooray, the first tree is complete. Double bound. Now that we have built the first tree, we can make new predictions. Just like other boosting methods, XG Boost for classification makes new predictions"
1986,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1057.28,1080.64,23.3599999999999," by starting with the initial prediction. However, just like with unextreed gradient boost for classification, we need to convert this probability to a log odds value. So, since this is the formula that converts probabilities to odds, we can get a formula that converts probabilities to the log odds"
1987,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1080.64,1098.48,17.840000000000146," by taking the log of both sides. Note, if these equations are freaking you out, just watch the stat quest on odds and log odds, the link is in the description below. In this case, we plug in p equals 0.5,"
1988,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1099.44,1122.96,23.519999999999985," do the math, and we see that when p equals 0.5, the log of the odds equals 0. So let's put that under the initial prediction so we don't forget. Now, just like unextreed gradient boost for classification, we add the log odds of the initial prediction"
1989,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1123.76,1147.28,23.519999999999985," to the output of the tree scaled by a learning rate. XG Boost calls the learning rate Eta, and the default value is 0.3, so that's what we'll use. Thus, the new predicted value for this observation with dosage equal 2 is the log of the odds for the original prediction,"
1990,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1147.28,1174.72,27.44000000000005," 0 plus the learning rate 0.3 times the output value negative 2, and that gives us a log of the odds value equal to negative 0.6. To convert a log of the odds value into a probability, we plug it into the logistic function. Note, if the logistic function makes you feel a little uncomfortable,"
1991,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1174.72,1195.2,20.480000000000015," check out the stack quest, logistic regression details part 2, fitting a line with maximum likelihood. Assuming we're cool with this equation, let's plug in the log of the odds. Do the math, and the new predicted probability is 0.35."
1992,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1196.16,1215.2,19.039999999999964," Remember, the original prediction was 0.5, and this was the original residual. Now the new predicted probability is 0.35, and the new residual is smaller than before, so we have taken a small step in the right direction."
1993,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1216.16,1238.48,22.319999999999936," Hooray! Note, you may be wondering why we even bothered adding the log of the odds of the initial prediction since it is 0. This is always the case if you use the default value 0.5 for the initial prediction. However, you can change the initial prediction to any probability,"
1994,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1238.48,1258.88,20.399999999999864," and any value other than 0.5 will give you something more interesting to add. For example, if 75% of the observations in the training data said that the drug was effective, we might set the initial prediction to 0.75, and now the initial log of the odds equals 1.1,"
1995,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1259.76,1279.84,20.079999999999927," so we would plug 1.1 into the equation instead of 0. But since the default initial prediction is 0.5, we will use 0 for the initial log of the odds in the remaining examples. Now let's make a new prediction for this observation, with dosage equal to 8."
1996,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1280.8,1302.8,22.0," We start with the original log of the odds prediction, 0, plus 0.3 times the output value, 2, and the predicted log of the odds equals 0.6. Now we convert the log of the odds to a probability, and we get 0.65."
1997,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1304.24,1320.24,16.0," And this residual is smaller than before, so we've taken another small step in the right direction. Likewise, the new predictions for the remaining observations have smaller residuals than before. Bam!"
1998,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1322.08,1341.28,19.200000000000045," Now that we have new residuals, we can build a second tree that is fit to the new residuals. Note, when we build the second tree, calculating the similarity score is a little more interesting because the previous probabilities are no longer the same for all of the observations."
1999,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1343.04,1369.04,26.0," For example, since all of the residuals start in the root of the tree, we would plug in the previously predicted probabilities for each observation into the denominator, and this time they are not all the same. Similarly, if we had to calculate an output value for the root, the denominator would also contain a mixture of previously predicted probabilities."
2000,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1370.4,1387.12,16.720000000000027," Small bam! Now that we have a new tree, we add it to all of the previous predictions and make new predictions that give us even smaller residuals. Then we build another tree based on the new residuals,"
2001,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1388.16,1407.44,19.279999999999973," and we keep building trees until the residuals are super small, or we have reached the maximum number of trees. Triple bam! In summary, when building XG Boost trees for classification, we calculate similarity scores,"
2002,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1409.52,1431.6799999999998,22.15999999999985," and gain to determine how to split the data, and we prune the tree by calculating the difference between gain values and a user defined tree complexity parameter gamma. For example, if we subtract gamma from this gain and get a negative value, we will prune, otherwise we're done."
2003,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1432.96,1450.8,17.839999999999918," If we prune, then we will subtract gamma from the next gain value, et cetera et cetera et cetera. Then we calculate the output values for the leaves, and lastly, lambda is a regularization parameter, and when lambda is greater than zero,"
2004,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1450.8,1468.08,17.279999999999973," it results in more pruning by shrinking the similarity scores and smaller output values for the leaves. Oh, and I almost forgot, when using XG Boost for classification, we have to be aware that the minimum number of residuals in a leaf"
2005,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1468.08,1483.44,15.360000000000127," is related to a metric called cover, which is the denominator of the similarity score minus lambda. Toon in next time for XG Boost Part 3, when we dive deep into the nitty-gritty details of the math"
2006,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1483.44,1499.52,16.079999999999927," that ties XG Boost trees for regression and classification into one elegant equation. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
2007,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1499.92,1513.28,13.3599999999999," And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below."
2008,XGBoost Part 2 (of 4): Classification,https://www.youtube.com/watch?v=8b1JEDvenQU,8b1JEDvenQU,1514.0800000000002,1517.84,3.759999999999991," All right, until next time, quest on."
2009,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,0.0,29.0,29.0," XG Boost Math Details. There's a lot of them. Watch out. Stat Quest. Hello, I'm Josh Starman. Welcome to Stat Quest. Today we're going to talk about XG Boost Part 3 mathematical details. This Stat Quest assumes that you already have a general idea of how XG Boost builds trees. If not, check out the quests. The links are in the description below."
2010,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,30.0,53.0,23.0," This Stat Quest also assumes that you know the details of how gradient boost works. If not, check out the quests. Lastly, it assumes that you are familiar with Ridge Regression. If not, the link is in the description below. In XG Boost Part 1, we saw how XG Boost builds XG Boost trees for regression."
2011,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,53.0,101.0,48.0," And in XG Boost Part 2, we saw how XG Boost builds XG Boost trees for classification. In both cases, we build the trees using similarity scores and then calculated the output values for the leaves. Now we will derive the equations for the similarity scores and the output values and show you how the only difference between regression and classification is the loss function. To keep the examples manageable, we'll start with a simple training dataset for regression and this simple training dataset for classification. For regression, we are using drug dosage on the X-axis to predict drug effectiveness on the Y-axis."
2012,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,102.0,145.0,43.0," For classification, we are using drug dosage on the X-axis to predict the probability the drug will be effective. For both regression and classification, we already know that XG Boost starts with an initial prediction that is usually 0.5. And in both cases, we can represent this prediction with a thick black line at 0.5. And the residuals, the differences between the observed and predicted values, show us how good the initial prediction is. Just like in regular, unextreed gradient boost, we can quantify how good the prediction is with a loss function."
2013,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,146.0,192.0,46.0," In gradient boost part 2, regression details, we learned how to use this loss function. One half times the squared residual for regression. To review, Y-subby stands for the Y-axis value from one of the observed values, Y-sub1, Y-sub2, and Y-sub3. And P-subby stands for a prediction, P-sub1, P-sub2, and P-sub3 that corresponds to one of the observations, Y-sub1, Y-sub2, and Y-sub3. For example, if we applied the loss function to the initial prediction, then we would add one term for each observation, so N equals 3."
2014,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,193.0,238.0,45.0," The first term in the summation corresponds to the first observation, Y-sub1. The second term corresponds to the second observation, Y-sub2, and the third term corresponds to the third observation, Y-sub3. So we just plug in the numbers, and do the math, and get 104.4. Later, we can apply the loss function to new predictions and compare the results to this one to determine if our predictions are improving or not. Note, if we had N observations, then we would add up N terms."
2015,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,238.0,293.0,55.0," No big deal. In gradient-boost part 4, classification details, we learned how to use this loss function, the negative log likelihood for classification. And just like before, Y-sub-I refers to a Y-axis value for one of the observed values, which is either 0 or 1, and P-sub-I refers to a predicted value between and including 0 and 1. Note, if you want more details and examples using this loss function, check out the stat quest, gradient-boost part 4, classification details. Now that we have one loss function for regression, and another loss function for classification, X-G-Boost uses those loss functions to build trees by minimizing this equation."
2016,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,294.0,344.0,50.0," Note, the equation in the original manuscript for X-G-Boost contains an extra term that I'm omitting. This term, gamma times T, where T is the number of terminal nodes or leaves in a tree, and gamma is a user-defined penalty, is meant to encourage pruning. I say that an encourages pruning, because, as we saw an X-G-Boost part 1, X-G-Boost can prune even when gamma equals 0. I'm omitting this term because, as we saw in parts 1 and 2, pruning takes place after the full tree is built, and it plays no role in deriving the optimal output values or similarity scores. So let's talk about this equation."
2017,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,344.0,379.0,35.0," The first part is the loss function, which we just talked about. In the second part, consists of a regularization term. The goal is to find an output value for the leaf that minimizes the whole equation. And in a way that is very similar to ridge regression, we square the output value from the new tree, and scale it with lambda. Later on, I will show you that, just like ridge regression, if lambda is greater than 0, then we will shrink the output value."
2018,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,380.0,424.0,44.0," The 1 half just makes the math easier. Note, because we are optimizing the output value from the first tree, we can replace the prediction, piece of I, with the initial prediction, p of 0, plus the output value from the new tree. Now that we understand all of the terms in this equation, let's use it to build the first tree. We start by putting all of the residuals into a single leaf, and now we need to find an output value for this leaf that will minimize this equation. But before we dive into the math, let's simplify things by setting lambda equal to 0."
2019,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,425.0,457.0,32.0," And that means removing the regularization term. Okay, now let's plug in different output values for the leaf and see what happens. For example, if we set the output value equal to 0, then we are left with the loss function for the initial prediction 0.5. Note, we already calculated the loss function for the initial prediction when we demonstrated the regression loss function. And we got 104.4."
2020,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,458.0,494.0,36.0," So let's move the equation up here and plot the result on this graph. The x-axis on this graph represents different values for the output value. And the y-axis represents the sum of the loss functions for each observed value. Now let's see what happens if we set the output value equal to negative 1. When the output value for the leaf is negative 1, then the new prediction is 0.5 plus negative 1, which equals negative 0.5."
2021,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,495.0,538.0,43.0," And negative 0.5 corresponds to this new thick black line. And that shrinks the residual for y-sub 1, but it makes the residuals for both y-sub 2 and y-sub 3 larger. When we do the math, we get 109.4. Now we plot the point on the graph and we see that negative 1 is a worse choice for the output value than 0, because it has a larger total loss. In contrast, if we set the output value to positive 1, then the new prediction is 0.5 plus 1, which equals 1.5."
2022,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,539.0,568.0,29.0," That makes the residual for y-sub 1 larger, but the residuals for y-sub 2 and y-sub 3 are smaller. Now when we do the math, we get 102.4. And we see that we have the lowest total so far. And thus, positive 1 is the best output value we have picked so far. Note, we can keep plugging in numbers for the output value to see what we get."
2023,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,569.0,603.0,34.0," And we can just plot the function as a curve. The curve shows us that when lambda is 0, then the optimal output value is at the bottom of the parabola, where the derivative is 0. Now let's see what happens when we increase lambda to 4. When lambda equals 4, the lowest point in the parabola shifts closer to 0. And if we increase lambda to 40, then the lowest point in the parabola shifts even closer to 0."
2024,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,604.0,636.0,32.0," In other words, the more emphasis we give the regularization penalty by increasing lambda, the optimal output value gets closer to 0. And this is exactly what regularization is supposed to do, so that's super cool. Bam! Now, one last thing before we solve for the optimal output value. You may remember that when regular, on-extreme gradient boost, found the optimal output value for a leaf,"
2025,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,637.0,678.0,41.0," it solved an equation very similar to the first part without regularization. On-extreme gradient boost, he used two techniques to solve this equation, one for regression, because the math was easy, and at different one for classification, because the math was not easy. Specifically, for classification, on-extreme gradient boost used a second-order Taylor approximation to simplify the math when solving for the optimal output value. In contrast, XG boost used the second-order Taylor approximation for both regression and classification."
2026,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,679.0,716.0,37.0," Unfortunately, explaining Taylor series approximations is out of the scope of this stack quest. So you'll just have to take my word for it that the loss function that includes the output value can be approximated by this mess of sums and derivatives. The genius of a Taylor approximation is that it's made of relatively simple parts. This part is just the loss function for the previous prediction. This is the first derivative of that loss function, and this is the second derivative of that loss function."
2027,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,717.0,751.0,34.0," Note, since the derivative of a function is related to something called a gradient, XG boost uses G to represent the derivative of the loss function. And since the second derivative of a function is related to something called a Hessian, XG boost uses H to represent the second derivative of the loss function. Now let's expand the summation. Bip, bip, bip, bip, add the regularization term, and plug in the second-order Taylor approximation for each loss function."
2028,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,752.0,781.0,29.0," Bip, bip, bip, bip, bip. Before we move on, let's remember that we want to find an output value that minimizes the loss function plus the regularization. And that all we have done so far is approximate the equation we want to minimize with the second-order Taylor polynomial. Now it's worth noting that these terms do not contain the output value. And that means they have no effect on the optimal output value,"
2029,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,781.0,806.0,25.0," so we can omit them from the optimization. Now all that remains are terms associated with the output value. So let's combine all of the unsquared output value terms into a single term, and combine all of the squared output value terms into a single term. And move the formula to give us some space to work."
2030,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,806.0,837.0,31.0," Now let's do what we usually do when we want a value that minimizes a function. One, take the derivative with respect to the output value. Two, set the derivative equal to zero, and three, solve for the output value. The derivative of the first term with respect to the output value is just the sum of the G's. When we take the derivative of the second term with respect to the output value,"
2031,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,838.0,864.0,26.0," the exponent, two, comes down and cancels out the one half, leaving us with the sum of the H's and lambda times the output value. Now we set the derivative equal to zero, and solve for the output value. So we subtract the sum of the G's from both sides, and divide both sides by the sum of the H's and lambda."
2032,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,865.0,892.0,27.0," Whoay! We have finally solved for the optimal output value for the leaf. Now we need to plug in the gradients, the G's, and the Hessians, the H's for the loss function. So let's move the equation out of the way so we have some room. If we're using XG boost for regression, then this is the most commonly used loss function."
2033,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,893.0,918.0,25.0," Shameless self-promotion. Just to remind you, this is the exact same loss function that we described in detail in gradient boost part two for regression details. So if the next few steps move too quickly, just check out the quest. The link is in the description below. The first derivative, aka the gradient G sub i,"
2034,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,918.0,948.0,30.0," who's respect to the predicted value, P sub i is, negative one times the difference between the observed value, Y sub i, and the predicted value, P sub i. In other words, G sub i is the negative residual. That means we plug in the negative residual for each G sub i in the numerator. Note, this negative sign cancels out all of these negative signs."
2035,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,950.0,977.0,27.0," So the whole numerator is just the sum of the residuals. Now we need to figure out what the H is are in the denominator. The second derivative, aka the Hessian H sub i, with respect to the predicted value, P sub i is, the number one. So that means we replace all in H's in the denominator with the number one."
2036,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,978.0,1007.0,29.0," In other words, the denominator is the number of residuals plus lambda. So, when we are using xg boost for regression, this is the specific formula for the output value for a leaf. To summarize what we've done so far, we started out with this data. Then we made an initial prediction 0.5, then we put all of the residuals in this leaf."
2037,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1008.0,1029.0,21.0," Then we asked ourselves what the output value of this leaf should be given, a loss function, and regularization. We then plotted the equation as a function of the output value, and solved for the lowest point where the derivative is zero. And this is what we got."
2038,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1030.0,1054.0,24.0," The equation for the output value gives us the x-axis coordinate for the lowest point in the parabola. Bam! Now, if we are using xg boost for classification, then this, the negative log likelihood is the most commonly used loss function. Shamedless self-promotion."
2039,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1055.0,1087.0,32.0," This is the exact same loss function that we worked with in gradient boost part 4 classification details. In that stack quest, we spent a long time deriving the first and second derivative of this equation. Calculating the derivatives took a long time because the output values are in terms of the log odds. So we converted the probabilities to log odds one step at a time, rather than skipping the fun parts like we are now."
2040,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1088.0,1117.0,29.0," Then we took the derivatives without skipping the fun parts like we're doing here. And lastly, we converted the log odds back to probabilities without skipping any steps in the math. Note, if you feel like you just missed out on a lot of fun stuff, we are not. The link to gradient boost part 4 classification details is in the description below. Now that we have the first derivative of the loss function,"
2041,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1118.0,1150.0,32.0," the gradient, g sub i, and the second derivative of the loss function, the hs and h sub i, we can plug them into the equation for the optimal output value. Just like for regression, g sub i is the negative residual. And we know that this negative sign will cancel out this negative sign in the numerator for the output value. So we can replace the numerator for the output value with the sum of the residuals."
2042,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1151.0,1191.0,40.0," In the denominator, we can just replace all of the h sub i's with the sum of p sub i times 1 minus p sub i. Note, in the denominator, we're using previous probability to specify the previously predicted probability rather than the previously predicted log odds. So when we are using xg boost for classification, this is the specific formula for the output value for a leaf. Bam! Now, regardless of whether we are using xg boost for regression or classification,"
2043,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1192.0,1223.0,31.0," we can calculate the optimal output value for this leaf. And we do it by plugging derivatives of the loss functions into the equation for the output value. Bam! Now we need to derive the equations for the similarity score so we can grow the tree. However, before we do that, remember that we derived the equation for the output value by minimizing the sum of the loss functions plus the regularization."
2044,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1224.0,1265.0,41.0," And let's also remember that depending on the loss function, optimizing this part can be hard, so we are approximated it with the second order Taylor polynomial. So we expanded the summation, bitta bitta bitta bitta bitta bitta. Added the regularization term and swapped in the second order Taylor approximation of the loss function. Then removed the constant terms and lastly did a little algebra to simplify everything. Now, because we removed constants when deriving this equation, it's not equal to what we started with."
2045,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1266.0,1312.0,46.0," However, if we plotted both equations on a graph, we'd see that the same x-axis coordinate represented by the optimal value tells us the location of the lowest points in both parabolas. I mentioned this because x-g-boost uses the simplified equation to determine the similarity score. So the first thing x-g-boost does is multiply everything by negative 1. And that makes each term negative and it flips the parabola over the horizontal line y equals 0. Now, the optimal output value represents the x-axis coordinate for the highest point on the parabola."
2046,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1312.0,1346.0,34.0," And this y-axis coordinate for the highest point on the parabola is the similarity score. At least, it's the similarity score described in the original x-g-boost manuscript. However, the similarity score used in the implementations is actually two times that number. The reason for this difference will become clear once we do the algebra. So let's do the algebra to convert this into the similarity scores we saw in x-g-boost parts 1 and 2."
2047,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1346.0,1381.0,35.0," First, let's plug in the solution for the output value. Now multiply together the sums of the gradients, g's on the left. Note, these negative signs cancel out and we get the square of the sum. Now we square the term on the right and let this sum cancel out this square. Now we add these two terms together and we end up with this fraction."
2048,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1381.0,1415.0,34.0," This is the equation for the similarity score as described in the original x-g-boost manuscript. However, in the x-g-boost implementations, this one half is omitted because the similarity score is only a relative measure. And as long as every similarity score is scaled the same amount, the results of the comparisons will be the same. This is an example of how extreme extreme gradient boost is. It will do anything to reduce the amount of computation."
2049,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1415.0,1450.0,35.0," Now, if we're using x-g-boost for regression and we're using this loss function, we plug the first derivative g sub i into the numerator. And since g sub i is the negative residual, the numerator is simply the sum of the residuals squared. Now we plug the second derivative h sub i into the denominator. And since h sub i equals 1, the denominator is just the number of residuals plus lambda."
2050,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1450.0,1474.0,24.0," Thus, this equation is the similarity score that we use for regression. Double bound. Now we need to derive the similarity score for classification. And that just means plugging in the first derivative for the loss function g sub i, and the second derivative h sub i."
2051,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1474.0,1507.0,33.0," Again, since g sub i is the negative residual, the numerator is simply the sum of the residuals squared. And since h sub i is the previously predicted probability times 1 minus the previously predicted probability, then the denominator is just the sum of the h sub i's plus lambda. Thus, this equation is the similarity score that we use for classification. Triple bound."
2052,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1507.0,1533.0,26.0," Now for one little annoying detail. In part two of this series, we talked about cover. Cover is related to the minimum number of residuals in a leaf. And we said that cover was the denominator of the similarity score minus lambda. In other words, cover is the sum of the hessions, the h sub i's."
2053,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1533.0,1569.0,36.0," For regression, the hession, aka the second derivative of the loss function is 1. And since there is one hession per residual in a leaf, cover for regression is simply the number of residuals in a leaf. For classification, the hession is p times 1 minus p. So cover is equal to the sum of the previously predicted probability times 1 minus the previously predicted probability. Small bound."
2054,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1569.0,1614.0,45.0," In summary, xg boost builds trees by finding the output value that minimizes this equation. The equation consists of a loss function and a regularization term that is just like ridge regression. We then solve for the optimal output value. And once we have the output value, we plug it into the simplified equation to get the similarity score. We can figure the output value and similarity score equations for regression or classification by plugging in the first derivative, g sub i, and the second derivative, h sub i of the loss function."
2055,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1614.0,1636.0,22.0," Bam. We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign. Becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate."
2056,XGBoost Part 3 (of 4): Mathematical Details,https://www.youtube.com/watch?v=ZVFeW798-2I,ZVFeW798-2I,1636.0,1643.0,7.0," The links are in the description below. Alright, until next time, quest on."
2057,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,0.0,12.38,12.38," I wanna do things fast, wanna do things faster. XG Boost got crazy optimizations that gonna blow your mind you better watch out cause it's so crazy. Stack Quest."
2058,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,13.7,34.82,21.12," Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're gonna talk about XG Boost Part 4 optimizations. Note, this Stack Quest assumes that you are already familiar with how XG Boost creates trees for classification and regression. If not, check out the quests."
2059,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,34.82,51.14,16.32," The links are in the description below. This Stack Quest also assumes that you are familiar with quantiles and percentiles. If not, check out the quest. XG Boost is a big algorithm with a lot of parts."
2060,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,52.06,69.14,17.08," And since gradient boost and regularization were covered in other Stack Quest videos, so far, this series has focused entirely on XG Boost's unique regression trees. So, now it's time to talk about the other parts."
2061,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,70.26,86.34,16.08," These parts are what make XG Boost relatively efficient with relatively large training data sets. In other words, the first three parts give us a conceptual idea of how XG Boost is fit to training data and how it makes predictions."
2062,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,87.5,108.22,20.72," And the last six parts describe optimizations for large data sets. So let's start by talking about the approximate greedy algorithm. In XG Boost Part 1, XG Boost trees for regression, we had a super simple training data set and used,"
2063,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,108.22,127.54,19.320000000000007," different drug dosages to predict drug effectiveness. The first thing we did was make an initial prediction, which could be anything like the mean drug effectiveness, but by default is 0.5. Then we calculated the residuals."
2064,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,128.54000000000002,143.9,15.359999999999983, And fit a tree to the residuals. We did this by calculating similarity scores and the gain for each possible threshold. And the threshold with the largest gain is the one XG Boost uses.
2065,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,145.22,165.1,19.879999999999995," Note, the decision to use the threshold that gives the largest gain is made without worrying about how the leaves will be split later. And that means XG Boost uses a greedy algorithm to build trees. In other words, since XG Boost uses a greedy algorithm,"
2066,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,165.1,184.54,19.44," it makes a decision without looking ahead to see if it is the absolute best choice in the long term. In contrast, if XG Boost did not use a greedy algorithm, it would postpone making a final decision about this threshold until after trying different thresholds"
2067,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,184.54,202.42,17.880000000000024," in the leaves to see how things played out in the long run. And this same process would be repeated for every single possible threshold for the root. In other words, by using a greedy algorithm, XG Boost can build a tree relatively quickly."
2068,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,203.86,223.34,19.47999999999999," That said, when we have a lot of measurements, then the greedy algorithm becomes slow because it still has to look at every possible threshold. And if we had a more interesting training data set that used, dosage, mass, favorite number,"
2069,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,224.86,239.9,15.039999999999992," and a bunch of other stuff to predict drug effectiveness, then checking every single threshold in every single variable would take forever. Want, want?"
2070,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,240.86,259.02,18.159999999999968," This is where the approximate greedy algorithm comes in. Going back to our example with a lot of observations, instead of testing every single threshold, we could divide the data into quantiles and only use the quantiles as candidate thresholds"
2071,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,259.02,274.58,15.560000000000002," to split the observations. For example, instead of using the smallest two dosages to define the first threshold, the approximate greedy algorithm uses the first quantile to define the first threshold."
2072,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,275.78000000000003,290.26,14.47999999999996," And the second quantile is the second threshold that we will consider. It's setra, et cetera, et cetera. Note, if we only used one quantile and split the observations in half,"
2073,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,291.14,305.58,14.439999999999998," then, since there are no other options, that quantile would be the threshold. This would make finding the best threshold very fast since we would not have to calculate gain or similarity to make the decision."
2074,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,306.62,319.82,13.199999999999989," But since both sides of the threshold represent a lot of people who have positive drug effectiveness values, and a negative drug effectiveness values, then this threshold would not do a good job"
2075,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,319.82,333.46,13.639999999999986," predicting drug effectiveness. In contrast, if we had two quantiles, then our predictions would improve because we would do a better job separating observations with positive values"
2076,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,333.46,352.74,19.28000000000003," for drug effectiveness from observations with negative values for drug effectiveness. So, for this data, two quantiles are better than one. If we had five quantiles, then our predictions would be more accurate since each threshold"
2077,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,352.74,370.38,17.639999999999986," represents a smaller cluster of observations. However, the more quantiles we have, the more thresholds we will have to test, and that means it will take longer to build the tree. For XG boost, the approximate greedy algorithm"
2078,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,370.38,388.38,18.0," means that instead of testing all possible thresholds, we only test the quantiles. And by default, the approximate greedy algorithm uses about 33 quantiles. Now the question is, why do we say about 33 quantiles"
2079,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,388.38,404.38,16.0," instead of exactly 33 quantiles? To answer this question, we need to talk about parallel learning and the weighted quantile sketch. When you have tons and tons of data,"
2080,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,405.38,418.38,13.0," so much data that you can't fit it all into a computer's memory at one time, then things that seem simple, like sorting a list of numbers and finding quantiles, become really slow."
2081,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,419.38,438.38,19.0," To get around this problem, a class of algorithms and batches can quickly create approximate solutions. Unfortunately, explaining the details of sketch algorithms is out of the scope of this stack quest, but we can discuss the general idea of how XG boost uses them."
2082,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,439.38,454.38,15.0," For this example, imagine we are just using a ton of dosages to predict drug effectiveness. Now let's move this data set to the top of the screen, and imagine splitting it into small pieces"
2083,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,454.38,473.38,19.0," and putting the pieces on different computers on a network. The quantile sketch algorithm combines the values from each computer to make an approximate histogram. Then, the approximate histogram is used to calculate approximate quantiles."
2084,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,473.38,497.38,24.0," And the approximate gradient algorithm uses approximate quantiles. Bam? No, not yet. So far, we have described the quantile sketch algorithm, but XG boost uses a weighted quantile sketch. So that means that these quantiles are not normal every day quantiles."
2085,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,498.38,516.38,18.0," Instead, they are weighted quantiles. Hey, what's a weighted quantile? Usually, quantiles are set up so that the same number of observations are in each one. In other words, if 10 observations were in this quantile,"
2086,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,517.38,536.38,19.0," then there would be 10 observations in this quantile. And 10 observations in this quantile. It's cetera, et cetera, et cetera. In contrast, with weighted quantiles, each observation has a corresponding weight."
2087,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,537.38,559.38,22.0," And the sum of the weights are the same in each quantile. For example, if the sum of the weights in this quantile was 10, then the sum of the weights in this quantile would also be 10. In the sum of the weights in this quantile would be 10, et cetera, et cetera, et cetera."
2088,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,560.38,586.38,26.0," The weights are derived from the cover metric that we discussed in parts 2 and 3 in this series. Specifically, the weight for each observation is the second derivative of the loss function, but we are referring to as the Hessian. That means for regression, the weights are all equal to 1. And that means the weighted quantiles are just like the normal quantiles"
2089,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,586.38,610.38,24.0," and contain an equal number of observations. In contrast, for classification, the weights are the previous probability times 1 minus the previous probability. So let's see how the equation for weights affects the quantiles in classification. And we'll do that with this simple data set."
2090,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,611.38,639.38,28.0," In this classification example, we are using drug dosage, to predict the probability that the drug is effective. The red dots are dosages in the training dataset that were not effective. And the green dots are dosages in the training dataset that were effective. These red and green axes correspond to the previously predicted probabilities that these dosages are effective,"
2091,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,640.38,659.38,19.0," and they start out at the initial prediction 0.5. After running the data down the first tree, most of the predictions improve. And as we add more trees, most of the predictions get better and better. Okay, cool."
2092,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,660.38,684.38,24.0," But what does this have to do with calculating the weights for the weighted quantile sketch? When using xg boost for classification, the weights for the weighted quantile sketch are calculated from the previously predicted probabilities. So let's calculate the weights for each observation. Note, I'm just showing one example of calculating weights."
2093,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,685.38,714.38,29.0," In practice, weights are calculated after building each tree. These predicted probabilities are very close to 0, indicating a high amount of confidence in classifying these dosages as ineffective. Since the previously predicted probability for these two points is 0.1, the weight is 0.1 times 1-0.1, which equals 0.09."
2094,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,715.38,745.38,30.0," So let's put 0.09 here, so we will remember it. These predicted probabilities are very close to 1, indicating we have high confidence in classifying these dosages as effective. Since the previously predicted probability for these two points is 0.9, the weight is 0.9 times 1-0.9, which equals 0.09."
2095,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,746.38,774.38,28.0," So let's put 0.09 here, so we will remember it. These two predicted probabilities are very close to 0.5, and that means that we are not very confident in how to classify these observations. Since the previously predicted probability for this point is 0.6, the weight is 0.24. So let's put 0.24 here."
2096,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,775.38,810.38,35.0," Lastly, since the previously predicted probability for this point is 0.4, the weight is 0.24. So let's put 0.24 here. Now we see that when the previously predicted probability is close to 0.5, meaning we don't have much confidence in the classification, the weights are relatively large. In contrast, when the previously predicted probability is very close to 0 or 1,"
2097,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,811.38,847.38,36.0," meaning we have a lot of confidence in the classification, the weights are relatively small. Now, if we split this data into equal quantiles, we would put a quantile here and here. But remember, we are treating each quantile as a unit, and lumping the last two observations together as a unit means they will end up in the same leaf together in the tree. And since the positive residual will cancel out the negative residual, it will be very difficult to improve the predicted probabilities."
2098,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,848.38,870.38,22.0," So, instead of using equal quantiles, XGBoost tries to make quantiles that have a similar sum of weights. In order to divide the observations into quantiles where the sum of the weights are similar, we divide them into these quantiles. The sum of the weights in the first quantile is 0.18."
2099,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,872.38,902.38,30.0," The sum of the weights in the second quantile is 0.18. The third quantile only has one observation, and its weight is 0.24. And the last quantile only has a single observation, and its weight is 0.24. By dividing the observations into quantiles where the sum of the weights are similar, we split the two observations with low confidence predictions into separate bends."
2100,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,903.38,935.38,32.0," In other words, the advantage of using the weighted quantile sketch is that we get smaller quantiles when we need them. Bam! So, when we have a huge training dataset, XGBoost uses an approximate greedy algorithm. And that means using parallel learning to split up the datasets so that multiple computers can work on it at the same time. And a weighted quantile sketch merges the data into an approximate histogram."
2101,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,935.38,972.38,37.0," And the histogram is divided into weighted quantiles that put observations with low confidence predictions into quantiles with fewer observations. Note, before we move on, I want to mention that XGBoost only uses the approximate greedy algorithm, parallel learning, and the weighted quantile sketch when the training dataset is huge. When the training datasets are small, like the ones in my examples, XGBoost just uses a normal, everyday, greedy algorithm. Bam!"
2102,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,973.38,1003.38,30.0," Now let's talk about sparsity aware split finding. So let's return to the example where we were using dosage to predict drug effectiveness. Only this time, we have a few missing values. Even though we have missing values, we can calculate the residuals, the differences between the observed drug effectiveness and the predicted drug effectiveness, using the initial prediction 0.5."
2103,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1004.38,1036.38,32.000000000000114," And just like we normally do when we build XGBoost trees, we can put all of the residuals into a single leaf. Now we need to determine if splitting the residuals into two leaves will do a better job clustering them. So, just like we always do for continuous data, we need to sort the dosages from low to high. Unfortunately, it's unclear how to sort the dosages with missing values. So, what we'll do is we'll split the data into two tables."
2104,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1037.38,1082.38,45.0," One table will contain all of the observations with dosage values, and another table will contain all of the observations without dosage values. Now, focusing on the table that has dosage values for every observation, we sort rows by dosage from low to high. Now we test the average of the first two dosages, 7.5, as a candidate threshold. Note, if this was a large data set, we would be using the first quantile here. In this case, we test the threshold by putting the residual for the one observation that has a dosage less than 7.5 in the leaf on the left."
2105,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1083.38,1127.38,44.0," And put the remaining residuals, which all have dosages greater than 7.5 into the leaf on the right. Now that we have all of the residuals with known dosages in the tree, we calculate two separate gain values. The first gain value, which we will call gain left, is calculated by putting all of the residuals with missing dosage values into the leaf on the left. Now we do the same thing using the average of the next two dosages, 15.5, as a candidate threshold. We put the residuals with dosages less than 15.5 in the leaf on the left, and the residuals with dosages on the left."
2106,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1128.38,1168.38,40.0," And the residuals with dosages on the left, and the residuals with dosages on the left, and the residuals with dosages on the left. And the leaf on the left, and the residuals with dosages greater than or equal to 15.5 in the leaf on the right. And then we put all of the residuals with missing dosages into the leaf on the left and calculate gain left. Then we put all of the residuals with missing dosage values into the leaf on the right and calculate gain right. Lastly, we do the same thing using the average of the last two dosages, 23, as a candidate threshold."
2107,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1169.38,1210.38,41.0," We calculate gain left, and we calculate gain right. In the end, we choose the threshold that gave us the largest value for gain overall. In this case, that meant picking gain left when the threshold was dosage less than 15.5. Note, this path, going to the left leaf when dosages less than 15.5, will be the default path for all future observations that are missing dosage values. For example, if this was the XG Boost model, and we got a new observation without a value for dosage,"
2108,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1210.38,1241.38,31.0," but we still needed to predict drug effectiveness, then we would assume that this observation goes to the leaf on the left. Thus, sparsity aware split-finding tells us how to build trees with missing data, and how to deal with new observations when there is missing data. Double-bowm. Now we need to talk about cash-aware access. This is where XG Boost starts to get super-nitty-gritty."
2109,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1243.38,1279.38,36.0," The basic idea is that inside each computer we have, a CPU, a central processing unit, and that CPU has a small amount of cash memory. The CPU can use this memory faster than any other memory in the computer. The CPUs also attach to a large amount of main memory, while the main memory is larger than the cash, it takes longer to use. Lastly, the CPU is also attached to the hard drive. The hard drive can store the most stuff, but is the slowest of all memory options."
2110,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1281.38,1310.38,29.0," If you want your program to run really fast, the goal is to maximize what you can do with the cash memory. So XG Boost puts the gradients and hashons in the cash so that it can rapidly calculate similarity scores and output values. Bam, that was pretty simple. Lastly, we need to talk about blocks for out-of-core computation. Going back to the super-simple computer schematic,"
2111,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1311.38,1343.38,32.0," When the data set is too large for the cash and main memory, then, at least some of it must be stored on the hard drive. Because reading and writing data to the hard drive is super slow, XG Boost tries to minimize these actions by compressing the data. Even though the CPU must spend some time decompressing the data that comes from the hard drive, it can do this faster than the hard drive can read the data. In other words, by spending a little bit of CPU time uncompressing the data,"
2112,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1344.38,1370.38,26.0," we can avoid spending a lot of time accessing the hard drive. Also, when there is more than one hard drive available for storage, XG Boost uses a database technique called Sharding to speed up disk access. For example, if this is the data set and it is very large, then XG Boost splits the data so that each drive gets a unique set of records."
2113,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1371.38,1409.38,38.0," Then, when the CPU needs data, both drives can be reading data at the same time. Bam. Thus, cash-aware access and blocks for out-of-core computation are optimizations that take the computer hardware into account. Lastly, I need to mention that XG Boost can also speed things up by allowing you to build each tree with only a random subset of the data. And XG Boost can speed up building trees by only looking at a random subset of features when deciding how to split the data."
2114,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1410.38,1434.38,24.0," Bam. In summary, XG Boost is fast for a lot of reasons. Some of these reasons, like cash-aware access, are not even vaguely related to statistics. And that makes XG Boost something more than just an applied statistical technique. And that means machine learning is more than just applied statistics."
2115,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1435.38,1453.38,18.0," Triple-BAM. Whoay! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign."
2116,XGBoost Part 4 (of 4): Crazy Cool Optimizations,https://www.youtube.com/watch?v=oRrKeUCEbq8,oRrKeUCEbq8,1454.38,1466.38,12.0," Becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below. Until next time, quest on."
2117,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,0.0,23.0,23.0," SG-Coost is a stream. But so is this webinar? It's totally extreme. Yes, Dan Quest. Hey, I'm Josh Starmer, and welcome to the stat quest webinar on XG Boost and Python from"
2118,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,23.0,42.7,19.700000000000003," Start to Finish. This is the Jupiter notebook that we're going to go through today. We're going to use XG Boost to build a collection of boosted trees, one of which is illustrated below. So this guy right here, and use continuous and categorical data from the IBM base samples"
2119,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,42.7,64.0,21.3," website to predict whether or not a customer will stop using a company's service. In business, Lingo, this is called customer churn. You can download the telco churn dataset or use the file provided with the Jupiter notebook. If you want to learn more about the telco churn dataset, you can click on this link in the Jupiter notebook."
2120,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,64.0,85.0,21.0," It's live. Or if you just want to learn more about the base samples, there's all kinds of data sets here that you could use for other experiments in machine learning. So it's a great resource for just testing out these models in general. Alright, XG Boost is an exceptionally useful machine learning method."
2121,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,85.0,113.0,28.0," When you don't want to sacrifice the ability to correctly classify observations, but you still want a model that's fairly easy to understand and interpret. In this lesson, you will learn about importing data from a file, missing data, including dealing with missing data XG Boost style, which is relatively unique. And then we're going to format data for XG Boost, including using one hot encoding, and actually"
2122,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,114.0,138.0,24.0," because of the way XG Boost uses handles missing data, we're going to have a many stack quest in the middle of this webinar to explain the specifics of one hot encoding and how it relates to how missing data is handled. After that, we're going to build a preliminary XG Boost model. And then we're going to optimize parameters with cross validation and grid search."
2123,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,138.0,165.0,27.0," And then lastly, we're going to draw validate and build, obviously, and then we're going to interpret and evaluate the optimized XG Boost model. Note, this tutorial already assumes that you know the basics of Python and are familiar with a theory behind XG Boost, cross validation, and confusion matrices. If not, check out the stack quest by clicking on the links for each topic."
2124,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,166.0,191.0,25.0," Also, note, I strongly encourage you to play around with the code. Playing with the code is the best way to learn from it. So, the very first thing we do is load in a bunch of Python modules. Python itself just gives us a basic programming language. These modules give us extra functionality to import the data, clean it up, and format it, and then build"
2125,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,191.0,216.0,25.0," evaluate and draw the XG Boost model. Note, you'll need Python 3, and at least these versions of the versions of the following modules. Pandas, NumPy, Scikit, learning, XG Boost. I've got instructions on how to install all this stuff right here, and also instructions on if you want to actually draw that beautiful tree, I've got instructions on how to install graph"
2126,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,216.0,243.0,27.0," this as well to make it all work out. Alright, so this is where we're going to load in the data. Since this is a jupy or notebook, we can run the Python code by either clicking on the play button, or we can select run selected cells, or we can select this keyboard combination to run the cells. Note, I'm using a Macintosh to that's my keyboard combination on your computer."
2127,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,243.0,274.0,31.0," The keyboard combination may be different. So, I'm going to run this, and we get a number over here that tells us that we've run the code. So, that's great. If, by the way, if Python is still cranking away, if you're doing something a little more complicated than loading in some modules, you'll see a star over here while Python is cranking away, but we just get a number, so that's good."
2128,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,275.0,302.0,27.0," Okay, now we're ready to import the data. We're going to load in a data set from the IBM base samples. Specifically, we're going to use the Telco-Churn data set, which allows us to predict if someone's going to stop using Telco's services, or not, using a variety of continuous and categorical data types. Note, when pandas reads in data, it returns a data frame, which is a lot like a spreadsheet."
2129,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,302.0,331.0,29.0," Data are organized in rows and columns, and each row can contain a mixture of text and numbers. The standard variable name for a data frame is the initials DF, and that's what we're going to use here. So, what we're doing is we're creating a new data frame that we're calling DF, and we're setting it to the output of this pandas function, read CSV. So, this is a CSV file that we're loading in. So, let's do that, bam."
2130,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,331.0,355.0,24.0," Now that we've loaded in the data into a data frame called DF, we're going to look at the first five rows using the head function. So, this is our data frame DF, and there's an associated function called head that we're going to run. And what it does is it prints out the first five rows. So, it says five rows by 33 columns. And when we scroll over, we see all these columns."
2131,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,355.0,384.0,29.0," Not all of them have been printed to the screen. I don't know if you can see there's a dot dot dot between gender and contract. And that just means that even though there's 33 columns, we didn't print all of them to the screen. Note, the last four columns, turn reason, CLTV, turn score. These are, what are these?"
2132,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,384.0,424.0,40.0," These are sort of exit interview data that were collected from people that left telco. And only people that left telco provided answers here. So, we don't want to use this data in our prediction, because generally speaking, someone's not going to do the exit interview before they leave the company. And so, since these things will give us perfect prediction, a perfect predictive abilities, we want to remove them from the data set. So, we do that using the drop function."
2133,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,424.0,457.0,33.0," So, we've got data frame, this is our data frame, and we're going to use the drop function. And then we list the columns that we want to drop. We set access equals to one to specify that we're dropping columns instead of rows. And we set in place to true, which means we want to modify DF, this data frame called DF directly. We're not making a copy of the data frame and only in the copy is that containing these, or without these columns."
2134,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,457.0,489.0,32.0," It has these columns drop. And then after what we're after that, what we're going to do is we're going to print the first five rows. And again, just to verify that we did everything correctly. So, let's do that. Okay, so when we scroll over to the right, we see that turn, reason, and CLTV, those things are gone. Some of the other columns in this data set only contain a single value, and will not be useful for classification."
2135,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,489.0,518.0,29.0," For example, this column count, we just see a bunch of ones. We can verify that the only value in this column is one by with our data frame. We specify we want to look at the count column, and then we call the unique function to print out all the unique values in that column. And when we run that, we see that the only unique value is one. So this column count only contains one, and that makes it useless for making predictions."
2136,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,518.0,543.0,25.0," Likewise, if we look at the unique values in country, just looking at it, we see United States a bunch of times, and if we print out the unique values in the country column, we see that the only value is United States. Similarly, in the state column, we see a bunch of entries for California. Let's print out all the unique values in state. Again, we just see one value, California."
2137,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,543.0,571.0,28.0," So that means we can omit count, country, and state from the analysis, because they're not going to help with predictions. In contrast, city, a column called city, contains a bunch of different city names. So let's look at that. So yeah, we see Los Angeles, Beverly Hills, Huntington Park, Standish, all kinds of stuff. So we're going to leave city in because city may help us with making predictions."
2138,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,571.0,600.0,29.0," However, we're also going to remove customer ID, and we're going to remove this. We're going to remove customer ID because there's a different value for every single person. So that's not going to be very helpful for predictions. And also, there's a column called last long, which contains both latitude and longitude of the resident of that was a customer. So we also have separate columns for latitude and longitude."
2139,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,600.0,619.0,19.0," So we don't need the column that emerges those two things. So we're going to drop those columns as well, just like we did before. So we've got our data frame. Drop, so we're using the drop function, then we pass in array of the columns that we want to drop. Specify that we're dropping columns instead of rows."
2140,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,619.0,645.0,26.0," And again, just like before, we're using in-place set to true, so that we modify the data frame DF itself. And as always, we're going to print out the first five rows to make sure we did everything correctly. Hurray, looks like we did it just right. Okay, now we're down to just 24 columns. Note, although it's okay to have white space in city names."
2141,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,645.0,672.0,27.0," So you see we've got, we've got loss, angeles, and there's a blank space between loss and angeles. And you see that up here when we print it out the unique values for each city name. We've got a blank space between Beverly and Hills and a blank space between Huntington and Park. Those blank spaces are perfectly okay for XG boost. XG boost doesn't care, partially because we're going to use one hot encoding for this."
2142,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,672.0,697.0,25.0," However, we can't have any white space or blank space if we want to draw the actual tree like we did all the way. Up here, we want to draw this tree later on at the very end. We need to remove the white spaces. So we're going to do that. Since we know there's a lot of white spaces in the city column, we specify, we've got our data frame,"
2143,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,697.0,715.0,18.0, and we specify that we were interested in the city column. And then we can use the replace function. And this is just a search and replace function like you'd find anywhere else. We're searching for blank spaces and we're going to replace it. Each blank space with an underscore.
2144,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,715.0,742.0,27.0," We set red X to true that's red X is short for regular expression. And if you're not familiar with the term red X or regular expressions, don't worry. Just think of it as advanced features for a search and replace function. And again, just like we've been doing before, we're going to make these modifications in place. So we're going to modify DF directly and then print out the first five rows to make sure we did it right."
2145,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,742.0,764.0,22.0," There we go. And we see an underscore between loss and Angela's. And we can also print out the first 10 city names. The first 10 unique city names and verify that we've got underscores between Beverly and Hills, between Huntington and Parked, between Marina and Dell and Dell and Ray."
2146,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,764.0,781.0,17.0," So we've eliminated those white spaces. Also, we need to eliminate the white spaces in the column names. So we're going to replace those with underscores as well. We do this slightly differently. So we've got our data frame, DF, and we specified that we're interested in the columns,"
2147,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,781.0,796.0,15.0," not in a specific column, but the column names. And what we do is we convert those column names to strings, and then we call the replace function. And again, this is just a search and replace. We're searching for white space and replacing with underscores."
2148,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,796.0,817.0,21.0," And just like we've done before, we're going to print out the first five rows. Bam. So now we see an underscore between zip and code, senior in citizen, tenure and months, etc. Ray, we've removed all of the data that will not help us to create an effective XG boost model."
2149,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,817.0,851.0,34.0," And we've reformatted the column names and city names so that we can now draw a tree. Okay, so now we're ready to identify and deal with missing data. Unfortunately, the biggest part of any data analysis project is making sure that the data are correctly formatted and fixing it when it's not. The first part of this process is identifying missing data. missing data is simply a blank space or a serget value like N.A. that indicates that we failed to collect one or more features."
2150,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,851.0,880.0,29.0," We failed to collect data for one or more features. For example, if we forgot to ask someone's age or forgot to write it down, then we would have a blank space in this data set next to that person's age. One thing that's relatively unique about XG boost is that it has a default behavior for missing data. It knows how to handle missing data. It's expecting it. So all we have to do is identify the missing values and make sure they are set to zero."
2151,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,880.0,926.0,46.0," Now, in the webinar that I've given a couple of times now, this is confused a lot of people because what happens if there is a zero already in your data set. I'm going to have a little mini stack quest about halfway through that shows how we can have zeroes that code for things as well as using zero to code for missing data and it actually works out. And even in situations, I'm just going to show you one scenario. And even if there's some scenario where it does not work out the way I'm demonstrating, I will through this out as well. The author of XG boost has said that even when you code something with zero and you use zero to mean missing data,"
2152,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,926.0,956.0,30.0," XG boost still does a great job. It doesn't, so for some reason that doesn't really interfere with how well it performs. So the first thing we're going to do is we're going to see what sort of data is in each column. This is what I always do when I'm looking for missing data. I print out the data types for each column because that can tell us if something is messed up or not. We see that a lot of the columns are object and that's okay because above here when we when we did head,"
2153,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,956.0,988.0,32.0," we saw that senior citizen has a bunch of nodes and probably has some yeses in there. Partner has yeses and nose, dependence has yeses and nose. And so it makes sense that a lot of these columns are object because they have text responses like yes and no. However, we should always verify that we're getting what we expect in each column. So for example, we'll look at the phone service column and we'll use our handy unique function to check to see what we're getting."
2154,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,988.0,1027.0,39.0," And what we get is yes and no and that's perfect because that means there's not question marks in this column. There's not an NA placeholder for a missing data. So we can verify that this column only contains yeses and nose and that's great. Okay, now in practice we should do that for every single column verify that it has the type of data that we're expecting and it only has the responses that we're expecting. And trust me, I did this, but right now we will focus on one specific column that looks like it could be a problem and that's total charges."
2155,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1027.0,1057.0,30.0," So if we looked at this output over here, we see that total charges looks like a bunch of numbers. However, if we look at the data type for total charges, we see that it's an object. And that we usually get that object when we get a mixture of numbers and characters. So one thing we can do is just print out the unique values and total charges and kind of see what we see. So let's do that, bam."
2156,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1057.0,1096.0,39.0," And when we do that, we see that there are too many values to print. We've got this dot dot dot, right in the middle. But what we see looks like a bunch of numbers. However, if we try to convert the column to numeric data or numeric values, and I'm trying that right here, when we run this code, we get an error. So I'm going to comment it out just in case you want to run all the code all at once. And let's look at this error. The nice thing about this error and the reason why I wanted to show you this error is it actually tells you what's wrong with the data."
2157,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1096.0,1131.0,35.0," It says unable to parse string quote nothing or blank space and quote. And what does that tell us that tells us that there are blank spaces in this in this column. In total charges. And so so we need to deal with those. Okay, so now we're ready to deal with missing data, XG boost style. Like I've said before, one thing that's relatively unique about XG boost is that it determines default behavior for missing data."
2158,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1131.0,1159.0,28.0," So all we have to do is identify missing values and make sure they are set to zero. However, before we do that, let's see how many rows are missing data. And if it's a lot, then we might have a problem on our hands that is bigger than what XG boost can do on its own. And if it's not that many, we'll just set them to zero. So we do this by searching for, we're using the location function."
2159,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1159.0,1184.0,25.0," So we got our data frame and we say, tell give me the rows where this is true. So the value in the total charges column is equal to zero. And then we're wrapping all of that in the line or length function. And that counts the number of rows that have blank spaces in the total charges column. And we see there's only 11 rows that have missing values."
2160,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1184.0,1205.0,21.0," So since it's only 11, we can look at them. So we're going to print them out. Bam. So we see, if we go over here, we see that in the total charges column, we have no values. We also see that in the tenure months column, we got zero for everybody."
2161,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1205.0,1225.0,20.0," And that means that the reason why these people have total charges equal to blank is that they have not been charged for anything yet. They just subscribe. And so they're on a plan. They've got, we've got an expected amount of money we're going to be getting from them, but they have not paid us anything yet."
2162,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1225.0,1258.0,33.0," And since it's just a handful of people, we're going to set these total charges to, or just going to set total charges to zero. And the way we do that is a lot like what we did before. And then, actually, this time, we're specifying, instead of having a low, return the entire row, which is what it was doing before,"
2163,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1258.0,1282.0,24.0," we're specifying that we just want the total charges. And again, we're interested in just the, the, the locations where total charges equals blank space. And we're setting that value and the total charges column to zero. So let's do that. We can verify that we modified total charges correctly by looking at everyone had 10-year months set to zero."
2164,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1282.0,1310.0,28.0," Note, I'm not looking at total charges equal to zero, because there could be other people that have not paid a dime. Even though they've been on board for a couple of months, they might have zeros there already. And so, but I'm pretty certain that everyone who had 10-year months equal to zero had a blank space, because they just signed up and they hadn't had a, they hadn't paid a bill yet. So we do that, and we see that we've got the 10-year months equal zero,"
2165,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1310.0,1328.0,18.0," and when we scroll over to the right, we've got total charges equal to zero. So that worked. Bam! We have verified that our data-frame DF contains zeros instead of spaces for missing values. Note, total charges still has the object data type,"
2166,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1328.0,1349.0,21.0," and that's no good, because XGB boost only allows ant, float, and Boolean data types. So we're going to fix this by converting that column with two new Merck. There are multiple ways to convert columns from one type to another. This is just one of them. So I'm using two new Merck, and it's a Pandas function."
2167,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1349.0,1368.0,19.0," And I'm specifying that I want to convert the total charges column, and I'm saving it in the original total charges slot. And when I'm done, I'm printing out the data types to verify that we've done it correctly. So let's do that, bam! And we go down here, and we see total charges is now float 64."
2168,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1368.0,1387.0,19.0," So hooray! Now that we've dealt with the missing data, what we're going to do is we're just going to replace all of the other white spaces in all of the columns with underscores. There could be other columns that have white spaces, and we're just going to do this all at once. So we're going to do it just like we did before."
2169,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1387.0,1404.0,17.0," We've got our data frame, and we're using the Replace function. However, this time we're not specifying a specific column. We're just going to do this data frame wide. We're replacing blank space with underscores, and then we're going to print out the first five rows to verify that we did it correctly."
2170,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1404.0,1424.0,20.0," So there we go, bam! And if we scroll over to the right, we see that one of the things that we fixed was under the payment method column, instead of blank spaces between mailed and check, now we have an underscore, an electronic and check we've got underscores. So bam!"
2171,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1424.0,1450.0,26.0," And remember, just to clarify, the only reason why we're replacing all these blank spaces is so that we can print out a nice, pretty looking tree. XJ boot XG boot itself doesn't really care. Partially because we're going to one hot and coat these things later anyways. Okay, so now that we've dealt with all those issues, we can now start formatting the data"
2172,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1450.0,1469.0,19.0, for an XG boot model. And the first step is to break it into two parts. We want to separate the columns that we will use to make classifications from the column that we want to predict. So we're going to use the conventional notation of capital X to represent the columns of data
2173,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1469.0,1489.0,20.0," that we will use to make classifications. And we're going to use lowercase y to represent the thing that we want to predict. In this case, we want to predict churn value. Let's look at this. This is one for people that left and it's zero for people that did not leave."
2174,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1490.0,1513.0,23.0," And so what we're doing is we're creating uppercase X here. And we've got our data frame and we're just like before we're calling the drop function and we're specifying that we want to drop churn value. However, we're not doing in place like we did before and we're saving the results in a new variable. And we're going to print out the first five rows of that new variable."
2175,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1513.0,1524.0,11.0," So there we go. And if you scroll over, we see that churn value is missing. We've got everything else, which is great. That's exactly what we wanted. Now we're going to make this lowercase y variable."
2176,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1524.0,1541.0,17.0," And it's going to be just the column in data frame called churn underscore value. And then we're going to print out the first five rows to verify that it looks the way it should. And there it is. Bam. Now that we've created capital X,"
2177,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1541.0,1562.0,21.0," which has the data that we want to use to make predictions and we've made lowercase y, which has the data that we want to predict, we're ready to continue formatting X or capital X so that it is suitable for making a model with XG boost. So that brings us to one hot encoding. Okay."
2178,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1562.0,1585.0,23.0," Now that we've split the data frame up into two pieces, we need to take a closer look at the variables within capital X. So the list below, which I got from the IBM website for this data set, tells us whether or not each column should be a float or categorical. So we see city is a category, long to its float."
2179,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1585.0,1607.0,22.0," Gender is a category, senior citizen is a category. You've got a bunch of categories, tenure months is a float. So we've got lots of columns. Now just to review, we're going to look at the data types in X to remember how Python is seeing the data right now. So we see that latitude, long to suit monthly charges and total charges."
2180,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1607.0,1627.0,20.0," Those are all float 64, which is great. That's exactly what we want. However, all of the other columns that are object, those need to one, we need to inspect those to make sure each one only contains a reasonable value. And then we also need to modify them with one hot encoding."
2181,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1627.0,1672.0,45.0," One hot encoding is a trick for taking data that is categorical and splitting it up into a format that X to boost and a lot of other algorithms can use. So the problem is that X to boost and a lot of other machine learning algorithms, they natively support continuous data like monthly charges and total charges. But they do not natively support categorical data like phone services, which has two different categories. So if we want to use categorical data with our model, we have to convert it with one hot encoding to kind of get around this limitation."
2182,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1672.0,1701.0,29.0," Okay, so at this point, you may be wondering, what's wrong with treating categorical data like continuous data? Can't we just convert the categories to numbers and be done with it? And to answer this question, we're going to look at an example and I've chosen payment method to be that example. We see it as a bunch of options. We've got mail check, we've got electronic check, we've got bank transfer, and we've got credit card."
2183,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1702.0,1752.0,50.0," If we converted those categories to numbers, one, two, three and four, and treated them like continuous data, then we would assume that four, which means credit card, is more similar to three, which means a bank transfer. Then it is to one or two, which are other forms of payment. That means the X to boost, X to boost tree would be more likely to cluster the people with fours and threes together than fours with ones. In contrast, if we treat these payment methods like categorical data, then we will treat each one as a separate category, and that is no more or less similar to any other category. Thus, the likelihood of clustering people who pay by mailed check with people who pay by electronic check is the same as clustering with mailed check and credit card."
2184,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1752.0,1785.0,33.0," So this approach seems more reasonable to me. Note, there are many different ways to do one hot encoding and Python. There are two main popular ways, and I describe the pros and cons of these two approaches in the Jupyter notebook in this paragraph. However, for the purpose of this webinar, we're going to use Git dummies, because I feel like it's the best way to teach what one hot encoding does. So what we're going to do is we're going to use this Pandus function, so Pd is sure for Pandus."
2185,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1785.0,1815.0,30.0," We're going to use this Pandus function called Git dummies. And we pass in the data frame that we're interested in processing, and the columns that we want to one hot encoding. Now, I'm not saving the results. I just want to see what happens. So I'm just going to print out the first five rows to show you how this column payment method is one hot encoding. So we run this code, and we see that all the columns that we did not modify are on the left side of the data frame."
2186,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1815.0,1850.0,35.0," But if we scroll to the right, we see payment method, payment method bank transfers. So we've got that column. We've got another column for payment method credit card, another column for payment method electronic check, and another column for payment method mail check. And in each column we've got a 1 if in the bank transfer column we've got a 1 if they use bank transfer, and a 0 for any other option. For credit card we've got a 1 in that column and a 0 for any other option for electronic check."
2187,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1850.0,1883.0,33.0," We've got a 1 in that column and 0 for any other option. And lastly, for mail check we've got a 1 in that column and 0 for any others. Note if you're familiar with linear regression or logistic regression, if you're not familiar with linear regression or logistic regression don't worry about what I'm about to say. But if you are familiar with those two things, one hot encoding is different from the way you would encode it for the same data. The one hot encoding gives us a result that is different from what we would do for linear and logistic regression."
2188,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1883.0,1938.0,55.0," So just keep in mind that one hot encoding is not for linear and logistic regression, but it works great for trees. So now that we know what get dummies does and we know that it works, we're going to use it on all of the categorical columns and we're going to save the result. Note in a real situation and not in a tutorial like this, we would go through each individual column and make sure it only contains reasonable data. However, since this is just a tutorial and I've already done all that work, we're just going to skip to the next step, which is to run this Pandus function, PD, we're going to run the Pandus function called Gid dummies, we're going to pass in our data frame. And all of the columns that we want to convert into categorical columns."
2189,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1939.0,1967.0,28.0," And we're going to save that in a new data frame called X underscore encoded and then when we're done, we're going to print out the first five rows. So let's do that. Bam. So now we see that we've encoded a bunch of columns. There's this dot dot dot stills because there's too many columns to print and if you look down here, obviously we've got five rows because we use the head function, which only prints out five rows."
2190,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1967.0,1986.0,19.0," But we now have 1,178 columns. Dang. A lot of those are because we've got a different column for each city name. So we've see all these, all these city names that just kind of bleed into the dot dot dot. We've got a lot of different city names and we've got a column for each one."
2191,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,1986.0,2027.0,41.0," Now, one last thing before we build an XG Boost model, we're going to verify that Y only contains 1s and 0s with a unique function. So we've got Y dot unique and we run that and it only has 1 and 0 and that's a double bam. So we finally finished formatting the data for making an XG Boost model. However, before we get before we do that, I want to do a mini stack quest to show how one hot encoding works, especially when we're coding missing values with zero. So, imagine the favorite color was a column or feature or variable in our dataset."
2192,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2027.0,2072.0,45.0," Two people loved the color blue, two people loved the color green, and two people had missing data. So, just like we did in the Jupiter notebook, we replaced the missing data with zeros. Now we convert favorite color with one hot encoding, just like we just did. There are ones in the blue column for the two people that liked blue and zeros in the green column because those people did not like green. Likewise, there are ones in the green column for the two people that liked green and zeros in the blue column because those people did not like blue."
2193,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2072.0,2101.0,29.0," Lastly, both blue and green columns get zeros for the people with missing data. Let's move this table to the left side. Now the question is, should the people with missing data be clustered with the people that like blue? Or should they be clustered with the people that like green? XG Boost answers these questions by comparing these two different ways to split the data."
2194,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2101.0,2132.0,31.0," On the left side, we reliably are splitting people who like blue from everyone else. In that means we are clustering the people who like green with the people with missing data. On the right side, we are splitting the people who like green from everyone else. In that means we are clustering the people who like blue with the people who have missing data. XG Boost then chooses the split that gives the best value for gain."
2195,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2132.0,2166.0,34.0," Okay, I get how XG Boost deals with missing data. But doesn't keep track of all these zeros take up a lot of memory? Because XG Boost uses sparse matrices, it only keeps track of the ones, and it doesn't allocate memory for the zeros. And that means that this branch is really just checking to see if memory is allocated for blue. If memory is allocated for blue, then we go to the left."
2196,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2166.0,2196.0,30.0," And if memory is not allocated for blue, then we go to the right. Likewise, this branch is only asking if memory is allocated for green. If memory is allocated for green, then we go to the left. And if memory is not allocated for green, then we go to the right. This is how XG Boost deals with missing data, and is memory efficient at the same time."
2197,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2196.0,2217.0,21.0," Bam! Alright, now let's return to the Jupiter notebook. And build our preliminary XG Boost model. Okay, so I know we just said that we were going to build the XG Boost model. But the first thing we need to do is we need to split the data into training and testing data sets."
2198,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2217.0,2252.0,35.0," However, let's first observe that this data is imbalanced by dividing the number of people who left the company where Y equals one by the total number of people in the data set. So since this column Y, lowercase Y, only contains zeros and ones, and it only contains ones for people that left the company, we can add up all the values in this column to get the number of people that left. And if we divide it by the length of that column, we'll get the percentage of people that left. And we see that only 27% of the people in the data set left the company."
2199,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2253.0,2294.0,41.0," Because of this, when we split the data into training and testing data sets, we will split using stratification in order to maintain the same percentage of people who left the company in both the training set and the testing data set. So we're using a function called train test split, and we're passing in X and coded, lowercase Y, we're setting the random state to 42 so that hopefully, hope against hope, you will get the same results that I got. And we're setting stratify to Y for yes. And this will return four variables and we're four four data sets. We're storing them in X under score train, X under score test, Y under score train, and Y under score test. So let's run this."
2200,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2294.0,2313.0,19.0, Bam. Now let's verify that stratify worked as expected. Now we're just doing the same math we did before. Only this time we're doing it all on the training data set. So we see that 27% of the training data set are contains people that left. And let's look at the testing data set.
2201,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2313.0,2332.0,19.0," And we see that 27% of the people in the testing data set left. So bam, stratify worked as expected. So now what we're going to do is we're ready. We've got our training set, we've got our testing data set. We're ready to build our XG boost model."
2202,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2332.0,2355.0,23.0," And the way we do that is we use XGB, which is the XG boost module or library. And we use a function called XGB classifier. And we specify the objective is binary logistic. And that's for classification. Because XG boost kind of uses a logistic regression approach to evaluate the value of the"
2203,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2355.0,2372.0,17.0," value to evaluating how good it is at classifying the observation. We're setting missing to none. The default value for missing is none. So I don't actually need to set this. And in the webinar, it's just confused a few people."
2204,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2372.0,2403.0,31.0," What we're supposed to be the purpose of this parameter is to, or this argument, is to tell XG boost what character we're using to represent missing values. The default value when you have missing equals none, the default value is zero or in p or as a numpy, not a number. But it uses zeros in that sparse matrix, just like we saw in the in the mini stat quest. It uses zeros to represent missing data."
2205,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2403.0,2423.0,20.0," So it doesn't have to allocate memory in those fields for people that have missing data. So that's the default behavior. And that's what we're specifying. If we used question marks to represent missing data, we could specify that here. And again, we're setting the seed to 42 in order to hopefully give you the same results as me."
2206,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2423.0,2447.0,24.0, We store this in a new variable. So this is just basically create a shell within which we are going to create a forest of extreme gradient boosted trees. And we're storing that in this variable. And we create those trees by then running fit. We pass in the training data.
2207,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2447.0,2470.0,23.0," We say, be verbose, sort of telling you what you're doing while you're doing it. And one thing we're doing that's kind of special is we're doing early stopping. So what we're doing is we're building trees. And at some point, the prediction will not improve. And then what what what actually boost is going to do is then it's going to build 10 more trees."
2208,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2470.0,2499.0,29.0," And if those 10, all 10 of those trees, none of them can improve on the predictions. Then it will stop. And we're using the AUC as a way to evaluate how well the predictions are being made. And we pass in the testing data set because with the training on the, it's training the trees. Training the trees on the testing data set on on on the train, excuse me."
2209,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2499.0,2528.0,29.0," It's training on the training data set, but it's evaluating how many trees to build using the testing data set. And this is what we would normally do anyways by hand using cross validation and a variety of other approaches. But actually boost will do it for us. Okay, so we'll see there's a star by the code meaning it's running. And you can see down here, we're just sort of printing out the results of each tree."
2210,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2528.0,2557.0,29.0," And we're going and going and going. And here we finally stop after building 55 trees. However, that meant that the previous 10 trees did not improve the classification. And so it says stopping the best iteration was actually the after building 45 trees. So, so we've only created 45 trees for our, our model."
2211,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2557.0,2580.0,23.0," Okay, so we've built an XG Boost model for classification. Now let's see how well it performs on the testing data set by running the testing data. Data set down the model and drawing a confusion matrix. We do this with the function called plot confusion matrix. We've passed in the model, the extreme gradient boost boosted trees."
2212,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2580.0,2608.0,28.0," And we passed in the testing data sets. And then the last two rows of stuff just make the, the, the, the, the confusion matrix. It just makes the confusion matrix look pretty. So in the confusion matrix, we see that the top row represents people that did not leave the company. And there are 100, excuse me, 1000, 294 people in this row."
2213,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2608.0,2632.0,24.0," And of those, 1000, 178 were correctly classified. So that's awesome. That's 95, 91%. However, the second row, this is the, these are the people that left the company. And there are 467 people in this row. And we see in this bottom right hand corner that only 239 or 51% were correctly classified."
2214,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2632.0,2654.0,22.0," So XG Boost was not awesome. Part of the problem is that the data is imbalanced. And we saw that earlier and we see that in the confusion matrix right now. Because leaving people, because people leaving the company costs the company a lot of money. What I'm going to do is I'm going to try to optimize the XG Boost model."
2215,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2654.0,2693.0,39.0," So it does a better job predicting people that have left or are leaving the company for a competitor for some other reason. And the good news is XG Boost has a parameter called scale, pause, weight that helps deal with imbalance the data. And basically what it does is it adds a penalty for incorrectly classifying the minority class. In this case, that's the people that left the company. And we want to increase that penalty so that the trees will try harder to correctly classify them."
2216,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2693.0,2720.0,27.0," And so we're going to try to improve our predictions using cross-validations to optimize these parameters. Okay, so XG Boost has a lot of hyperparameters. These are parameters that we have to manually configure and are not determined by XG Boost itself. And these include max depth, max depth is how many how deep the tree can go. So if it's just a stump, then we just go down one level."
2217,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2720.0,2747.0,27.0," If we allow more branching, then we can go down two, three, four, five, or six more levels. We can optimize the learning rate, which is eta. And if you've watched the the stat quest videos on XG Boost, you know all about that. You can also optimize gamma, the the parameter that encourages permanent pruning, and the regularization parameter for lambda. And that's for ridge regression."
2218,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2747.0,2776.0,29.0," So let's try to find the optimal values for these hyperparameters and hopes that we can improve the accuracy with the testing data set. Since we have a lot of parameters, we're going to do that using grid search CV or cross-validation. However, because this is just a tutorial, I've commented all this out. This takes a while to do it runs. It takes about 10 minutes to run."
2219,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2776.0,2819.0,43.0," However, there's a few notes in here that I want you to be aware of. In the manual for XG Boost for XG Boost, it says if you have imbalance data, then you should use the AUC to evaluate the performance of your. Of your, you know, of the fit. And that we should also try to optimize this scale pause weight parameter. And what I've done, and I'm, and I've commented this all this out, but I've done the optimization in two separate rounds, because optimizing everything all at once just took too long."
2220,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2819.0,2856.0,37.0," So what I did is for max depth, I gave it three different values three, four and five. So we can have a tree that's got three levels or four levels or five levels. We've got three different values for max learning rate that we can try out three different values for gamma, three different values for the regularization parameter, and three different values for scale pause weight. And then I optimized. And oh, and here's something I need to point out, in order to speed up the cross-validation."
2221,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2856.0,2896.0,40.0," For each tree, we're using our random subset of the actual data. We're not using all the data. We're just using 90% and that's randomly selected per tree. We're also only selecting per tree 50% of the columns in that data set. So for every tree we make, we select a different 50% of the columns to make that tree. And that helps with overfitting, and that also helps speed things up considerably. Other than that, we're just using the AUC to score, and we're not doing a lot of cross-validation. We're not doing 10fold, we're just doing threefold, but you get the idea of how we're doing it."
2222,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2896.0,2935.0,39.0," So when I ran this, when I ran the first round, it gave me a max depth of four, which is the middle value here. I gave, you know, we could have bent gone down as low as three, and we could have gone as high as five. Because I got the middle value, I set that value in the second round. However, for learning rate, I got a value on the edge of the range. So these are values that are, or, you know, so this is on this, sort of the high end for, for learning rate, but it could go higher."
2223,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2935.0,2984.0,49.0," And so what I did is because it's possible that it could go higher, I continued to explore in that direction the next time I went through cross-validation. Likewise for gamma, the first time we got this, we got the middle value, so I just set gamma to 0.25, but for the regularization parameter, we got 10. And so I continued to explore with larger values for the lambda. But it did settle on scale, pause, wait for three, and so we're just going to stick with that. And so when I, so then I just ran the second round, and ultimately I got gamma equals 0.25, the learning rate equals 0.1, max depth equals four, and regularization parameter is 10."
2224,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,2984.0,3044.0,60.0," Okay, so now that we've optimized the parameters, we can, we can build the final XG boost model. And now this call to XG boost classifiers a little more complicated, because we're specifying a lot more of the parameters here, because we're not just using default settings now, we're using the ones that we optimized. We're using early stopping just like we did before, and that means we don't have to optimize the number of trees in the, that we're going to use in the random forest. So we'll run this, and it's going, it's going, going, going, going. And there it is, it stopped after making 65 trees, but remember, that means there were 10 trees that did not improve before that."
2225,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3044.0,3089.0,45.0," And so it says that the best iteration was the 50 fifth, so it built 55 trees, and that's, that's where it's, it's going to end. Now let's plot the confusion matrix, this is just like before. And now we see that we're doing a much better job classifying people that left the company. Now we've captured 390, that's 84% before we're only getting 51%, however, this improvement comes at the expense of correctly classifying the people that did not leave. However, I mean, the company may feel differently, and that's fine with them, but for my perspective since this is my tutorial."
2226,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3089.0,3150.0,61.0," The way I see it is when people leave the company, they take their money with them, and that's money that the company does not get. And so it'd be nice to be able to catch those people before they leave, and then send them a coupon for a milkshake or an ice cream cone, and maybe, if we do that, they'll stick around and they'll continue to pay us every month for their internet and their telecommunications needs. Now, like I said, we are not doing such a hot job predicting people that aren't going to leave, but that's an error I'm willing to make, and so what this means is maybe I'll send a free coupon to someone who isn't going to leave. The company they'll get a free ice cream cone or a free milkshake or something like that. And that's just going to make them feel better about the company. They're going to make a hay. I really like this teleco company. They give me milkshakes every now and then. So I feel like we're making errors, of course, but we're making better errors than we were making before."
2227,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3151.0,3212.0,61.0," Okay, the last thing we do is draw the tree. This code right here where we're creating the XG boost container and then training everything is the same as before except for this thing, which is key, which tells us tells XG boost that we only want to build one tree. So we want to build 65 trees or 55 trees. We just want to build one because all we want to do is draw that first tree. Now, why do we want to draw this first tree? One reason we want to draw it is, say like we don't have an idea of what are some reasonable values to set for the cross validation for those parameters for the regularization or for all these things or for gamma, printing out the first tree can give us a sense of what a good ballpark is. We'll see what the values are. You know, we'll see what the gain is. We'll see what the weight is. We'll see what the cover is. We'll see all these things."
2228,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3212.0,3286.0,74.0," And those will give us a starting point for optimizing these parameters when we go back to optimize the tree, but we've already optimized the tree, but it's still kind of cool also to look at the tree as well. So we're going to down here. I've just got stuff that makes the tree pretty. I've also got stuff for printing out sort of numerically how that tree performed. So here's what we're going to do. We're going to run the code. And here is these are our metrics for how the tree did and these are some of the values that we can use to help us figure out what values we need to use in when we're optimizing the tree. And here's the tree itself. And I know it's really hard to see the individual values inside of these boxes. So I'll just tell you what they are. In the root. So in each of these green nodes, we have a column name and we have a threshold for splitting observations. In the root, in the root node, we have contract month to month less than one."
2229,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3286.0,3345.0,59.0," And that means all of the people who were that's true were the value in that column was less than one. They go to the left. And all the people were that statement is false. They go to the right. And actually, when we draw this tree out, it's hard to see, but there's a little no there. And on the branch that goes to the right. And on the branch that goes to the left, there's a little yes comma missing. So that means people that have values less than one for month to month. Plus the missing data, as we saw before, are going to go to the left. So that's how to interpret the tree. The leaves. They don't give us crossifications. Remember, this is actually boost. And the leaves just give us sort of a small incremental piece of probability that we add together for all of the trees."
2230,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3345.0,3384.0,39.0," And that gives us the final probability that an observation is one classification or the other. They're going to leave the company or not. In conclusion, we have loaded data from a file identified and dealt with missing data. Formatted the data for XG boost using one hot encoding, built an XG boost model for classification, optimized the XG boost parameters with cross validation and grid search, built, drew, interpreted, and evaluated the optimized XG boost model. And that gets us to the triple ban."
2231,XGBoost in Python from Start to Finish,https://www.youtube.com/watch?v=GrJP9FLV3FE,GrJP9FLV3FE,3384.0,3402.0,18.0," Hooray, we've made it to the end of the Jupiter notebook. I want to thank all of you guys for supporting stat quest. And it just means a lot to me that you'd be willing to show up for a webinar and I hope you're all safe. And until next time, quest on."
2232,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,0.0,24.36,24.36," I think that the coaxe sign is fine. So we'll calculate it between this and that line's dead quest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about the co-sign similarity and it's going to be clearly explained."
2233,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,24.36,47.32,22.96," When you build them and deploy them, awesome stuff and it's really fast to use them right now. We're ready. This StatQuest is also brought to you by the letters A, B and C. A always, B, B, C, curious. Always B, curious."
2234,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,47.32,74.0,26.68," Imagine we saw these sentences about the 1990 hit movie, Troll 2. By I, it's pretty easy to see that the first three sentences are similar to each other. They all express a positive feeling towards the movie. In contrast, this last sentence is from someone who does not like troll 2. So when we don't have many sentences, it's pretty easy to see which ones are similar"
2235,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,74.0,92.8,18.8," to each other and which ones are different. However, what would we do if we collected all of the Twitter traffic for the last month? How would we determine similarities and differences in the tweets? In this case, we can no longer rely on doing things by I and instead have to get a computer"
2236,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,92.8,107.48,14.679999999999993, to do it. This is where the co-sign similarity comes in handy. The co-sign similarity is a relatively easy to calculate metric that can tell us how similar or different things are. BAM!
2237,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,107.48,129.72,22.24000000000001," To get an understanding of how the co-sign similarity works, let's start with a super simple example. Here we want to know how similar the phrase hello world is to hello. So the first thing that we do is make a table for the words that appear in the phrases hello and world."
2238,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,129.72,157.12,27.40000000000001," In the first phrase hello world we see the word hello once and we see the word world once. In contrast, in the second phrase hello we only see the word hello one time and we don't see the word world at all. Now given this table of counts for each word in each phrase we can create a two-dimensional"
2239,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,157.12,186.36,29.23999999999998, graph that has the number of times we saw the word hello on the x-axis and the number of times we saw the word world on the y-axis. Now since the first phrase hello world had one occurrence of each word we can plot a point for it in the center of the graph. In contrast the point for the second phrase hello is on the x-axis at one.
2240,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,186.36,219.88,33.52000000000001, Now if we draw lines from the origin of the graph zero zero to the points we can see that there is a 45 degree angle between the two lines. In the cosine of a 45 degree angle the cosine of 45 degrees is 0.71 and thus the cosine similarity between the phrases hello world and hello is the cosine of 45 degrees which equals 0.71.
2241,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,219.92,254.66,34.74000000000001, Now the table has a three for hello since it's in the phrase three times but we still have a zero for the word world. In this case the point on the graph representing hello hello hello would be further out on the x-axis. However the angle between the two lines would still be 45 degrees and thus the cosine
2242,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,254.66,275.28,20.619999999999976, similarity would be the exact same as what we got before the cosine of 45 degrees equals 0.71. In other words the cosine similarity is determined entirely by the angle between the lines and not by the lengths of the lines. Bam.
2243,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,275.36,301.32,25.96000000000004, Now if both phrases are hello world then they are exactly the same and we end up plotting the dots that represent the phrases on top of each other creating a kind of swampy green color and the angle between the lines from the origin and the points would be zero degrees and the cosine similarity is the cosine of zero degrees which equals one.
2244,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,301.32,331.16,29.83999999999997, So when both phrases are exactly the same the angle between them will be zero degrees and the cosine similarity is one. In contrast if the phrases don't have any words in common like here where the first phrases hello and the second phrases were old then the angle between the two phrases will be 90 degrees and the cosine similarity will be the cosine of 90 degrees which
2245,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,331.16,356.20000000000005,25.04000000000002, equals zero. To summarize when two phrases have absolutely nothing in common the cosine similarity is zero and when the phrases are the exact same the cosine similarity is one and when there is some overlap between the two phrases but they are not exactly the same the cosine similarity is between zero and one.
2246,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,356.20000000000005,385.76,29.559999999999945, Double bam. Now the way we've been computing the cosine similarity step one make a table of the word counts step two plot the points step three figure out the angle and lastly step four calculate the cosine of the angle is pretty tedious. The good news is that there is a relatively simple formula that can be computed super quickly
2247,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,385.76,405.28,19.519999999999985, on a computer to calculate the cosine similarity directly from the table of word counts. Dang that equation looks complicated. Don't worry Squatch we'll go through it one step at a time. Thanks. These segments are short hand for adding up a bunch of stuff.
2248,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,405.28,432.48,27.19999999999999, The eyes short for index and is used to keep track of the word we are working on. Eye starts out set to one the first word in a table in this case the first word is hello and n is the number of different words in the phrases in this case n equals two because we have two words hello and world. Lastly a and b refer to the two phrases.
2249,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,432.48,464.64,32.160000000000025, In this case let's let a be the first phrase hello world and b be the second phrase hello now when i equals one we plug in the word counts for the first word hello and when i equals two we plug in the word counts for the second word world. And when we do the math we get 0.71 which is the same value we got when we took the cosine of 45 degrees.
2250,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,464.64,491.48,26.840000000000032, Bam. Note when we only have two different words in our table like we do here then we can easily plot the points on a two dimensional graph because we can put one word like hello on the x axis the first dimension and the other word world on the y axis the second dimension. But when we have phrases with more than two different words then we need more than two
2251,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,491.48,519.88,28.399999999999977, dimensions to plot the points. For example if we wanted to calculate the cosine similarity for these two phrases where the first phrase i love troll two is phrase a and the second phrase i love jim kata is phrase b then the table will have five different words in it and that means we would need a five dimensional graph to plot the points and i have no idea how to draw five dimensional
2252,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,519.92,535.12,15.200000000000044, graph. Want want. The good news is that this is another way that having the equation for the cosine similarity can come in handy. Rather than worry about how to draw five dimensional graph we just plug the numbers into
2253,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,535.12,554.04,18.91999999999996, the equation. Then we do the math and we get 0.58. So the cosine similarity between these two phrases is 0.58. Triple B. In summary the cosine similarity is a relatively easy to calculate metric that can tell
2254,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,554.04,574.7199999999999,20.67999999999995, us how similar or different things are. Note if you haven't already seen it jim kata is an awesome 1985 movie about a gymnast who uses the parallel bars to defeat his enemies in mortal combat. Bam. Now it's time for some shameless self promotion.
2255,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,574.72,593.0400000000001,18.32000000000005, If you want to review statistics and machine learning offline check out the stat quest PDF study guides and my book the stat quest illustrated guide to machine learning at statquest.org. There's something for everyone. Hooray we've made it to the end of another exciting stat quest.
2256,"Cosine Similarity, Clearly Explained!!!",https://www.youtube.com/watch?v=e9U0QAFbfLI,e9U0QAFbfLI,593.0400000000001,613.24,20.19999999999993," If you like this stat quest and want to see more please subscribe and if you want to support stat quest consider contributing to my Patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below. Alright until next time, quest on."
2257,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,0.0,22.0,22.0," Support, vector, machines, have a lot of terminology associated with them. Brace yourself. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about support vector machines and they're going to be clearly explained."
2258,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,22.0,46.0,24.0," Note, this Stack Quest assumes that you are already familiar with the trade-off that is all of machine learning, the bias variance trade-off. You should also be familiar with cross-validation. If not, check out the quests. The links are in the description below. Let's start by imagining we measured the mass of a bunch of mice."
2259,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,46.0,68.0,22.0," The red dots represent mice that are not obese. And the green dots represent mice that are obese. Based on these observations, we can pick a threshold. And when we get a new observation that has less mass than the threshold, we can classify it as not obese."
2260,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,68.0,95.0,27.0," And when we get a new observation with more mass than the threshold, we can classify it as obese. However, what if we get a new observation here? Because this observation has more mass than the threshold, we classify it as obese. But that doesn't make sense because it is much closer to the observations that are not obese."
2261,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,95.0,117.0,22.0," So this threshold is pretty lame. Can we do better? Yes. Going back to the original training data set, we can focus on the observations on the edges of each cluster, and use the midpoint between them as the threshold."
2262,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,117.0,139.0,22.0," Now, when a new observation falls on the left side of the threshold, it will be closer to the observations that are not obese. Then it is to the obese observations. So it makes sense to classify this new observation as not obese. Bam!"
2263,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,139.0,162.0,23.0," Oh no, it's a terminology alert. The shortest distance between the observations and the threshold is called the margin. Since we put the threshold halfway between these two observations, the distances between the observations and the threshold are the same and both reflect the margin."
2264,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,162.0,186.0,24.0," When the threshold is halfway between the two observations, the margin is as large as it can be. For example, if we move the threshold to the left a little bit, then the distance between the threshold and the observation that is not obese would be smaller. And thus, the margin would be smaller than it was before."
2265,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,187.0,213.0,26.0," And if we move the threshold to the right a little bit, then the distance between the obese observation and the threshold would get smaller. And again, the margin would be smaller. When we use the threshold that gives us the largest margin to make classifications, heads up, terminology alert, we are using a maximal margin classifier."
2266,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,213.0,230.0,17.0," Bam! No, no bam! Maximum margin classifiers seem pretty cool, but what if our training data looked like this? And we had an outlier observation that was classified as not obese,"
2267,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,230.0,254.0,24.0," but was much closer to the obese observations. In this case, the maximum margin classifier would be super close to the obese observations. And really far from the majority of the observations that are not obese. Now, if we got this new observation, we would classify it as not obese,"
2268,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,254.0,275.0,21.0," even though most of the not obese observations are much further away than the obese observations. So, maximal margin classifiers are super sensitive to outliers in the training data, and that makes them pretty lame. Can we do better? Yes!"
2269,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,275.0,297.0,22.0," To make a threshold that is not so sensitive to outliers, we must allow misclassifications. For example, if we put the threshold halfway between these two observations, then we will misclassify this observation. However, now when we get a new observation here,"
2270,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,297.0,325.0,28.0," we will classify it as obese. And that makes sense because it is closer to most of the obese observations. Choosing a threshold that allows misclassifications is an example of the bias variance tradeoff that plagues all of machine learning. In other words, before we allowed misclassifications, we picked a threshold that was very sensitive to the training data."
2271,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,325.0,347.0,22.0," It had low bias. And it performed poorly when we got new data. It had high variance. In contrast, when we picked a threshold that was less sensitive to the training data and allowed misclassifications, so it had higher bias, it performed better when we got new data,"
2272,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,347.0,365.0,18.0," so it had low variance. Small ban. Oh no, it is another terminology alert. When we allow misclassifications, the distance between the observations and the threshold is called a soft margin."
2273,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,365.0,393.0,28.0," So the question is, how do we know that this soft margin is better than this soft margin? The answer is simple. We use cross-validation to determine how many misclassifications and observations to allow inside of the soft margin to get the best classification. For example, if cross-validation determined that this was the best soft margin,"
2274,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,393.0,419.0,26.0," then we would allow one misclassification and two observations that are correctly classified to be within the soft margin. Bam. When we use a soft margin to determine the location of a threshold, brace yourself, we have another terminology alert. Then we are using a soft margin classifier,"
2275,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,419.0,446.0,27.0," aka a support vector classifier to classify observations. The name support vector classifier comes from the fact that the observations on the edge and within the soft margin are called support vectors. Super small bam. Note, if each observation had a mass measurement and a height measurement,"
2276,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,446.0,476.0,30.0," then the data would be too dimensional. When the data are too dimensional, a support vector classifier is aligned. And in this case, the soft margin is measured from these two points. The blue parallel lines give us a sense of where all of the other points are in relation to the soft margin. These observations are outside of the soft margin,"
2277,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,476.0,503.0,27.0," and this observation is inside the soft margin and misclassified. Just like before, we used cross validation to determine that allowing this misclassification results in better classification in the long run. Bam. Now, if each observation has a mass, a height, and an age,"
2278,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,503.0,533.0,30.0," then the data would be three-dimensional. Note, the axis that ages on is supposed to represent depth, and these circles are larger in order to appear closer and thus younger. And these circles are smaller in order to look further away and thus older. When the data are three-dimensional, the support vector classifier forms a plane instead of a line."
2279,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,534.0,561.0,27.0," And we classify new observations by determining which side of the plane they are on. For example, if this were a new observation, we would classify it as not obese, since it is above the support vector classifier. Note, if we measured mass, height, age, and blood pressure, then the data would be in four dimensions."
2280,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,562.0,585.0,23.0," And I don't know how to draw four-dimensional graph. What? What? But we know that when the data are one-dimensional, the support vector classifier is a single point on a one-dimensional number line. Psst. In mathematical jargon, a point is a flat, affian zero-dimensional subspace."
2281,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,585.0,619.0,34.0," And when the data are in two dimensions, the support vector classifier is a one-dimensional line in a two-dimensional space. Psst. In mathematical jargon, a line is a flat, affian, one-dimensional subspace. And when the data are three-dimensional, the support vector classifier is a two-dimensional plane in a three-dimensional space. Psst. In mathematical jargon, a plane is a flat, affian, two-dimensional subspace."
2282,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,619.0,648.0,29.0," And when the data are in four or more dimensions, the support vector classifier is a hyperplane. Psst. In mathematical jargon, a hyperplane is a flat, affian subspace. Note. Technically speaking, all flat, affian subspaces are called hyperplanes. So, technically speaking, this one-dimensional line is a hyperplane."
2283,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,648.0,680.0,32.0," But we generally only use the term when we can't draw it on paper. Small ban. Because this is just more terminology. Ugg. Support vector classifiers seem pretty cool because they can handle outliers, and because they can allow misclassifications, they can handle overlapping classifications. But what if this was our training data, and we had tons of overlap?"
2284,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,680.0,708.0,28.0," In this new example, with tons of overlap, we are now looking at drug dosages. And the red dots represent patients that were not cured. And the green dots represent patients that were cured. In other words, the drug doesn't work if the dosages too small or too large. It only works when the dosages just write."
2285,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,708.0,743.0,35.0," Now, no matter where we put the classifier, we will make a lot of misclassifications. So, support vector classifiers are only semi-cool, since they don't perform well with this type of data. Can we do better than maximal margin classifiers and support vector classifiers? Yes. Since maximal margin classifiers and support vector classifiers can't handle this data, it's high time we talked about, support vector machines."
2286,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,744.0,785.0,41.0," So let's start by getting an intuitive sense of the main ideas behind support vector machines. We start by adding a y-axis so we can draw a graph. The x-axis coordinates in this graph will be the dosages that we have already observed. And the y-axis coordinates will be the square of the dosages. So, for this observation, with dosage equals 0.5 on the x-axis, the y-axis value equals dosage squared, which equals 0.5 squared, which equals 0.25."
2287,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,785.0,840.0,55.0," Now we use dosage squared for this y-axis coordinate, and then we use dosage squared for the y-axis coordinates for the remaining observations. Since each observation has x and y-axis coordinates, the data are now two-dimensional. And now that the data are two-dimensional, we can draw a support vector classifier that separates the people who work here, from the people who are not cured. And the support vector classifier can be used to classify new observations. For example, if a new observation had this dosage, then we could calculate the y-axis coordinate by squaring the dosage, and classify the observation as not cured, because it ended up on this side of the support vector classifier."
2288,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,840.0,880.0,40.0," On the other hand, if we got a new observation with this dosage, then we would square the dosage and get a y-axis coordinate, and classify this observation as cured, because it falls on the other side of the support vector classifier. Bam! The main ideas behind support vector machines are, one, start with data in a relatively low dimension. In this example, the data started in one dimension. Two, move the data into a higher dimension."
2289,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,880.0,911.0,31.0," In this example, we move the data from one dimension to two dimensions. Three, find a support vector classifier that separates the higher dimensional data into two groups. That's all there is to it. Double-bow! Going back to the original one-dimensional data, you may be wondering why we decided to create y-axis coordinates with dosage squared."
2290,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,911.0,946.0,35.0," Why not dosage cubed? Or pi divided by four times the square root of dosage. In other words, how do we decide how to transform the data? In order to make the mathematics possible, support vector machines use something called kernel functions to systematically find support vector classifiers in higher dimensions. So let me show you how a kernel function systematically finds support vector classifiers in higher dimensions."
2291,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,946.0,987.0,41.0," For this example, I use the polynomial kernel, which has a parameter D, which stands for the degree of the polynomial. When D equals one, the polynomial kernel computes the relationships between each pair of observations in one dimension. And these relationships are used to find a support vector classifier. When D equals two, we get a second dimension based on dosage squared. And the polynomial kernel computes the two-dimensional relationships between each pair of observations."
2292,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,987.0,1022.0,35.0," And those relationships are used to find a support vector classifier. And when we set D equal three, then we would get a third dimension based on dosage's cubed. And the polynomial kernel computes the three-dimensional relationships between each pair of observations. And those relationships are used to find a support vector classifier. And when D equals four or more, then we get even more dimensions to find a support vector classifier."
2293,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1023.0,1059.0,36.0," In summary, the polynomial kernel systematically increases dimensions by setting D that degree of the polynomial. And the relationships between each pair of observations are used to find a support vector classifier. Last but not least, we can find a good value for D with cross validation. Double-bound. Another very commonly used kernel is the radial kernel, also known as the radial basis function kernel."
2294,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1059.0,1090.0,31.0," Unfortunately, the radial kernel finds support vector classifiers in infinite dimensions, so I can't give you an example of what it does exactly. However, when using it on a new observation like this, the radial kernel behaves like a weighted nearest neighbor model. In other words, the closest observations, aka the nearest neighbors, have a lot of influence on how we classify the new observation."
2295,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1090.0,1119.0,29.0," And observations that are further away have relatively little influence on the classification. So, since these observations are the closest to the new observation, the radial kernel uses their classification for the new observation. Bam! Now, for the sake of completeness, let me mention one last detail about kernels."
2296,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1119.0,1164.0,45.0," Although the examples I've given show the data being transformed from a relatively low dimension to a relatively high dimension, kernel functions only calculate the relationships between every pair of points as if they are in the higher dimensions. They don't actually do the transformation. This trick, calculating the high dimensional relationships without actually transforming the data to the higher dimension, is called the kernel trick. The kernel trick reduces the amount of computation required for support vector machines by avoiding the math that transforms the data from low to high dimensions."
2297,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1164.0,1201.0,37.0," And it makes calculating the relationships in the infinite dimensions used by the radial kernel possible. However, regardless of how the relationships are calculated, the concepts are the same. When we have two categories, but no obvious linear classifier that separates them in a nice way. Support vector machines work by moving the data into a relatively high dimensional space. And finding a relatively high dimensional support vector classifier that can effectively classify the observations."
2298,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1201.0,1225.0,24.0," Triple-bound! Hey, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate."
2299,Support Vector Machines Part 1 (of 3): Main Ideas!!!,https://www.youtube.com/watch?v=efR1C6CvhmE,efR1C6CvhmE,1225.0,1231.0,6.0," The links are in the description below. Alright, until next time, quest on!"
2300,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,0.0,21.32,21.32," A once-new kernel, its name was Fred. The stat quest isn't about that kernel. Stat Quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to talk about support vector machines part two, the Polynomial kernel."
2301,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,21.32,44.24,22.92," Specifically, we're going to talk about the Polynomial kernel's parameters. And how the Polynomial kernel calculates high dimensional relationships. Note, this stat quest assumes that you are already familiar with support vector machines. If not, check out the quest. The link is in the description below."
2302,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,44.24,69.24,24.999999999999996," In the stat quest on support vector machines, we had a training dataset based on drug dosages measured in a bunch of patients. The red dots represent at patients that were not cured. And the green dots represent at patients that were cured. In other words, the drug doesn't work if the dosages too small or too large."
2303,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,69.24,94.68,25.44," It only works when the dosage is just right. Because this training dataset had so much overlap, we were unable to find a satisfying support vector classifier to separate the patients that were cured from the patients that were not cured. However, when we gave each point a Y-axis coordinate by squaring their original dosage measurements,"
2304,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,94.68,122.24,27.560000000000016, we could draw a line that separated the two categories of patients. So we used a support vector machine with a Polynomial kernel to compute the relationships between the observations in a higher dimension. And then found a good support vector classifier based on the high dimensional relationships. The Polynomial kernel that I used looks like this.
2305,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,122.24,152.8,30.56," A and B refer to two different observations in the dataset. R determines the coefficient of the Polynomial. And like I mentioned in the earlier stack quest, D sets the degree of the Polynomial. In my example, I set R equals 1 half and D equals 2. Since we are squaring the term, we can expand it to be the product of two terms."
2306,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,152.8,174.72,21.919999999999987," Now we just do the multiplication. Beepie, boo, beepie, boo, boo, boop. And combine these two terms and just because it will make things look better later, let's flip the order of these two terms. Finally, this Polynomial is equal to this dot product."
2307,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,174.72,194.68,19.960000000000008," A dot product sounds fancy. But all it is is the first terms multiplied together, plus the second terms multiplied together, plus the third terms multiplied together. The dot product gives us the high dimensional coordinates for the data."
2308,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,194.68,218.68,24.0," The first terms are the x-axis coordinates, and the second terms are the y-axis coordinates. The third terms are z-axis coordinates. But since they are the same for both points, we can ignore them. Thus, we have x and y-axis coordinates for the data in the higher dimension."
2309,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,218.68,244.6,25.919999999999987," Bam! Alternatively, we could have set our equals 1 and d equals 2. Now when we do the math, we get this Polynomial and this dot product. We can verify that the dot product is correct by multiplying each term together, and then add everything up,"
2310,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,244.6,270.28000000000003,25.680000000000035," and the result should be equal to the Polynomial. Using this dot product, the new x-axis coordinates are the square root of 2 times the original dosage values. So we move the points on the x-axis over by a factor of the square root of 2. The new y-axis coordinates are the same as before,"
2311,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,270.28000000000003,293.72,23.44," the original dosage values squared. And just like before, we can ignore the z-axis coordinate since it is a constant value. Now, just like before, we can use the high dimensional relationships to find a support vector classifier. Double bam! Now brace yourself."
2312,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,293.72,318.28000000000003,24.56," Things are about to get a little crazy. Going back to the Polynomial kernel with r equals 1 half and d equals 2, it turns out that all we need to do to calculate the high dimensional relationships is calculate the dot products between each pair of points. And since this kernel is equal to this dot product,"
2313,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,319.48,347.32,27.83999999999997," all we need to do is plug values into the kernel to get the high dimensional relationships. For example, if we wanted to know the high dimensional relationships between these two observations, then we plug the dosages into the kernel, do the math, and 16,2.25 is one of the two dimensional relationships that we need to solve for the support vector"
2314,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,347.32,373.56,26.24000000000001," classifier, even though we didn't actually transform the data to two dimensions. Triple bam! Unfortunately, why we only need to compute the dot product is out of the scope of this stack quest. What? What? To review, the Polynomial kernel computes relationships between pairs of observations."
2315,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,374.92,398.28,23.360000000000014," A and B refer to the two observations that we want to calculate the high dimensional relationships for. Our determines the Polynomials coefficient, and d determines the degree of the Polynomial. Note, R and d are determined using cross validation. Once we decide on values for R and d,"
2316,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,399.24,421.64,22.399999999999977," we just plug in the observations and do the math to get the high dimensional relationships. Hooray! We've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, consider contributing to my Patreon campaign,"
2317,Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3),https://www.youtube.com/watch?v=Toet3EiSFcM,Toet3EiSFcM,421.64,434.52,12.880000000000052," becoming a channel member, buying one or two of my original songs, or a t-shirt, or a hoodie, or just donate. The links are in the description below. All right, until next time, quest on!"
2318,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,0.0,32.0,32.0," The radio, Colonel, works in infinite dimensions. I know that sounds kinda crazy, but it's actually not that bad, stat quest. Hello, I'm Josh Starmor and welcome to StacQuest. Today we're going to talk about support vector machines, part 3, the radio Colonel. Specifically, we're going to talk about the radio Colonel's parameters,"
2319,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,32.0,65.0,33.0," how the radio Colonel calculates high dimensional relationships, and then show you how the radio Colonel works in infinite dimensions. Note, this stack quest assumes that you are already familiar with support vector machines and the polynomial Colonel. If not, check out the quests, the links are in the description below. In the stack quest on support vector machines, we had a training dataset based on drug dosages measured in a bunch of patients."
2320,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,65.0,89.0,24.0," The red dots represent at patients that were not cured, and the green dots represent at patients that were cured. In other words, the drug doesn't work if the dosages too small or too large. It only works when the dosage is just right. Because this training dataset had so much overlap,"
2321,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,89.0,122.0,33.0," we were unable to find a satisfying support vector classifier to separate the patients that were cured from the patients that were not cured. One way to deal with overlapping data is to use a support vector machine with a radial colonel. A.K.A. the radial basis function, R.B.F. Because the radial colonel finds support vector classifiers in infinite dimensions, it's not possible to visualize what it does."
2322,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,122.0,148.0,26.0," However, when using it on a new observation like this, the radial colonel behaves like a weighted nearest neighbor model. In other words, the closest observations, A.K.A. the nearest neighbors, have a lot of influence on how we classify the new observation. And the observations that are further away have relatively little influence on the classification."
2323,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,148.0,182.0,34.0," So, since these observations are the closest to the new observation, the radial colonel uses their classification for the new observation. Now let's talk about how the radial colonel determines how much influence each observation in the training dataset has on classifying new observations. Just like with the polynomial colonel, A and B refer to two different dosage measurements. The difference between the measurements is then squared,"
2324,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,182.0,216.0,34.0," giving us the squared distance between the two observations. Thus, the amount of influence one observation has on another is a function of the squared distance. Gamma, which is determined by cross-validation, scales the squared distance, and thus, it scales the influence. For example, if we set Gamma equal to 1, and plug in the dosages from two observations that are relatively close to each other,"
2325,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,216.0,257.0,41.0," we will set Gamma equal to 2, and plug in the same two dosages as before, and do the math. When Gamma equals 2, we get 0.01. Now let's set Gamma equal to 2, and plug in the same two dosages as before, and do the math. When Gamma equals 2, we get 0.01, which is less than when Gamma equals 1. So we see that by scaling the distance, Gamma scales the amount of influence two points have on each other."
2326,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,257.0,284.0,27.0," Small bound. Now let's set Gamma equal to 1 again, and determine how much influence two observations have when they are relatively far from each other. So we plug in the two dosages, do the math, and when the points are relatively far from each other, we get a number very close to zero."
2327,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,284.0,319.0,35.0," Thus, the further two observations are from each other, the less influence they have on each other. Note, just like with the polynomial kernel, when we plug values into the radial kernel, we get the high-dimensional relationship. Thus, 0.01 is the high-dimensional relationship between these two observations that are relatively close to each other, and a number very close to zero is the high-dimensional relationship between these two observations that are relatively far from each other."
2328,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,319.0,360.0,41.0," Medium bound. Now, before we move on, I want to simplify the training dataset to just two observations. And use the polynomial kernel to give us intuition into how the radial kernel works in infinite dimensions. When R equals 0, the polynomial kernel simplifies to a single term, and that gives us a dot product with a single coordinate. When D equals 2, we get a squared times b squared, which is equal to the dot product of a squared and b squared."
2329,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,360.0,405.0,45.0," Since this dot product only has one coordinate, the new coordinate is just the square of the original measurement on the original axis. For example, if we plug these two dosages into the kernel, we get this dot product, and that means the new access coordinate for this point is 2.5 squared, which equals 6.25, and the new x-axis coordinate for this point is 4 squared, which equals 16. In other words, when R equals 0 and D equals 2, all the polynomial kernel does is shift the data down the original axis."
2330,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,406.0,440.0,34.0," When R equals 0 and D equals 3, we still only have one coordinate in the dot product, and we shift the data down the x-axis further because now we are cubing each value. Lastly, when R equals 0 and D equals 1, we still only have one coordinate in the dot product, but now the data just stays in its original location. So setting R equals 0 seems silly because no matter what values we use for D,"
2331,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,440.0,470.0,30.0," the dot products leave the data in the original dimension. And in this example, the data stays on the same one dimensional line. However, it turns out that setting R equals 0 can result in some pretty awesome stuff. Going back to the original training dataset, let's talk about what happens if we take a polynomial kernel with R equals 0 and D equals 1,"
2332,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,470.0,498.0,28.0," and add another polynomial kernel with R equals 0 and D equals 2. This gives us a dot product with coordinates for two dimensions. The first coordinate is the original dosage, and the second coordinate is dosage squared. Now we can plot the transform data on x, y, x, y, x, y, and find a support vector classifier"
2333,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,498.0,522.0,24.0," to separate the data. And by now, you know that we don't actually do the transformation. We just solve for the dot product to get the high dimensional relationships. Now, if we added another polynomial kernel with R equals 0 and D equals 3, then the dot product has coordinates for three dimensions,"
2334,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,522.0,550.0,28.0," and we can plot the transform data on x, y, z, x, y, z, and find a support vector classifier to separate the data. Now, what if we just kept adding polynomial kernels with R equals 0 and increasing D until D equals infinity? That would give us a dot product with coordinates for an infinite number of dimensions."
2335,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,550.0,583.0,33.0," That would be awesome, right? Well, that's exactly what the radial kernel does, so let's talk about it. Warning, this part gets very Matthew, so feel free to skip to the end if this is not your thing. Let's start with the radial kernel and multiply out the square. Now, because we can set gamma equal to anything, let's set it to 1 half, so that this 2 goes away."
2336,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,583.0,616.0,33.0," Now let's create the Taylor series expansion of this last term. Wait a minute, what's a Taylor series expansion? This big thing is a Taylor series. Although there are exceptions, the main idea is that a function f of x can be split into an infinite sum. Since this is very abstract, let's walk through how to convert e to the x into an infinite sum."
2337,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,616.0,670.0,54.0," In other words, we are setting f of x equal to e to the x, and that means f of a equals e to the a. Now we plug in the derivative of e to the x evaluated at a divided by 1 factorial and multiply by x minus a. Note, in case you don't already know, the derivative of e to the x equals e to the x, so taking the derivative of e to the x is super easy. We plug in the second derivative of e to the x evaluated at a divided by 2 factorial and multiply by x minus a squared. Then we keep adding terms based on higher derivatives until we get to infinity."
2338,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,670.0,703.0,33.0," Thus, this is the Taylor series expansion of e to the x. Now the question is, what is a? The definition of the Taylor series says that a can be any value as long as f of a exists. And since e to the 0 equals 1, e to the 0 exists, so we will set a equal to 0. And simplify."
2339,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,703.0,740.0,37.0," Thus, if we can accept that the Taylor series expansion does what it says it does, e to the x is equal to this infinite sum. Large bound. Going back to the radio kernel, we can now create the Taylor series expansion of this last term. To create the Taylor series expansion of e to the ab, we plug in ab for x. Now we have the Taylor series expansion of the last part of the radio kernel."
2340,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,740.0,776.0,36.0," OK, time to take a deep breath. We've done a lot, but we still have a few more steps before we are done and get to eat snacks. Before we move on, let's remember that when we added up a bunch of polynomial kernels with r equals 0 and d going from 0 to infinity, we got a dot product with coordinates for an infinite number of dimensions. Now, observe that a polynomial kernel with r equals 0 and d equals 0 is equal to 1."
2341,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,776.0,809.0,33.0," And a polynomial kernel with r equals 0 and d equals 1 is equal to a times b. And a polynomial kernel with r equals 0 and d equals 2 is equal to a times b squared. Et cetera, et cetera, et cetera. Thus, each term in this Taylor series expansion contains a polynomial kernel with r equals 0 and d, going from 0 to infinity."
2342,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,809.0,838.0,29.0," Now, just to remind you, converting this sum to a dot product was easy, because the dot product tells us to multiply each term together, and then add up all the terms. With that in mind, the dot product for e to the ab is this. We can verify that the dot product is correct by multiplying each term together,"
2343,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,838.0,870.0,32.0," and add up the new terms to get the Taylor series expansion of e to the ab. Double bam. Going back to the original radial kernel, we can plug in the dot product for e to the ab. Now, the radial kernel is equal to this term times the dot product. To make the radial kernel all one dot product, instead of something times a dot product,"
2344,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,870.0,900.0,30.0," we just multiply both parts of the dot product by the square root of this term. So we can fit everything onto the screen. Let's let s equal the square root of the first term. Now we multiply the dot product by s. And at long last, we see that the radial kernel is equal to a dot product that has coordinates"
2345,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,900.0,925.0,25.0," for an infinite number of dimensions. That means when we plug numbers into the radial kernel and do the math, the value we get at the end is the relationship between the two points in infinite dimensions. Triple bam. Now we can go eat snacks."
2346,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,925.0,945.0,20.0," Hooray! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs, or a t-shirt, or a hoodie,"
2347,Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3),https://www.youtube.com/watch?v=Qc5IyLW_hns,Qc5IyLW_hns,945.0,952.0,7.0," or just donate. The links are in the description below. Alright, until next time, quest on."
2348,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,0.0,21.0,21.0," So point, vector machines in a high-thru, from some bandit who's ready, step quest. Bam! Alright, let's get started. First thing I need to do is share my screen with you. Let's get back going."
2349,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,21.0,48.0,27.0," Alright, here we go. So welcome. Hello. I'm Josh Darmer and welcome to the stat quest on support vector machines in Python from start to finish. In this lesson, we will build a support vector machine for classification using scikit-learn and the radial basis function."
2350,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,48.0,73.0,25.0," Our training dataset contains continuous and categorical data from the UCI machine learning repository to verdict. Whether or not a person will default on their credit card. And note, throughout this jupy or notebook, all these links are live. So if you want to learn more about the UCI machine learning repository, just click on the link. And there it is."
2351,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,73.0,96.0,23.0," If you want to learn more about the dataset that we're going to be using, just click on the link. And there it is. And you see we've got attribute information that describes the data in the dataset. And so there's lots lots of sort of stuff you can click on and learn more about. Here is a picture of the decision surface that we will create with our support vector machine."
2352,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,96.0,112.0,16.0," We'll do this at the end, but I just want to give you a sneak preview of what's going to happen. It's kind of a messy figure, but it's what we're going to do. So let's just let's move on. Okay. So support vector machines. Why would you want to do them?"
2353,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,112.0,148.0,36.0," They are one of the best machine learning algorithms out there for when getting the correct answer is a higher priority than actually understanding why you get the correct answer. They work really well with relatively small datasets and they tend to work well out of the box. In other words, they tend to not require much optimization. So in this lesson, we're going to learn about importing data from a file. We're going to learn about missing data, down sampling data, formatting the data for support vector machines."
2354,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,148.0,168.0,20.0," And we're going to build a preliminary support vector machine. Then we're going to optimize it. And then we're going to build evaluate draw and interpret a final support vector, support vector machine. And we're going to compare it to that preliminary machine. And we're going to see if it does better or doesn't doesn't do better."
2355,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,168.0,196.0,28.0," One of the things about support vector machines is because they're great out of the box. Sometimes optimizing doesn't give you a huge bonus like when you're using classification trees. When we did that, it was night and day. It was like we went from a huge tree to a prune tree and that prune tree was like infinitely better at doing classification. And then we got a new problem in contrast support vector machines tend to be pretty good out of the box."
2356,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,196.0,228.0,32.0," So Giovanni, you know, this tutorial assumes that you already know the basics of Python and are familiar with the theory behind support vector machines. The radial basis function regularization cross validation and confusion matrices. You can check out those questions by clicking these links and you're good to go. Also, I want to strongly encourage all of you to play around with the code once you get it. Playing with code is the best way to learn from it."
2357,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,228.0,250.0,22.0, There's alternative ways to do what what we want to do. In this in this Jupyter notebook. And there are different ways to optimize and there's different ways. You can even just plug in different machine learning algorithms for that matter and just see what happens. There's a lot of learning that can take place by just playing around.
2358,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,250.0,267.0,17.0," And that's when I think that's the exciting part. I mean, this is pretty fun too, so don't get me wrong. Okay. So the very first thing we're going to do is we're going to import the modules that will do all the work. Python itself just gives us a basic programming language."
2359,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,267.0,303.0,36.0," These modules give us extra functionality to import the data, clean it up and format it and then build evaluate and draw the support vector machine. Note you're going to need Python 3 and I've got instructions on how to install that and make sure all your modules are up to date down here. I don't need to go through that because I've already got this set up on my own computer. However, I do want to say that we're using a Jupyter notebook. And as you can see, Jupyter notebooks are a nice way to combine text with code."
2360,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,303.0,325.0,22.0," And so here are some code. This is where we're importing the modules. We're importing pandas to load and manipulate the data and we're using it for one hot encoding. There's also numpy that we're going to use for data manipulation. We're importing mat, plot, lib for graphs and making things look cool."
2361,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,325.0,353.0,28.0," And we're also importing a lot of scikit learn stuff to do support vector machines and do confusion matrices and whatnot. Now, when you have a Jupyter notebook, the way you run the code is you click somewhere in that block of code. And you can either click on this play button up here to run the code. There's a run menu and you can say run selected cells. Or on your computer, there's a key combination on a Macintosh."
2362,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,353.0,381.0,28.0," It's command or a command enter or control enter. On different computer systems, it's different. So just click the run menu and it'll tell you what to do. So what I'm going to do is I'm going to click in this block of code and we're going to run it and when you run it. Briefly, you might have missed it. There was a star there."
2363,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,381.0,406.0,25.0," Before it turned into a number one, the star means that Python is doing something and that time it was loading the modules. And when it gives you a number, that means it's done doing whatever it was doing. So it loaded the modules pretty quickly. Okay, now we're ready to import the data. Now we load the data set from the UCI machine learning repository."
2364,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,406.0,441.0,35.0," Specifically, we're going to use the credit card default data set. This data set will allow us to predict if someone will default in their credit card payments based on their sex, age and a variety of other metrics. Note when pandas, which is what we're going to use to read in the data, when it reads in data, it returns a data frame, which is a lot like a spreadsheet. The data are organized in rows and columns and each row can contain a mixture of text and columns. Oh, excuse me."
2365,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,441.0,459.0,18.0, Text and numbers. The data frame is the initials DF and that's what we're going to use here. And I've got two blocks of code here. One is to read in the file that we're going to use. This is a file that will be included with the Jupyter notebooks. You don't have to download it or anything.
2366,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,459.0,496.0,37.0," We're going to be using read under scores C, C, S, C to read it in. The file has a header row. So the very first row actually it's the second row in the data set. Has the header information. The first row has some other kind of nonsense and I don't actually remember what it was. And it's tabmed-delimited. So we're setting the separator to the tab escape sequence. However, you can also read this directly from the UCI machine learning repository directly. You don't have to download the file first."
2367,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,496.0,521.0,25.0," And this is commented out. So we won't be running. But when you play around the code later, you can feel free to play with it yourself. Okay, so we'll load in the data and there we are. We've got a number here. That means we ran the code. And so now that we've loaded the data into a data frame called DF, we're going to look at the first five rows using the head function. So let's just run that."
2368,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,521.0,540.0,19.0, So we've got DF dot head and that returns us the first five rows. And we see that we have a bunch of columns for the variables collected for each customer. The columns are ID. That's just an ID number. Limit balance is the credit limit for that customer. Sex is male or female.
2369,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,540.0,573.0,33.0," Education, marriage, age, payment. These these columns tell us whether or not the last payment was on time or late or how late it was. There's there's one column for different months of payments. So so it's not just one last month's payments. It goes back six months. Then we have the bill amounts for the last six months, six bills and how much was paid for the last six bills."
2370,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,573.0,596.0,23.0," And then lastly, we have default payment next month. This is the variable that we're going to try to predict. And here I've listed the column names as well. But note, the last column name this one default payment next month. That is a mouthful. So we're going to change it to just be default."
2371,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,596.0,615.0,19.0, So did they default or not? And we do that by doing data frame dot rename. And we pass in the column name that we want to change and then the name we want to change it to. When we set access to columns. We're specifying that we want to change a column name.
2372,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,615.0,639.0,24.0," Lastly, we're saying we want to do it in place, meaning we're going to modify this data frame. That we're not going to make it copy and then save it as a new variable. And then what we're going to do is we're going to print out the next the first five rows. And we're going to verify that we renamed the column correctly. Okay."
2373,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,639.0,655.0,16.0," So we look over here and we see that the last column name is noun default. And that's great. That's what we wanted. The other thing is, is this ID column. These are just random numbers that we're assigned to each customer. And they're not informative. So we're going to drop it."
2374,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,655.0,673.0,18.0," So we'll do that with df dot drop and we're specifying the ID column. Again, this is a column. And we're going to do this in place just like we did before. And we're going to print out the first five rows to verify that we removed the column. And there it is."
2375,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,673.0,687.0,14.0," Like we've got ID up here. And now we no longer have the ID. So hooray, we've cleaned up the columns a bit. And now that we have the data in a data frame called df, we are ready to identify and deal with missing data."
2376,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,687.0,711.0,24.0," Unfortunately, the biggest part of any data analysis project is making sure that the data are correctly formatted and fixing it when it is not. The first part of this process is identifying and dealing with missing data. missing data is simply a blank space or a surrogate value like n a that indicates that we failed"
2377,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,711.0,730.0,19.0," collect data for one of the features. For example, if we were forgot to ask someone's age or forgot to write it down, then we would have a blank space in the data set for that person's age. There are two main ways to deal with missing data. We can remove the rows that contain missing data from the data set."
2378,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,730.0,757.0,27.0," Or we can impute the values that are missing. And in this context, impute is just a fancy way of saying we can make an educated guess about what that value should be. So in this section, we're going to focus on identifying missing data in the data set. First, let's see what sort of data is in each column. To do that, we've got our data frame and we're asking for the data types."
2379,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,757.0,789.0,32.0," So we're going to check out the data types with the D types command. And when we run that code, we see that every column is n64, which is good, or at least it looks good, because it doesn't tell us off the bat that the person mixed letters and numbers. In other words, there are no in A values. And that suggests that maybe things are in good hands."
2380,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,789.0,820.0,31.0," And because they didn't use character-based placeholders for missing data and data frame. That said, we should still make sure that each column contains acceptable values. The list below describes what are allowed in each column. And it was based on the column descriptions in credit card in the credit card default webpage. So the limit balance, the credit balance or credit limit, excuse me, is the amount available, available credit."
2381,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,820.0,830.0,10.0, And that's an integer. Sex is a category. We have one for mail and two for female. Education is also a category. We've got four categories for that.
2382,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,831.0,840.0,9.0, Graduate school is one. Two equals universities. Three is high school and four is others. Marriage is also a category. We have one for married.
2383,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,840.0,859.0,19.0," Two for single and three for other. Ages and integer pay. This is the, the, these are a series of columns that tell us when the last six bills were put paid. Negative one needs on time. And then we have how much after that we've got how much it's been delayed."
2384,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,859.0,894.0,35.0," We've got bill amount pay amount and default, which is a binary column that has zero for did not default and one for defaulted. So we're going to start by making sure sex only contains numbers one and two. And we do that with our data frame and in square brackets, we specify that we want to look at the column named sex. And we've got that in single quotes and we're what we want to do is we want to look at the unique values in that column. And we do that with the unique function."
2385,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,894.0,915.0,21.0," So it's run that. And we see that the unique values in this column named sex are two and one. So bam, it does. Now we're going to look at education to make sure it only contains one, two, three and four. So we do the exact same thing we did before only we swapped sex with education."
2386,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,915.0,946.0,31.0," When we run that, we see that for reasons unknown. In addition to the allowed numbers one, two, three and four education also contains zero five and six. It is possible that zero represents missing data and five and six represent categories. Not mentioned in the original specification, but that's just a guess. Now we're going to look at marriage and make sure it only contains one, two and three."
2387,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,946.0,969.0,23.0," So we do that the exact same code is before only this time we're specifying the column named marriage. And like education, marriage contains zero which I'm guessing represents missing data. Now, note. This data say this part of an academic publication that is not open access. It's owned by a company called Elsavere."
2388,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,969.0,996.0,27.0," So in theory, I could pay Elsavere a lot of money to get the article and find out if zero represents missing data or not. There's a good chance that the authors even didn't even mention in the article. So there's a good chance that even if I paid a lot of money for the article, I still wouldn't know. And because this is just a demo, I'm not going to worry too much about being correct. And instead we're just going to see what happens when we treat zero as missing data."
2389,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,996.0,1033.0,37.0," And I will say this. I try to both ways and the support vector classifier performs better when we treat zero as missing data. So I at least try to both ways and I said, well, since it works better this way, we'll just assume that that means missing data. Since scikit-learn support vector machines do not support data sets with missing values, we need to figure out what to do with the zeros and the data set. We can either delete these columns from the training data set or impute values for the missing data."
2390,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1033.0,1080.0,47.0," First, let's see how many rows contain missing values. We do that by using the Location function that is associated with our data frame. And what you do is you specify sort of a logical statement that if it's true, it'll return those specific rows that were interested in. And if we're interested in finding rows that have zero in the Education column or. And that's a logical or and old school computer language talk we call that a pipe."
2391,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1080.0,1122.0,42.0," But pipe character, but it's a logical or so we want all the lines in the data set where there's a zero in the Education column or. There's a zero in the marriage column and we wrapped all of this up in the Lynn function which is short for length. And so we're going to count the number of rows that have zeros in those columns. And when we run that we see that there are 68 rows that have missing values. So now we're going to count the total number of rows in the data set and we're going to use that Lynn or length function just like we did before only this time we're not specifying."
2392,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1122.0,1157.0,35.0," Specifying which rows we want and we don't specify specific rows we get them all. And we see that there are 30,000 rows in the data set to begin with. And so 68 of the 30,000 rows or less than 1% contain missing values. Since that still leaves us with more data than we need for a support vector machine. And so we're going to try to improve the values and we're going to try to improve the values."
2393,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1157.0,1186.0,29.0, And we're going to try to do imputing imputation in a future webinar hopefully in two months. So the way we do this we select all the rows that do not contain zero in either education or marriage. And the way I said that was a little awkward because we're actually going to use an and a logical and in this statement. And so we're going to use an and we're going to use an or we're getting all the rows we're using this low function to get the rows that where this. Logical statement is true.
2394,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1186.0,1215.0,29.0, And what we want are rows that do not have zero in education and do not have zero in marriage. And we're going to save all those rows that don't have zero in education and. And we're going to save all those rows in a new data frame called data frame no missing. So we'll run that. And since data frame no missing has 68 fewer rows in the original data frame.
2395,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1215.0,1236.0,21.0," It should only have 29,932 rows. So we're going to count the number of rows using that length function again. And we've got it. Gray, the math works out. However, we can also make sure that education no longer contains zeros by printing out the unique values."
2396,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1236.0,1256.0,20.0," This is exactly what we did before. However, now we're specifying data frame no missing rather than just data frame the original data set. And now we see that there's no zero there and when we do the same thing for marriage. We print out the unique values from marriage. Now we just have one, two and three."
2397,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1256.0,1280.0,24.0," So bam, we have verified that data frame no missing does not contain any missing values. All right. And we're ready to move on to the next section where we down sample the data set. So like I've said, support vector machines are great with small data sets. They, the data set that we used for classification trees is relatively small and support vector machines do pretty well."
2398,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1280.0,1305.0,25.0," With that data set. However, like I said, they can take a long time with large data sets. And this is relatively large. So we're going to down sample both categories, customers who did not default in customers that did down to a thousand each. So first thing we're going to use is remind ourselves how many rows of data we're working with because we removed some of them."
2399,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1305.0,1322.0,17.0," So we use the length function again. And that tells us we've got 29,932 samples. That's relatively large. So we're going to down, down sample it to a thousand of each category. So the way we do that is we've created this."
2400,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1322.0,1341.0,19.0, We've already have this data frame called data frame new missing and we're specified. We want all the rows where that someone did not default. So default is zero and we're going to store that in a new variable called DF. No default. And then we're doing the same thing for the people that default it.
2401,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1341.0,1359.0,18.0, We're storing them in another data frame. So we're splitting the data into two variables here. One for people that default it and one for people that did not default. And now what we're doing is we're down sampling the people that did not default. We're using the resample function.
2402,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1359.0,1377.0,18.0, And we're passing it the data frame that consists of people that did not default. We're setting a replace to false. So that means when we pull something out of there we don't. And we put it in our new data frame. We don't put that back in the pool possible people that we could select again.
2403,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1377.0,1393.0,16.0, And we're saying that we want a thousand samples. And we're setting the random state to 42. That's a random seed. And all that does is make sure that you get the same answer that I get. And then what we're going to do is we're going to print out the length of this new data frame that we're creating.
2404,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1393.0,1404.0,11.0, Data frame no no default down sampled. So let's run that. Oh no. I got there too early. I didn't split the data set in the half.
2405,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1404.0,1425.0,21.0," So yeah, so when you see this pink that means there that means something went wrong. And instead of feeling great shame what you got to do is just got a default debug it. And so let's rerun this again. Because before the first time I ran it I hadn't run this chunk of code first. And so this data frame no default this guy did not exist."
2406,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1425.0,1437.0,12.0, But now it exists. So let's see what happens. And it runs and tells us we've got a thousand rows and our new data frame. Data frame no default down sampled. Now we're going to do the exact same thing.
2407,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1437.0,1460.0,23.0, But this time we're using the people that defaulted. And we're going to print out the number of rows there. And there. So we've got two new variables each containing a thousand rows each. And now what we want to do is we want to merge them back into a single data frame and print out the total number of rows to make sure everything is hunky dory.
2408,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1460.0,1474.0,14.0, To merge the two data frames that we're creating. We're using this Pantus function called concat which will concat Nate. The two two things. So there we go. Damn.
2409,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1474.0,1491.0,17.0, 2000. So now that we've taken care of the missing data we are ready to start formatting the data for making a support vector machine. The first step is to split the data into two parts. We're going to have one part that contains the columns of data that we will use to make classifications.
2410,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1491.0,1511.0,20.0, And one part is going to be a column of data that contains the things we want to predict. So we're going to use the conventional notation of capital X. You represent the columns of data that we will use to make classifications. And we're going to use lowercase y. You represent the thing we want to predict.
2411,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1511.0,1530.0,19.0," In this case, we want to predict default whether or not someone defaulted on their payments. So here's how we're going to do that. We've got our down sample data set. And what we're doing is we're going to drop the default column. And we're going to copy that."
2412,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1530.0,1543.0,13.0, We're going to store that into uppercase X. And that's going to be our set of columns that we're going to use to make predictions. And then we're going to print out the first five rows. So let's do that. Bam.
2413,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1543.0,1561.0,18.0, Okay. You'll notice we scroll over to the right. We no longer have that column called default. So this data set does not contain the thing we want to predict. Now here we're going back to that data frame that contains the down sample samples.
2414,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1561.0,1574.0,13.0, And now we're just specifying that one column default. And we're making a copy of it. And we're storing it in a new variable called y. And then we're going to print out the first five rows. Bam.
2415,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1574.0,1591.0,17.0," Okay. Now we've created uppercase X, which is the data we're going to use to make predictions. And lowercase y, which has the data we want to predict. We are ready to continue formatting. X over here so that it's suitable for making a support vector machine."
2416,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1591.0,1607.0,16.0," Okay. So now we're on to one hot encoding. Now we have to split the data frame into two pieces. uppercase X, which contains, okay, we've already done this. We've split things up."
2417,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1607.0,1624.0,17.0," Now what we need to do is take a look at the variables in X. And this list tells us whether the variable source will be an integer or a category. So we see limit balance as an integer. That's the credit limit. Sex is a category that's male or one for male, two for female."
2418,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1624.0,1638.0,14.0," Education has a bunch of categories, one, two, three, and four. We've already talked about these. Okay. So it looks like sex, education, marriage, and pay are supposed to be categorical. And they need to be modified."
2419,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1639.0,1662.0,23.0," This is because psychic learn support vector machine, support vector machines, while they natively support continuous data, like limit balance and age, they do not natively support categorical data, like marriage, which contains three different categories. Thus, in order to use the categorical data with psychic learn support vector machines,"
2420,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1662.0,1685.0,23.0," we have to use a trick that converts the column of categorical data into multiple columns of binary values. And this trick is called one-hot encoding. So at this point, you may be wondering, what's wrong with treating categorical data like it's continuous? So to answer that question, let's look at an example."
2421,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1685.0,1705.0,20.0," For the marriage columns, we have three options. One married, two single and three other. If we treated those values, one to in three, like continuous data, then we would assume that three, which means other, is more similar to two, which means single than it is to one, which means married."
2422,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1705.0,1727.0,22.0," And that means the support vector machine would be more likely to cluster people with threes and twos together than people with threes and ones together. And in contrast, if we treat these numbers like categorical data, then we will treat each one as a separate category that is no more or less similar to any of the other categories."
2423,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1727.0,1748.0,21.0," Thus, the likelihood of clustering people with twos and threes is the same as clustering threes and ones. And that approach is more reasonable. Note, I have a huge treatise on different ways to do one-hot encoding. There's two very common ways to do it. There's a function called column transformer from scikit-learn."
2424,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1748.0,1767.0,19.0," And there's a function called Git Dummies from Pandas. Because I believe that Git Dummies is better for teaching how one-hot encoding works, we're going to use it. I'll have, or just know there's alternatives, and you should definitely read this description at your leisure to learn about the pros and cons of these two different approaches."
2425,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1767.0,1786.0,19.0," Okay. First, before we commit to converting columns with one-hot encoding, I just want to show you what happens when we convert marriage without saving the results. So to make this easiest C, we're going to use Git Dummies. Git Dummies is a Panda function."
2426,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1786.0,1806.0,20.0," And so we pass it x, capital x. That's the columns that we want to transform, and we specify the columns that we want to transform. For this demonstration, we're just going to transform marriage. And then we're going to print out the first five rows to see what it did. Okay."
2427,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1806.0,1825.0,19.0," Scroll over here, and you'll see that on the left side of the data frame, are the columns that we did not touch. And on the right side, we've got three columns for marriage. We used to just have one that contain three values. Now we have three columns, and each one contains a zero and a one, one,"
2428,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1825.0,1850.0,25.0," in this column, marriage one, there's a one, if there was a one, in the original marriage column there. And a zero otherwise, for marriage two, there's a zero, if, where there's a one, if they originally had a two in the marriage column, and a zero otherwise, and marriage three is one, if they originally had a three in the original column,"
2429,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1850.0,1870.0,20.0, and zero otherwise. So that is how one-hunting coding works. Okay. Now we're going to do it for all of the categorical columns. And then we're going to print out the first five rows just to see what it looks like.
2430,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1870.0,1886.0,16.0," All right. So here is our new data frame. We've got a dot dot dot, specifying that some columns are not shown, but you see that pay these columns have all been converted into new columns. So we have a double bam."
2431,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1886.0,1912.0,26.0," Blast part of formatting the data for a support vector machine is to center and scale the data. The radial basis function that we're going to use assumes that the data are centered in scale, scaled. So in other words, each column should have a mean of zero and a standard deviation of one. So what we're going to do is first what we're going to do is we're going to split the data into training"
2432,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1912.0,1925.0,13.0," and test data set. So we're using train tests split. Again, we're setting the random state so that you could reproduce what I've got. We're passing it in x and coded. That's the one-hotted coded x."
2433,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1925.0,1943.0,18.0," These columns over here that are going to make the predictions. And we're passing in y. That's the thing we want to predict. And we're creating x-trained, x-test, y-trained, and y-test. And we believe the default setting is for 70% of the data to go into the training data set."
2434,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1943.0,1958.0,15.0," And 30% to go into testing. Don't quote me on that. I just believe that's the case. After we do that, we are scaling the data sets using the scale function. Damn."
2435,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1958.0,1976.0,18.0, Right. Moving along. Now we're going to talk about building a pre-liminary support vector machine. We've done a lot of stuff. We've almost been a whole hour just formatting data.
2436,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1976.0,1996.0,20.0," And now we're finally getting to the good part. The way we do that is we call SVC for support vector classifier. We set the random state to 42. And what this does is it kind of makes an untrained shell of a support vector classifier. The next step, and we're saving that shell as classifier under a support vector machine."
2437,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,1996.0,2019.0,23.0," The next step is to call fit using that shell. And so what we're doing is we're fitting it or we're training it on the training data. So we've got x-trained scale and y-trained. And we don't scale y-trained because that's just your own one. And by brain, we'll left my head for a second."
2438,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2019.0,2042.0,23.0," So anyway, so that's correct and we run it. And it just prints out the support vector classifier plus the default settings that it has. And now that we've trained the classifier, we can see how it performs with confusion matrix. And we're going to use the test data set. We're passing in."
2439,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2042.0,2065.0,23.0," We're using plot confusion matrix, we're passing in our train support vector machine. And we're passing in the test. x-t data set and the test y. And below we're just formatting it so it looks pretty. So here is our class, here is our confusion matrix."
2440,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2065.0,2095.0,30.0," It did all right. It did not do great. Of the 257 people that did not default, 79% were correctly classified. Of the 243 people in this row that defaulted 61% were correctly classified. So the support vector machine was not awesome. So we're going to try to improve that using cross validation to optimize the parameters."
2441,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2096.0,2124.0,28.0," We're going to use grid search cross validation or grid search CV. And when we optimize a support vector machine, it's all about finding the best value for gamma and potentially the regularization parameter. C. So what we're going to do is when we use grid search CV, we specify the parameters that we want to try in this. In a matrix."
2442,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2124.0,2155.0,31.0," And so we've got this is the parameter C which is the regularization parameter and we're going to try these values. Note, the regularization parameter has got to be greater than zero. And there's gamma and these are the values we're specifying for gamma. In theory, we could try other kernels if we wanted to and specify those as well. That ends up taking a long time. So we're just going to stick with the radial basis function."
2443,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2155.0,2177.0,22.0," Because typically, it gives us the best performance. Okay, so then we run grid search CV. And we pass in the shell of a support vector classifier. And we pass in the parameters. The CV is the number of folds across cross validation we want to do."
2444,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2177.0,2190.0,13.0, And we pass in the scoring of metric that we want to use. There are lots of options. And I tried a bunch of them. And true to its word. Support vector machines tend to be good out of the box.
2445,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2190.0,2211.0,21.0," And I was trying to find a metric that would give us huge improvements. And I wasn't able to find one. But you can uncomment these and try different metrics. There's even more than I'm using here. Anyways, this will return something we're saving in a variable called optimal parameters."
2446,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2211.0,2228.0,17.0, And then what we do is we run the cross validation on the parameter values by running optimal values. Thit using the training data set. And then we're going to print out the optimal best parameters. So we'll do that. This is going to take a little long.
2447,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2228.0,2246.0,18.0, You'll see this a star here. That means a Python is hard at work doing cross validation for us. When that turns into a number. It'll print out the output down here. And this is one of the reasons why we downscaled the data set.
2448,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2246.0,2258.0,12.0," We went from 30,000 rows to just 2,000. And it still took a little bit of time. I mean, we're not talking hours. But it still took a little bit of time. These are the optimal parameters by the way."
2449,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2258.0,2271.0,13.0, We're done running. We've got a number here. And we can see that the ideal value for C. Because that's a alliteration is 100. Which means that we will use regularization.
2450,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2271.0,2293.0,22.0," And the ideal value for gamma is 0.001. So now we're ready to build a value eight draw and interpret the final support vector machine. So now we're doing exactly what we did before. However, this time we're specifying C equals 100. And gamma equals 0.001."
2451,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2293.0,2313.0,20.0," And then we are. So that creates the shell and then we train it using the training data. We'll run that, bam. And now we're going to print out a new confusion matrix with our new support vector machine. And we're going to run that."
2452,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2313.0,2332.0,19.0," And here's our output. And these are the results, the optimized support vector machine. And they are just a little bit better than before. Four more people were correctly classified as not defaulting. However, one person was incorrectly classified as defaulting."
2453,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2332.0,2354.0,22.0," So I mean, depending on how you want it depending on whether or not it's more important. So we're going to classify these people or these people. We may have improved or we may have slightly gotten worse. But that just tells you again that support vector machines are pretty good out of the box. Optimization didn't help us that much."
2454,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2354.0,2381.0,27.0," Okay, now we can actually draw what the decision boundary is. This is a pretty complicated procedure. And at best, it is an approximation of of what is really going on. And we can quantify how good data approximation is. Mary's ways that we can quantify how good data approximation is going to be."
2455,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2381.0,2410.0,29.0," And we'll go over that really quickly. The first thing we're going to do though is we're going to look at see how many columns in our data set. So we've got 24 columns in our data set and this is a problem because that means it would require a 24 dimensional graph. Or maybe even more because we're using the radio basis function and that operates an infinite dimensions. And since we can't draw an infinite dimension, we have to do data."
2456,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2410.0,2446.0,36.0," We have to collapse the data into two dimensions so we can draw a picture of that. We're going to use principle component analysis to do that. And if you don't know what principle component analysis is right now, just know that what we're doing is we're taking those 24 columns and we're going to shrink them down to two. And then so this is the code for doing that and we're also going to plot what's called a screen plot. And the screen plot tells us how good this approximation of the true classifier is."
2457,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2446.0,2478.0,32.0," The, what we would like is for the first two principle components, the first two columns here to be much taller than all the rest. And that means that those first two components can be or those, those two columns that we're going to use for our new data set would accurately reflect the information. And this case, we don't see that. What we see is that the first principle component, the first column is stands on its own. That's good as we want."
2458,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2478.0,2501.0,23.0," However, the second one is just barely taller than the others. And that's not good. And that tells us that this approximation is not going to be great. That said, I'm also including in this. In the email, I'm going to send out."
2459,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2501.0,2516.0,15.0, You're going to be able to download. How to do support vector machines with this different with this heart date heart disease data set. And in that case. The image is better and actually classification in general is better. So it's so big.
2460,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2516.0,2561.0,45.0," And we're going to run through both of these because you'll get different results and you'll see that how to decide when it's going to be a good sort of collapsing of data and when it's not. The next data is pretty complicated. Next data code chunk, however, just know that what we're doing is retraining and reoptimizing a support vector machine on just those two columns that we collapse the data down to. And that takes this if we're doing cross validation. So this is going to take a minute or two. And while this running, I will address this one question. Yes, we are recording this as a video and I'll put it online and you'll be able to access this later on in your leisure and I'll email you the link."
2461,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2561.0,2587.0,26.0," Okay, so it's done running. We've got optimal parameters notice. We've got different optimal parameters and before that's because we're actually using a different data set instead of all 24 columns we're just using two now. And that's you know, like I said, we're approximating what we did with the 24. Um, columns here, this is when we're actually drawing that decision boundary, lots of code, but also very well commented."
2462,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2587.0,2620.0,33.0," So you can go through this at your leisure. We're right now. We're just going to run it really quickly. And we're going to look at our picture and here it is kind of a mess, but kind of what we expected because the first two principle components which form the. And why access of this graph don't do a great job capturing sort of all the variation that's in the data. And we knew that going into this. So when we get a kind of a messy thing, it's what it is. Okay, so bam."
2463,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2620.0,2656.0,36.0," The pink part of this graph represents the decision area where we if someone falls in there would classify them as not defaulted. The yellow part is where we'll classify people as defaulted. Red dots are from the training data set that are known to have defaulted. So this person has known to be defaulted. They landed there. The green dots are people that are. Oh, red is no default and yes is default. Anyways, that's how to interpret this graph. You can read more about it here."
2464,Support Vector Machines in Python from Start to Finish.,https://www.youtube.com/watch?v=8A7L0GsBiLQ,8A7L0GsBiLQ,2656.0,2688.0,32.0," So in conclusion, we've loaded data from a data file identified and dealt with missing data. We downsampled the data. We formatted the data for support vector machine using one hot encoding. We built a support vector machine for classification. We optimize the support vector machine using cross validation and then we built Drew interpreted and evaluated the final support vector machine. And that gives us a triple bam array. We've made it to the end."
2465,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,0.0,28.04,28.04," Neural networks seem so complicated but then not. StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about neural networks part one inside the black box. Neural networks, one of the most popular algorithms in machine learning, cover a broad"
2466,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,28.04,51.24,23.2," range of concepts and techniques. However, people call them a black box because it can be hard to understand what they're doing. The goal of this series is to take a peek into the black box by breaking down each concept and technique into its components and walking through how they fit together step by step."
2467,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,51.24,73.64,22.40000000000001," In this first part, we will learn about what neural networks do and how they do it. In part two, we'll talk about how neural networks are fit to data with back propagation. Then we will talk about variations on the simple neural network presented in this part, including deep learning. Note."
2468,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,73.64,99.64,26.0," Crazy, awesome news. I have a new way to think about neural networks that will help beginners and seasoned experts alike gain a deep insight into what neural networks do. For example, most tutorials use cool looking but hard to understand graphs and fancy mathematical notation to represent neural networks."
2469,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,99.64,122.52,22.879999999999995," In contrast, I'm going to label every little thing on the neural network to make it easy to keep track of the details. In the math, we'll be as simple as possible while still being true to the algorithm. These differences will help you develop a deep understanding of what neural networks actually do."
2470,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,122.52,152.84,30.320000000000007," So, with that said, let's imagine we tested a drug that was designed to treat an illness and we gave the drug to three different groups of people with three different dosages. Low, medium, and high. The low dosages were not effective, so we set them to zero on this graph. In contrast, the medium dosages were effective, so we set them to one."
2471,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,152.84,177.56,24.72," And the high dosages were not effective, so those are set to zero. Now that we have this data, we would like to use it to predict whether or not a future dosage will be effective. However, we can't just fit a straight line to the data to make predictions, because no matter how we rotate the straight line, it can only accurately predict two of the three"
2472,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,177.56,203.36,25.80000000000001," dosages. The good news is that a neural network can fit a squiggle to the data. The green squiggle is close to zero for low dosages, close to one for medium dosages, and close to zero for high dosages. And even if we have a really complicated data set like this, a neural network can fit a"
2473,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,203.36,229.04,25.67999999999998," squiggle to it. In this stack quest, we're going to use this super simple data set and show how this neural network creates this green squiggle. But first, let's just talk about what a neural network is. A neural network consists of nodes and connections between the nodes."
2474,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,229.04,256.48,27.440000000000023," Note, the numbers along each connection represent parameter values that were estimated when this neural network was fit to the data. For now, just know that these parameter estimates are analogous to the slope and intercept values that we saw for when we fit a straight line to data. Likewise, a neural network starts out with unknown parameter values that are estimated when"
2475,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,256.48,276.6,20.120000000000005," we fit the neural network to a data set using a method called back propagation. And we will talk about how back propagation estimates these parameters in part two in this series. But for now, just assume that we've already fit this neural network to this specific data set."
2476,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,276.6,308.72,32.120000000000005," And that means we have already estimated these parameters. Also, you may have noticed that some of the nodes have curved lines inside of them. These bent or curved lines are the building blocks for fitting a squiggle to data. The goal of this stack quest is to show you how these identical curves can be reshaped by the parameter values and then added together to get a green squiggle that fits the data."
2477,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,308.72,338.96,30.239999999999952," Note, there are many common bent or curved lines that we can choose for a neural network. This specific curved line is called soft plus, which sounds like a brand of toilet paper. Alternatively, we could use this bent line called RLU, which is short for rectified linear unit and sounds like a robot. Or we could use a sigmoid shape or any other bent or curved line."
2478,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,338.96,363.2,24.24000000000001," Oh no, it's the dreaded terminology alert. The curved or bent lines are called activation functions. When you build a neural network, you have to decide which activation function or functions you want to use. In most people teach neural networks, they use the sigmoid activation function."
2479,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,363.2,391.08,27.88000000000005," However, in practice, it is much more common to use the RLU activation function, or the soft plus activation function. So we'll use the soft plus activation function in this stack quest. Anyway, we'll talk more about how you choose activation functions later in this series. Note, this specific neural network is about as simple as they get."
2480,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,391.08,417.8,26.719999999999917," It only has one input node where we plug in the dosage. Only one output node to tell us the predicted effectiveness and only two nodes between the input and output nodes. However, in practice, neural networks are usually much fancier. And have more than one input node, more than one output node, different layers of nodes"
2481,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,417.8,445.16,27.360000000000014," between the input and output nodes, and a spider web of connections between each layer of nodes. Oh no, it's another terminology alert. These layers of nodes between the input and output nodes are called hidden layers. When you build a neural network, one of the first things you do is decide how many hidden layers you want and how many nodes go into each hidden layer."
2482,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,446.44,473.16,26.720000000000027," Although there are rules of thumb for making decisions about the hidden layers, you essentially make a guess and see how well the neural network performs, adding more layers and nodes if needed. Now, even though this neural network looks fancy, it is still made from the same parts, used in this simple neural network, which has only one hidden layer with two nodes."
2483,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,474.84,510.76,35.91999999999996," So let's learn how this neural network creates new shapes from the curved or bent lines in the hidden layer, and then add them together to get a green squiggle that fits the data. Note, to keep the math simple, let's assume dosages go from zero for low to one for high. The first thing we are going to do is plug the lowest dosage zero into the neural network. Now, to get from the input node, to the top node in the hidden layer,"
2484,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,511.72,553.32,41.60000000000008," this connection multiplies the dosage by negative 34.4 and then adds 2.14. And the result is an x-axis coordinate for the activation function. For example, the lowest dosage zero is multiplied by negative 34.4, and then we add 2.14 to get 2.14 as the x-axis coordinate for the activation function. To get the corresponding y-axis value, we plug 2.14 into the activation function,"
2485,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,553.32,577.64,24.319999999999936," which, in this case, is the soft plus function. Note, if we had chosen the sigmoid curve for the activation function, then we would plug 2.14 into the equation for the sigmoid curve. And if we had chosen the real u bent line for the activation function, then we would plug 2.14 into the real u equation."
2486,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,578.8399999999999,607.24,28.40000000000009," But since we are using soft plus for the activation function, we plug 2.14 into the soft plus equation. And the log of 1 plus e raised to the 2.14 power is 2.25. Note, in statistics, machine learning, and most programming languages, the log function implies the natural log or the log base e."
2487,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,607.5600000000001,643.4,35.83999999999992," Anyway, the y-axis coordinate for the activation function is 2.25. So let's extend this y-axis up a little bit and put a blue dot at 2.25 for when dosage equal zero. Now, if we increase the dosage a little bit and plug 0.1 into the input, the x-axis coordinate for the activation function is negative 1.3. And the corresponding y-axis value is 0.24."
2488,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,644.92,675.4,30.480000000000015," So let's put a blue dot at 0.24 for when dosage equal 0.1. And if we continue to increase the dosage values all the way to 1, the maximum dosage, we get this blue curve. Note, before we move on, I want to point out that the full range of dosage values from 0 to 1 corresponds to this relatively narrow range of values from the activation function."
2489,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,676.5999999999999,704.4399999999999,27.840000000000032," In other words, when we plug dosage values from 0 to 1 into the neural network, and then multiply them by negative 34.4 and add 2.14, we only get x-axis coordinates that are within the red box. And thus, only the corresponding y-axis values in the red box are used to make this new blue curve. Bam!"
2490,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,705.7199999999999,743.0,37.280000000000086," Now we scale the y-axis values for the blue curve by negative 1.3. For example, when dosage equal 0, the current y-axis coordinate for the blue curve is 2.4 to 2.5. So we multiply 2.25 by negative 1.3 and get negative 2.93. And negative 2.93 corresponds to this position on the y-axis. Likewise, we multiply all of the other y-axis coordinates on the blue curve by negative 1.3."
2491,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,743.9599999999999,777.0799999999999,33.120000000000005," And we end up with a new blue curve. Bam! Now let's focus on the connection from the input node to the bottom node in the hidden layer. However, this time we multiply the dosage by negative 2.52 instead of negative 34.4. And we add 1.29 instead of 2.14 to get the x-axis coordinate for the activation function."
2492,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,777.32,808.76,31.43999999999994," Remember, these values come from fitting the neural network to the data with back propagation. And we'll talk about that in part 2 in this series. Now, if we plug the lowest dosage 0 into the neural network, then the x-axis coordinate for the activation function is 1.29. Now we plug 1.29 into the activation function to get the corresponding y-axis value."
2493,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,809.8000000000001,834.84,25.039999999999964," And get 1.53. And that corresponds to this yellow dot. Now we just plug in dosage values from 0 to 1 to get the corresponding y-axis values. And we get this orange curve. Note, just like before, I want to point out that the full range of dosage values from 0 to 1"
2494,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,835.8000000000001,868.8399999999999,33.03999999999985," corresponds to this narrow range of values from the activation function. In other words, when we plug dosage values from 0 to 1 into the neural network, we only get x-axis coordinates that are within the red box. And thus, only the corresponding y-axis values in the red box are used to make this new orange curve. So we see that fitting a neural network to data gives us different parameter estimates on the connections,"
2495,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,869.64,897.64,28.0," and that results in each node in the hidden layer using different portions of the activation functions to create these new and exciting shapes. Now, just like before, we scale the y-axis coordinates on the orange curve. Only this time, we scale by a positive number 2.28. And that gives us this new orange curve."
2496,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,899.24,931.0,31.75999999999999," Now the neural network tells us to add the y-axis coordinates from the blue curve to the orange curve. And that gives us this green squiggle. Then, finally, we subtract 0.58 from the y-axis values on the green squiggle. And we have a green squiggle that fits the data. Bam! Now, if someone comes along and says that they are using dosage equal to 0.5,"
2497,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,931.88,996.92,65.03999999999996," we can look at the corresponding y-axis coordinate on the green squiggle and see that the dosage will be effective. Or, we can solve for the y-axis coordinate by plugging dosage equal 0.5 into the neural network and do the math. And we see that the y-axis coordinate on the green squiggle is 1.03. And since 1.03 is closer to 1, then 0, we will conclude that a dosage equal to 0.5 is effective. Double-bound."
2498,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,998.92,1029.08,30.159999999999968," Now, if you've made it this far, you may be wondering why this is called a neural network, instead of a big fancy squiggle-fitting machine. The reason is that way back in the 1940s and 50s when neural networks were invented, they thought the nodes were vaguely like neurons, and the connections between the nodes were sort of like synapses. However, I think they should be called big fancy squiggle-fitting machines,"
2499,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,1029.08,1059.96,30.88000000000011," because that's what they do. Note, whether or not you call it a squiggle-fitting machine, the parameters that we multiply are called weights, and the parameters that we add are called biases. Note, this neural network starts with two identical activation functions, but the weights and biases on the connections slice them, flip them, and stretch them into new shapes. Which are then added together to get a squiggle that is entirely new."
2500,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,1060.92,1089.96,29.039999999999964," And then the squiggle is shifted to fit the data. Now, if we can create this green squiggle with just two nodes in a single hidden layer, just imagine what types of green squiggles we could fit with more hidden layers and more nodes in each hidden layer. In theory, neural networks can fit a green squiggle to just about any data set, no matter how complicated. And I think that's pretty cool. Triple-bound."
2501,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,1090.76,1121.32,30.559999999999945," Now it's time for some, shameless self-promotion. If you want to review statistics and machine learning offline, check out the StacQuest study guides at StacQuest.org. There's something for everyone. Hooray, we've made it to the end of another exciting StacQuest. If you like this StacQuest and want to see more, please subscribe. And if you want to support StacQuest, consider contributing to my Patreon campaign."
2502,Neural Networks Pt. 1: Inside the Black Box,https://www.youtube.com/watch?v=CqOfi41LfDw,CqOfi41LfDw,1121.32,1133.72,12.400000000000093," Becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate, the links are in the description below. All right, until next time, Quest On!"
2503,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,0.0,15.92,15.92," Backpropagation is a really big word, but it's not a really big deal. StatQuest. Hello, I'm Josh Starman. Welcome to StatQuest. Today we're going to talk about neural networks part two."
2504,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,15.92,35.88,19.96," Backpropagation may not be ideas. Note, this StatQuest assumes that you are already familiar with neural networks. The Chain Rule. In gradient descent, if not, check out the quests. The links are in the description below."
2505,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,35.88,57.4,21.52," In the StatQuest on neural networks part one, inside the black box, we started with a simple dataset. That showed whether or not different drug dosages were effective against a virus. The low and high dosages were not effective. But the medium dosage was effective."
2506,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,57.4,87.75999999999999,30.359999999999992," When we talked about how a neural network like this one, fits a green squiggle to this dataset. Remember, the neural network starts with identical activation functions. But, using different weights and biases on the connections, it flips and stretches the activation functions into new shapes, which are then added together to get a squiggle. It was shifted to fit the data."
2507,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,87.75999999999999,112.64,24.88000000000001," However, we did not talk about how to estimate the weights and biases. So let's talk about how backpropagation optimizes the weights and biases in this and other neural networks. Note, backpropagation is relatively simple, but there are a ton of details, so I split it up into bite-sized pieces."
2508,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,112.64,141.44,28.8," In this part, we talk about the main ideas of backpropagation. One, using the chain rule to calculate derivatives and two, plugging the derivatives into gradient descent to optimize parameters. In the next part, we'll talk about how the chain rule and gradient descent apply to multiple parameters simultaneously and introduce some fancy notation."
2509,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,141.44,161.96,20.52000000000001," Then we will go completely bonkers with the chain rule and show how to optimize all seven parameters simultaneously in this neural network. BAM! First, so we can be clear about which specific weights we are talking about. Let's give each one a name."
2510,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,161.96,191.08,29.120000000000005," We have W sub 1, W sub 2, W sub 3, and W sub 4. And let's name each bias. B sub 1, B sub 2, and B sub 3. Note, conceptually, backpropagation starts with the last parameter and works its way backwards to estimate all of the other parameters."
2511,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,191.08,219.96,28.879999999999995," However, we can discuss all of the main ideas behind a backpropagation by just estimating the last bias B sub 3. So in order to start from the back, let's assume that we already have optimal values for all of the parameters except for the last bias term, B sub 3. Note, throughout this and the next stat quests, I'll make the parameter values that have"
2512,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,219.96,243.84,23.879999999999995," already been optimized green. An unoptimized parameters will be red. Also note, to keep the math simple, let's assume dosages go from 0 for low to 1 for high. Now, if we run dosages from 0 to 1, through the connection to the top node in the hidden"
2513,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,243.84,270.92,27.080000000000013," layer, then we get the x-axis coordinates for the activation function that are all inside this red box. And when we plug the x-axis coordinates into the activation function, which, in this example, is the soft plus activation function, we get the corresponding y-axis coordinates, and this blue curve."
2514,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,270.92,296.24,25.319999999999997," Then we multiply the y-axis coordinates on the blue curve by negative 1.22, and we get the final blue curve. Bam. Now, if we run dosages from 0 to 1, through the connection to the bottom node in the hidden layer, then we get x-axis coordinates inside this red box."
2515,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,296.24,322.04,25.799999999999955," Now we plug those x-axis coordinates into the activation function to get the corresponding y-axis coordinates for this orange curve. Now we multiply the y-axis coordinates on the orange curve by negative 2.3, and we end up with this final orange curve. Bam."
2516,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,322.04,356.8,34.76000000000005," Now we add the blue and orange curves together to get this green squiggle. Now we are ready to add the final bias, b-sub 3 to the green squiggle, because we don't get no the optimal value for b-sub 3, we have to give it an initial value. And because bias terms are frequently initialized to 0, we will set b-sub 3 equal to 0. Now, adding 0 to all of the y-axis coordinates on the green squiggle leaves it right where"
2517,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,356.8,378.24,21.44," it is. However, that means the green squiggle is pretty far from the data that we observed. We can quantify how good the green squiggle fits the data by calculating the sum of the squared residuals. A residual is the difference between the observed and predicted values."
2518,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,378.24,408.48,30.24000000000001," For example, this residual is the observed value 0 minus the predicted value from the green squiggle, negative 2.6. This residual is the observed value 1 minus the predicted value from the green squiggle, negative 1.61. Lastly, this residual is the observed value 0 minus the predicted value from the green"
2519,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,408.48,436.68,28.19999999999999," squiggle, negative 2.61. Now we square each residual and add them all together to get 20.4 for the sum of the squared residuals. So in b-sub 3 equals 0, the sum of the squared residuals equals 20.4. And that corresponds to this location on this graph that has the sum of the squared"
2520,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,436.68,465.56,28.879999999999995," residuals on the y-axis and the bias b-sub 3 on the x-axis. Now if we increase b-sub 3 to 1, then we would add 1 to the y-axis coordinates on the green squiggle and shift the green squiggle up 1. And we end up with shorter residuals. When we do the math, bit, bit, bit, bit, bit."
2521,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,465.56,498.24,32.68000000000001," The sum of the squared residuals equals 7.8. And that corresponds to this point on our graph. If we increase b-sub 3 to 2, then the sum of the squared residuals equals 1.11. And if we increase b-sub 3 to 3, then the sum of the squared residuals equals 0.46. And if we had time to plug in tons of values for b-sub 3, we would get this pink curve."
2522,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,498.24,524.36,26.120000000000005," And we could find the lowest point, which corresponds to the value for b-sub 3 that results in the lowest sum of the squared residuals here. However, instead of plugging in tons of values to find the lowest point in the pink curve, we use gradient to send to find it relatively quickly. And that means we need to find the derivative of the sum of the squared residuals with"
2523,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,524.36,548.1999999999999,23.83999999999992," respect to b-sub 3. Now, remember, the sum of the squared residuals equals the first residual squared plus all of the other squared residuals. Now, because this equation takes up a lot of space, we can make it smaller by using summation notation."
2524,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,548.1999999999999,579.48,31.280000000000086," The Greek symbol, sigma, tells us to sum things together. And i is an index for the observed and predicted values that starts at 1. And the index goes from 1 to the number of values in, which in this case is set to 3. So when i equals 1, we are talking about the first residual. When i equals 2, we're talking about the second residual."
2525,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,579.48,608.6800000000001,29.200000000000045," Now when i equals 3, we are talking about the third residual. Now let's talk a little bit more about the predicted values. Each predicted value comes from the green squiggle. And the green squiggle comes from the last part of the neural network. In other words, the green squiggle is the sum of the blue and orange curves plus b-sub 3."
2526,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,608.68,634.64,25.96000000000004," Now remember, we want to use gradient descent to optimize b-sub 3. And that means we need to take the derivative of the sum of the squared residuals with respect to b-sub 3. And because the sum of the squared residuals are linked to b-sub 3 by the predicted values, we can use the chain rule."
2527,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,634.66,666.68,32.01999999999998," To solve the derivative of the sum of the squared residuals with respect to b-sub 3. The chain rule says that the derivative of the sum of the squared residuals with respect to b-sub 3 is the derivative of the sum of the squared residuals with respect to the predicted values. Times the derivative of the predicted values with respect to b-sub 3. Now, before we calculate the derivative of the sum of the squared residuals with respect to the predicted values,"
2528,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,666.68,702.48,35.80000000000007," let's clean up our workspace and move these equations out of the way. Now we can solve for the derivative of the sum of the squared residuals with respect to the predicted values by first substituting in the equation. And then use the chain rule to move the square to the front, and then we multiply that by the derivative of the stuff inside the parentheses with respect to the predicted values, negative 1. Now we simplify by multiplying 2 by negative 1."
2529,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,702.48,734.68,32.19999999999993," And we have the derivative of the sum of the squared residuals with respect to the predicted values. So let's move that up here, and now we are done with the first part. Now let's solve the second part, the derivative of the predicted values with respect to b-sub 3. We start by plugging in the equation for the predicted values. Remember, the blue and orange curves were created before we got to b-sub 3."
2530,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,734.78,764.5799999999999,29.799999999999955," So the derivative of the blue curve with respect to b-sub 3 is 0, because the blue curve is independent of b-sub 3. And the derivative of the orange curve with respect to b-sub 3 is also 0. Lastly, the derivative of b-sub 3 with respect to b-sub 3 is 1. Now we just add everything up, and the derivative of the predicted values with respect to b-sub 3 is 1."
2531,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,765.58,799.7800000000001,34.200000000000045," So we multiply the derivative of the sum of the squared residuals with respect to the predicted values by 1. Note, this times 1 part in the equation doesn't do anything, but I'm leaving it in to remind us that the derivative of the sum of the squared residuals with respect to b-sub 3 consists of 2 parts. The derivative of the sum of the squared residuals with respect to the predicted values, and the derivative of the predicted values with respect to b-sub 3."
2532,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,799.7800000000001,830.38,30.59999999999991," Bam! And at long last, we have the derivative of the sum of the squared residuals with respect to b-sub 3. And that means we can plug this derivative into gradient descent to find the optimal value for b-sub 3. So let's move this equation up, and show how we can use this equation with gradient descent. Note, if you're not familiar with gradient descent, check out the quest."
2533,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,830.38,858.7800000000001,28.40000000000009," The link is in the description below. Anyway, first, we expand the summation. Then we plug in the observed values, and the values predicted by the green squiggle. Remember, we get the predicted values on the green squiggle by running the dosages through the neural network. Now we just do the math and get negative 15.7."
2534,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,858.7800000000001,890.78,31.999999999999886," And that corresponds to the slope for when b-sub 3 equals 0. Now we plug the slope into the gradient descent equation for step size, and in this example, we'll set the learning rate to 0.1. And that means the step size is negative 1.57. Now we use the step size to calculate the new value for b-sub 3 by plugging in the current value for b-sub 3, 0,"
2535,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,890.78,923.78,33.0," and the step size, negative 1.57. And the new value for b-sub 3 is 1.57. Changing b-sub 3 to 1.57 shifts the green squiggle up, and that shrinks the residuals. Now, plugging in the new predicted values and doing the math gives us negative 6.26. Which corresponds to the slope when b-sub 3 equals 1.57."
2536,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,923.78,957.18,33.40000000000009," Then we calculate the step size and the new value for b-sub 3, which is 2.19. Changing b-sub 3 to 2.19 shifts the green squiggle up further, and that shrinks the residuals even more. Now we just keep taking steps until the step size is close to 0. And because the step size is close to 0 when b-sub 3 equals 2.61,"
2537,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,957.18,985.78,28.59999999999991," we decide that 2.61 is the optimal value for b-sub 3. Double bound. So the main idea for back propagation are that when a parameter is unknown like b-sub 3, we use the chain rule to calculate the derivative of the sum of the squared residuals with respect to the unknown parameter, which in this case was b-sub 3."
2538,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,985.78,1007.98,22.19999999999993," Then we initialize the unknown parameter with a number, and in this case we set b-sub 3 equal to 0, and used gradient descent to optimize the unknown parameter. Triple bound. In the next stack-west, we'll show how these ideas can be used to optimize all of the parameters"
2539,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,1007.98,1026.68,18.700000000000045," and a neural network. Now it's time for sum, shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat-quest study guides at statquest.org. There's something for everyone."
2540,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,1026.68,1045.78,19.09999999999991," Hooray! We've made it to the end of another exciting stat-quest. If you like this stat-quest and want to see more, please subscribe. And if you want to support stat-quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie,"
2541,Neural Networks Pt. 2: Backpropagation Main Ideas,https://www.youtube.com/watch?v=IN2XmBhILt4,IN2XmBhILt4,1045.78,1052.98,7.2000000000000455," or just donate. The links are in the description below. Alright, until next time, quest on."
2542,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,0.0,22.96,22.96," The sun is out and it's nice outside, it's the perfect way the full stat quest. Yeah. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about back propagation details, part one. Note, this stat quest assumes that you have already watched neural networks part two"
2543,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,22.96,42.96,20.0," back propagation main ideas. If not, check out the quest. The link is in the description below. In back propagation main ideas, we had this super simple data set that showed whether or not different drug dosages were effective against a virus."
2544,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,42.96,71.2,28.24," Then we had this simple neural network that already had optimal values for all of the parameters except for the last bias term, B sub 3. Then using everything in the neural network except for the last bias, B sub 3, we drew this green squiggle. Then we demonstrated the main ideas behind back propagation by optimizing B sub 3."
2545,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,71.2,102.84,31.639999999999983, We first used the chain rule to calculate the derivative of the sum of the squared residuals with respect to the unknown parameter which in this case was B sub 3. Then we initialized the unknown parameter with a number and in this case we set B sub 3 equal to 0 and used gradient descent to optimize the unknown parameter. We can optimize the last bias term B sub 3.
2546,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,102.84,135.56,32.72000000000001," Now let's pretend we don't know B sub 3's optimal value and start working our way backwards so that along with B sub 3 we optimize the last two weights, W sub 3 and W sub 4. Note the goal of this quest is to learn how the chain rule and gradient descent applies to multiple parameters into introduce sum, fancy notation. In the next part we'll go completely bonkers with the chain rule and learn how to optimize"
2547,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,135.6,165.28,29.680000000000007," all seven parameters in this neural network simultaneously. Bam! So let's go back to not knowing the optimal values for W sub 3, W sub 4 and B sub 3. And just like before, we'll assume that the other weights and biases are already optimized. The first thing we do is initialize the weights, W sub 3 and W sub 4 with random starting values."
2548,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,165.28,198.72,33.44," And in this example, that means we randomly select two values from a standard normal distribution. Then we initialize the last bias, B sub 3 to 0, because bias terms frequently start at 0. Now if we run dosages from 0 to 1, through the connection to the top node in the hidden layer, then just like before, we get the corresponding Y-axis coordinates. And this blue curve."
2549,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,198.72,229.2,30.47999999999999," Now we multiply the Y-axis coordinates on the blue curve by W sub 3, which starts out with the random value 0.36. And we get this new blue curve. Now if we run dosages from 0 to 1, through the connection to the bottom node in the hidden layer, then just like before, we get the corresponding Y-axis coordinates for this orange curve."
2550,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,229.2,256.56,27.360000000000014," Now we multiply the Y-axis coordinates on the orange curve by W sub 4, which starts with the random value 0.63. And we get this new orange curve. Now we add the blue and orange curves together and get this green squiggle. Lastly, since the initial value for B sub 3 is 0, adding it to the Y-axis values on the green"
2551,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,256.56,282.76,26.19999999999999," squiggle does not change anything. In other words, given the current parameters for this neural network, some of which are optimal, and some of which are not optimal, we end up with this green squiggle. Now, just like before, we can quantify how well the green squiggle fits the data by calculating the sum of the squared residuals."
2552,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,282.76,313.08,30.319999999999997," And we get the sum of the squared residuals equals 1.4. Now, even though we have not yet optimized W sub 3 and W sub 4, we can still plot the sum of the squared residuals with respect to B sub 3. And just like before, if we change B sub 3, then we will change the sum of the squared residuals. And that means, just like before, we can optimize B sub 3 by finding the derivative of the"
2553,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,313.08,346.28,33.19999999999999," sum of the squared residuals with respect to B sub 3 and plugging the derivative into the gradient to send algorithm to find the optimal value for B sub 3. And just like before, because the predicted values in the sum of the squared residuals come from the green squiggle and the green squiggle is the sum of the blue and orange curves plus B sub 3, then the sum of the squared residuals are linked to B sub 3 by the predicted values."
2554,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,347.23999999999995,378.44,31.200000000000045," So, by the chain rule, the derivative of the sum of the squared residuals with respect to B sub 3 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to B sub 3. Note, this is the exact same derivative that we calculated in back propagation main ideas. The point of this is that even though we are now optimizing more than one parameter,"
2555,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,379.0,413.56,34.56," the derivatives that we have already calculated with respect to the sum of the squared residuals do not change. Bam! Now let's talk about how to calculate the derivatives of the sum of the squared residuals with respect to the weights to be sub 3 and to be sub 4. Unfortunately, before we can do that, we have to introduce some fancy notation. First, let's remember that the I in this summation notation is an index for the data in the"
2556,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,413.56,449.56,36.0," data set. For example, when I equals 1, we are talking about observed sub 1, which is 0, and we are talking about predicted sub 1, which is 0.72. However, we can also talk about dosage sub i, and when i equals 1, we are talking about dosage sub 1, which is 0. When i equals 2, we're talking about dosage sub 2, which is 0.5, and when i equals 3, we are talking about dosage sub 3, which is 1."
2557,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,450.52,493.24,42.71999999999997," And because dosage sub i is the input value, we call it input sub i. And that means this connection multiplies input sub i by weight W sub 1, which is 3.34. And it adds bias sub 1, which is negative 1.43, to get an x-axis coordinate for the activation function in the top node in the hidden layer. Meanwhile, the other connection multiplies input sub i by weight W sub 2, which is negative 3.53. And adds bias B sub 2, which is 0.57,"
2558,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,494.04,526.36,32.32000000000005," to get an x-axis coordinate for the activation function in the bottom node in the hidden layer. So we have two different x-axis coordinates for input sub i. In order to keep track of things, let's call this x-axis coordinate x sub 1, i. Where the 1 in 1, i refers to the activation function in the top node. And the i in 1, i tells us that it corresponds to input sub i."
2559,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,526.36,559.4799999999999,33.11999999999989," Likewise, let's call this x-axis coordinate x sub 2, i. Where the 2 in 2, i refers to the activation function in the bottom node. And the i in 2, i tells us that it corresponds to input sub i. For example, if i equals 3, then we are talking about the third dosage, dosage sub 3. And that means we're talking about input sub 3, which is 1, the maximum dosage."
2560,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,560.5999999999999,584.2,23.600000000000136," And that means the x-axis coordinate for the activation function in the top node, x sub 1, comma 3, is equal to 1.91. And the x-axis coordinate for the activation function in the bottom node, x sub 2, 3 is equal to negative 2.96. Bam."
2561,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,585.4000000000001,613.64,28.239999999999892," If we plugged in all values for i into dosage sub i, we get x sub 1, i values in this red box. And x sub 2, i values in this red box. Now, in order to get the y-axis coordinates for the activation function in the top node, we plug x sub 1, i into the activation function, which in this example is the soft plus function,"
2562,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,614.28,642.0400000000001,27.760000000000105," and that gives us y sub 1, i. Just like before, the 1 in 1, i tells us that we are talking about the activation function in the top node. And the i tells us which dosage we are talking about. Likewise, in order to get the y-axis coordinates for the activation function in the bottom node, we plug x sub 2, i into the activation function."
2563,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,643.24,663.0,19.75999999999999," And that gives us y sub 2, i. Bam. Now that we understand the fancy notation, we can talk about how to calculate the derivatives of the sum of the squared residuals with respect to the weights, w sub 3, and w sub 4."
2564,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,664.4399999999999,703.48,39.04000000000008," First, remember that y sub 1, i represents the y-axis coordinates for the top activation function. And they form this initial blue curve. However, we get the final blue curve by multiplying the y-axis coordinates, y sub 1, i, by w sub 3. And that means we can plug y sub 1, i times w sub 3 into the equation for the predicted values. Likewise, w sub 4 multiplies the y-axis coordinates y sub 2, i from the bottom activation function"
2565,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,704.52,736.2800000000001,31.760000000000105," to create the final orange curve. And that means we can plug y sub 2, i times w sub 4 into the equation for the predicted values. Now, since this sum creates the green squiggle, and the green squiggle gives us predictions that we evaluate with the sum of the squared residuals, then the sum of the squared residuals are linked to w sub 3 and w sub 4 by the predicted values."
2566,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,737.5600000000001,765.8000000000001,28.24000000000001, That means we can use the chain rule to determine the derivative of the sum of the squared residuals with respect to w sub 3. And with respect to w sigma 4. The chain rule says that the derivative of the sum of the squared residuals with respect to w sub 3 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values
2567,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,765.8,797.9599999999999,32.15999999999997," with respect to w sub 3. Likewise, the derivative with respect to w sub 4 is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to w sub 4. Double ban? Not yet. Note, in both cases, the derivative of the sum of the squared residuals with respect to the predicted values is the exact same as the derivative used for b sub 3."
2568,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,799.4,830.36,30.96000000000004," Just to remind you, we start by substituting the sum of the squared residuals with its equation. Then we use the chain rule to move the square to the front and then we multiply that by the derivative of the stuff inside the parentheses with respect to the predicted values, negative 1. Lastly, we simplify by multiplying 2 by negative 1. And this is the derivative of the sum of the squared residuals with respect to the predicted values."
2569,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,831.48,857.72,26.24000000000001," So we just plug it in. Now, to solve for the derivative of the predicted values with respect to w sub 3, we plug in the equation for the predicted values and the derivative of the first term with respect to w sub 3 is y sub 1 comma i. And the derivatives of the other terms are both zero since they do not contain w sub 3."
2570,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,858.52,886.9200000000001,28.40000000000009," And we end up with just y sub 1 comma i. So we multiply the derivative of the sum of the squared residuals with respect to the predicted values by y sub 1 comma i. Likewise, the derivative of the predicted values with respect to w sub 4 is zero for the first term plus y sub 2 comma i for the second term plus zero for the third term."
2571,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,887.96,909.0,21.039999999999964," Which is just y sub 2 comma i. So we multiply the derivative of the sum of the squared residuals with respect to the predicted values by y sub 2 comma i. Double bound. Now that we have the derivatives of the sum of the squared residuals with respect to w sub 3,"
2572,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,909.8,937.8,28.0," w sub 4 and b sub 3, we can plug them into gradient descent to optimize w sub 3, w sub 4 and b sub 3. First, we initialize w sub 3 and w sub 4 with random values and set b sub 3 equal to zero. Now, starting with the derivative of the sum of the squared residuals with respect to w sub 3."
2573,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,939.16,965.08,25.91999999999996," First, we expand the summation. Then we plug in the observed values and plug in the predicted values from the green squiggle. Remember, we get the predicted values on the green squiggle by running the dosages through the neural network. Now we plug in the y-axis coordinates for the activation function in the top node y sub 1 comma i."
2574,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,966.44,990.92,24.479999999999905," Lastly, we do the math and get 2.58. Likewise, we calculate the derivative of the sum of the squared residuals with respect to w sub 4 and with respect to b sub 3. Now we use the derivatives to calculate the new values for w sub 3. W sub 4."
2575,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,996.92,1020.76,23.840000000000032," And b sub 3. Bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbidu, bubbid passages. Now we repeat that process until the predictions no longer improve very much, or we reach a maximum number of steps, or we meet some other criteria. Now let's check out a fancy animation that shows the gradient descent in action."
2576,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,1021.88,1048.28,26.399999999999977, These gray dots represent the data that we are using to train the neural network. In the orange and blue curves represent the orange and blue curves. In the green squiggle represents the sum of the orange and blue curves plus b sub 3. Now watch how the green squiggle fits the data after 175 steps in gradient descent. Bam!
2577,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,1049.32,1070.6799999999998,21.3599999999999," So, after a bunch of steps, we see how gradient descent optimizes the parameters. Triple Bam! In the next act quest, we'll go totally bonkers with the chain rule and show how to optimize all of the parameters in a neural network simultaneously. Now it's time for some."
2578,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,1071.48,1092.92,21.44000000000005," Shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat quest study guides at statquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe."
2579,Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.,https://www.youtube.com/watch?v=iyn2zdALii8,iyn2zdALii8,1093.56,1111.4,17.840000000000146," And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate, the links are in the description below. All right, until next time, Quest on!"
2580,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,0.0,19.92,19.92," Oh, MJ, let's do the chain rule with me. It's gonna be cool you'll see. StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're gonna talk about back propagation details part two."
2581,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,19.92,47.96,28.04," Note, this StatQuest assumes that you have already seen back propagation details part one. If not, check out the quest. The link is in the description below. Since you've already seen back propagation details part one, you already know that in this StatQuest we're going to go totally bonkers with the chain rule and gradient descent."
2582,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,47.96,70.8,22.84," In order to optimize all of the weights and biases in this neural network so that it can fit a green squiggle to this data. Bam. Note, the derivatives that we derived in part one for bias B sub 3 and weights W sub 3 and W sub 4 do not change."
2583,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,70.8,102.68,31.88000000000001," So we can just plug these derivatives into the gradient descent algorithm. However, now we need to derive the derivatives of the sum of the squared residuals with respect to W sub 1, B sub 1, W sub 2 and B sub 2. Let's start with the derivative of the sum of the squared residuals with respect to W sub 1. Remember, the neural network starts by multiplying input sub i by W sub 1."
2584,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,102.68,133.04000000000002,30.360000000000014, Then it adds the bias B sub 1 and that gives us an x-axis coordinate for the activation function that we call x sub 1 comma i. We then plug x sub 1 comma i into the activation function and that means plugging x sub 1 comma i into this equation because we are using the soft plus activation function. And that gives us a y-axis value of y sub 1 comma i.
2585,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,133.04,164.36,31.319999999999997," Remember y sub 1 comma i times w sub 3 gives us the final blue curve, which we add to the final orange curve and the bias B sub 3 to get the green squiggle and the predicted values. Lastly, remember that we use the predicted values to calculate the sum of the squared residuals. Now let's clear the screen and spread out the equations."
2586,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,164.36,192.8,28.440000000000023," Now we can see that the sum of the squared residuals are linked to W sub 1. First by the predicted values, then y sub 1 comma i, the y-axis values that come from the activation function. And lastly, x sub 1 comma i, the x-axis values that are input for the activation function. Thus, the chain rule tells us that the derivative of the sum of the squared residuals with"
2587,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,192.8,221.1,28.299999999999983," respect to W sub 1, is the derivative of the sum of the squared residuals with respect to the predicted values, times the derivative of the predicted values with respect to y sub 1, times the derivative of y sub 1 with respect to x sub 1, times the derivative of x sub 1 with respect to W sub 1. As we've seen before, the derivative of the sum of the squared residuals with respect"
2588,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,221.1,248.82,27.72," to the predicted values is, the sum of negative 2 times the observed minus the predicted values. Now the derivative of the predicted values with respect to y sub 1 is W sub 3 for the first term, and 0 for the other two terms. So the derivative of the predicted values with respect to y sub 1 is W sub 3."
2589,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,248.82,278.54,29.720000000000027," Now we need to solve for the derivative of y sub 1 with respect to x sub 1. This requires knowing that the derivative of the log of z is 1 divided by z, and the derivative of e to the x is e to the x. The chain rule tells us to take the derivative of the stuff outside of the parentheses, the log function, times the derivative of the stuff inside the parentheses, 1 plus e to"
2590,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,278.54,310.54,32.0," the x. If we let z equal 1 plus e to the x, then the derivative of the stuff outside is 1 divided by 1 plus e to the x, and the derivative of the stuff inside is 0 for the first term, times it does not include x plus e to the x for the second term. And this whole thing simplifies to, e to the x divided by 1 plus e to the x."
2591,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,310.54,341.82,31.279999999999973," Lastly, the derivative of x sub 1 with respect to W sub 1 is the input value for the first term, plus 0 for the second term, because it does not include W sub 1, which simplifies to just the input values. Plugging in the derivatives gives us a big fancy equation, or bf e for short. Hooray, we solved for the derivative of the sum of the squared residuals with respect to"
2592,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,341.82,365.18,23.360000000000014," the first weight, W sub 1. Note, keep in mind that the x's in the e to the x terms are the individual x-axis coordinates for the activation function for each input value. Bam. Now let's find the derivative of the sum of the squared residuals with respect to b sub 1."
2593,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,365.18,394.5,29.319999999999997," The good news is that the exact same equations that linked the sum of the squared residuals to W sub 1 also linked the sum of the squared residuals to b sub 1. Thus, the chain rule tells us that the derivative of the sum of the squared residuals with respect to b sub 1, is the derivative of the sum of the squared residuals with respect to the predicted values times the derivative of the predicted values with respect to y sub 1."
2594,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,394.5,418.58,24.079999999999984," Tons the derivative of y sub 1 with respect to x sub one, times the derivative of x sub 1 with respect to b sub 1. In the derivative of the sum of the squared residuals with respect to the predicted values is the same as before. And so is the derivative of the predicted values with respect to y sub 1."
2595,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,418.58,449.34,30.75999999999999," And the derivative of y sub 1 with respect to x sub 1 is also the same as before. And the only thing different is the derivative of x sub 1 with respect to b sub 1, which is 0 for the first term since it does not include b sub 1 plus 1 for the second term, which simplifies to just 1. Plugging in the derivatives gives us another big fancy equation."
2596,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,449.34,467.98,18.640000000000043, Double bound. We've solved for the derivatives of the sum of the squared residuals with respect to w sub 1 and b sub 1. Now we need to solve for the derivatives of the sum of the squared residuals with respect to w sub 2 and b sub 2.
2597,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,467.98,495.38,27.399999999999977," The good news is that the only difference is that instead of using the top node in the hidden layer, we use the bottom node. And that means now we multiply and put sub i by w sub 2 and add b sub 2. And that gives us x sub 2 comma i, which we plug into the activation function to get y sub 2 comma i."
2598,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,495.38,525.14,29.75999999999999," Then we multiply y sub 2 comma i by w sub 4 to get the final orange curve, which we add to the final blue curve and b sub 3 to get the green squiggle. Then the green squiggle gives us the predicted values, which gives us the residuals and the sum of the squared residuals. Thus, just like before, the sum of the squared residuals are linked to w sub 2."
2599,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,525.14,546.18,21.039999999999964," And we use the chain rule to derive the derivative of the sum of the squared residuals with respect to w sub 2. Now calculating the derivatives is just like before, but with slightly different parameter names. And plugging in the derivatives gives us another b fa."
2600,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,546.18,567.1,20.920000000000076," So in the end, this is the derivative of the sum of the squared residuals with respect to w sub 2. Similarly, we can derive the derivative of the sum of the squared residuals with respect to b sub 2. Now we just combine these derivatives with all of the others."
2601,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,567.1,590.0600000000001,22.96000000000004," And use gradient descent to optimize all of the parameters simultaneously. First, we initialize the weights and biases. In this example, we picked numbers from a standard normal distribution for the weights. Oh no, it's a technical detail alert. I think this is the first time that's ever happened."
2602,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,590.0600000000001,611.0600000000001,21.0," Just so you know, using a standard normal distribution is just one of many ways to initialize weights. Small Bham. Now we initialize the bias terms to zero because bias terms frequently start at zero. Now, starting with the derivative of the sum of the squared residuals with respect to"
2603,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,611.0600000000001,631.7,20.639999999999983," w sub 1, we expand the summation. Then we plug in the observed values. In the values predicted by the green squiggle. Remember, we get the predicted values on the green squiggle by running the dosages through the neural network."
2604,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,631.7,661.14,29.43999999999994," Now we can plug in the current value for w sub 3 and the x-axis coordinates for the activation function in the top node, x of 1 comma i. Lastly, we plug in the input values. Then we do the math and get 0.76. Then we solve for all of the other derivatives."
2605,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,661.14,692.06,30.91999999999996, And then we use each derivative to calculate a step size and a new value. Then we update the parameters in the neural network and repeat until the predictions no longer improve very much or you meet some other criteria. Now let's check out a fancy animation that shows the gradient descent in action. These gray dots represent the data that we are using to train the neural network.
2606,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,692.06,738.3,46.24000000000001," In the orange and blue curves represent the orange and blue curves. In the green squiggle, which fits the data so poorly that it's going off the screen represents the sum of the orange and blue curves plus b sub 3. Now watch how gradient descent fits the green squiggle to the data after 450 steps. We went totally bonkers with the change and gradient descent."
2607,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,738.3,759.1,20.800000000000068," And we finally have a neural network that fits a green squiggle to the data. Triple it back. Now it's time for some shameless self promotion. If you want to review statistics and machine learning offline, check out the stat quest study guides at statquest.org."
2608,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,759.1,775.9,16.799999999999955," There's something for everyone. Whoay! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign."
2609,Backpropagation Details Pt. 2: Going bonkers with The Chain Rule,https://www.youtube.com/watch?v=GKZoOHXGcLo,GKZoOHXGcLo,775.9,788.26,12.360000000000014," Becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right. Until next time, quest on."
2610,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,0.0,27.86,27.86," Some people say, I miss pronounced value, but that is okay. At least it's okay with it if me. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to do neural networks part three, the Raleue Activation function in action."
2611,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,27.86,39.66,11.799999999999995," Note. This Stack Quest assumes that you are already familiar with the main ideas behind neural networks. If not, check out the Quest. The link is in the description below."
2612,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,39.66,72.54,32.88000000000001," In neural networks part one, inside the black box, we started with a simple data set. That showed whether or not different drug dosages were effective against a virus. The low and high dosages were not effective, but the medium dosage was effective. Then we talked about how a neural network like this one that uses the soft plus activation function in the hidden layer can fit a green squiggle to the dataset."
2613,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,72.54,92.74,20.19999999999999," Bam. Now let's see what happens if we swap out the soft plus activation function in the hidden layer with one of the most popular activation functions for deep learning and convolutional neural networks. The Raleue Activation function, which is short for rectified linear unit, and sounds like"
2614,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,92.74,112.26,19.52000000000001," a robot. And as a bonus, because it is common to put an activation function before the final output, we'll do that too. Bam. Remember, to keep the math simple, let's assume dosages go from zero for low to one for"
2615,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,112.26,143.78,31.52," high. So if we plug in the lowest dosage zero, the connection from the input to the top node in the hidden layer, multiplies the dosage by 1.70, and then adds negative 0.85. The result is an x-axis coordinate for the activation function. So if we plug in zero for dosage, then the x-axis coordinate for the activation function"
2616,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,143.78,172.38,28.599999999999994," is negative 0.85. Now we plug negative 0.85 into the Raleue activation function. The Raleue activation function outputs whichever value is larger, zero, or the input value, which in this case is negative 0.85. And because zero is greater than negative 0.85, the output from the Raleue activation function"
2617,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,172.38,194.98,22.599999999999994," is zero. And the corresponding y-axis value is zero. So let's put a blue dot at zero for when dosage equals zero. Now if we increase dosage to zero.2, the x-axis coordinate for the activation function is negative 0.51."
2618,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,194.98,221.86,26.879999999999995," And again, because zero is greater than negative 0.51, the output from the Raleue activation function is zero. And the corresponding y-axis value is zero. So let's put a blue dot at zero for when dosage equals zero.2. And if we increase the dosage value to zero.4, we get zero for the y-axis coordinate."
2619,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,221.86,256.06,34.20000000000002," Again, however, when dosage equals zero.6, the x-axis coordinate is 0.17. And now when we plug zero.17 into the Raleue activation function, the output is zero.17, because zero.17 is greater than zero. And the corresponding y-axis value is zero.17. And if we continue to increase the dosage values all the way to one, the maximum dosage,"
2620,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,256.06,280.82,24.75999999999999," we get this bent blue line. And we multiply the y-axis coordinates on the bent blue line by negative 40.8. And the new bent blue line goes off the screen. Wawwawwaww. Note, for those of you drawing this at home, your final bent blue line will have a steeper"
2621,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,280.82,312.46,31.639999999999983," slope than mine, because, for the sake of clarity, I am not drawing things perfectly to scale. Now when we run dosages through the connection to the bottom node in the hidden layer, we get the corresponding y-axis coordinates that go off the screen for this straight orange line. Now we multiply the y-axis coordinates on the straight orange line by 2.70."
2622,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,312.5,346.06,33.56," And we end up with this final straight orange line. Now we add the bent blue line and the straight orange line together to get this green wedge. Now we add the final bias term, negative 16, to the y-axis coordinates on the green wedge. Lastly, because we included the ROWU activation function right in front of the output, we use the green wedge as its input."
2623,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,346.06,379.5,33.44," For example, the y-axis coordinate for this point on the green wedge is negative 16, which corresponds to this x-axis coordinate for the ROWU activation function. And when we plug that into the ROWU activation function, we get 0, because 0 is greater than negative 16. And 0 corresponds to this green dot. Likewise, the y-axis coordinate for this point on the green wedge is negative 9.2."
2624,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,379.5,412.3,32.80000000000001," And just like before, when we plug that into the ROWU activation function, we get 0. And 0 corresponds to this green dot. Now in contrast to the last two points we fed into the ROWU activation function, this one is positive. And when we plug it into the ROWU activation function, we get 0.49. And 0.49 corresponds to this green dot."
2625,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,412.3,437.9,25.600000000000023," Likewise, the remaining positive y-axis coordinates stay the same. And the negative values are set to 0. And at long last, we end up with this green pointy thing. Double bam. Thus, the ROWU activation function may seem weird because it's not curvy, and the equation is really simple."
2626,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,439.18,461.34,22.159999999999968," But just like for any other activation function, the weights and biases on the connections slice them, flip them and stretch them into new shapes. Which are added together to get an entirely new shape that fits the data. Triple bam. Oh no, it's another technical detail alert."
2627,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,462.3,489.5,27.19999999999999," Some of you may have noticed that the ROWU activation function is bent and not curved. This means that the derivative is not defined where the function is bent. And that's a problem because gradient descent, which we use to estimate the weights and biases, requires a derivative for all points. However, it's not a big problem because we can get around this by simply defining the derivative"
2628,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,489.5,511.74,22.24000000000001," at the bent part to be 0 or 1. It doesn't really matter. Small bam. And now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StacQuest study guides at StacQuest.org. There's something for everyone."
2629,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,512.6999999999999,533.58,20.88000000000011," Hooray, we've made it to the end of another exciting StacQuest. If you like this StacQuest and want to see more, please subscribe. And if you want to support StacQuest, consider contributing to my Patreon campaign, becoming a channel member by one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below."
2630,Neural Networks Pt. 3: ReLU In Action!!!,https://www.youtube.com/watch?v=68BZ5f7P94E,68BZ5f7P94E,534.3,537.58,3.2800000000000864," All right, until next time, Quest Dawn."
2631,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,0.0,29.0,29.0," Way up, north as an island out in the sea. And way out there, they've got neural networks in the cool, stat quest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about neural networks part four, multiple inputs and outputs. Note, this StatQuest was supported in part by Ital."
2632,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,29.0,56.0,27.0," Also, I thought I'd mention that the inspiration for this StatQuest came from my friend Michael in his fall board. Lastly, this StatQuest assumes that you already understand the main idea behind neural networks and the ROWU activation function. If not, check out the quests, the links are in the description below. So far, the neural networks that we've looked at have been super simple"
2633,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,56.0,85.0,29.0," and only predict whether or not the dosage of a drug will be effective. The neural networks just have one input node and one output node. When there is only one input node, then the data we are using to make predictions, in this case, dosages can all fit on the x-axis of this graph. In other words, the input is one dimensional, since it only needs one axis in the graph."
2634,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,86.0,114.0,28.0," Likewise, a single dimension, the y-axis, represents the output values. Combined, we get a two-dimensional graph with the input dosage on the x-axis and the output drug effectiveness on the y-axis. Because the input and output combine to form a two-dimensional graph, we can see how the weights and biases in this neural network slice, flip, and stretch"
2635,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,114.0,137.0,23.0," the curved or bent activation functions into new shapes. That are added together to make a two-dimensional squiggle or shape that fits the data. P-p-p-p-p-p-p-p-p-m. Now let's look at a more complicated network that has more than one input node, and more than one output node."
2636,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,137.0,162.0,25.0," Now, this neural network may look really fancy, but all it does is take two measurements from an iris flower, the width of a petal, which is this part of the flower, and the width of a sepal, which is this part of the flower. And with that information, it predicts the species, either Sittosa, Versicolor, or Virginia."
2637,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,162.0,184.0,22.0," Now, raise your hand if you already knew that this was a sepal and not a petal. Not me, I thought they were all petals. Anyway, to keep things simple at the start, let's begin with both input nodes, but just one output node for Sittosa. Later on, we'll add the other two output nodes,"
2638,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,184.0,204.0,20.0," but for now, let's keep things simple and just use one output node. Now, let's see what happens when we plug values for petal width and sepal width into this simplified neural network. Since we have two inputs and one output, if we're going to draw a graph of what's going on,"
2639,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,204.0,228.0,24.0," then we need a three-dimensional graph. The inputs, petal width, and sepal width, each get an axis. And the output, the prediction for Sittosa, gets the y-axis. Note, to keep the math simple, I scaled the inputs to be between zero for the smallest value, and one for the largest value."
2640,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,228.0,252.0,24.0," So, let's start in this corner, where petal and sepal width equal zero, and plug those values into the neural network. First, let's determine the x-axis coordinate for the top node in the hidden layer. We multiply the petal width by the weight associated with the connection to the top node in the hidden layer, negative 2.5,"
2641,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,252.0,274.0,22.0," and we multiply the sepal width by the weight associated with its connection to the top node in the hidden layer 0.6. Then we add the two terms together, and add the bias 1.6, and that gives us the x-axis coordinate for the activation function, which is 1.6."
2642,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,274.0,308.0,34.0," Now we plug 1.6 into the real-u activation function to get the y-axis coordinate 1.6, because 1.6 is greater than zero, and 1.6 corresponds to this blue point on the graph when petal and sepal widths are both zero. Now let's increase petal width to 0.2, but keep sepal width at zero. Now when we do the math, we get 1.1 for the x-axis coordinate,"
2643,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,308.0,330.0,22.0," and 1.1 for the y-axis coordinate. And 1.1 corresponds to this blue point on the graph. Likewise, when we increase petal width to the maximum value, 1, but keep sepal width at zero, we get these blue dots. Now let's increase sepal width to 0.2,"
2644,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,330.0,353.0,23.0," and run values for petal width from 0 to 1 through the neural network. Likewise, if we keep increasing sepal width to 1, for different values of petal width, we get this blue bent surface. The bend corresponds to the points where the real-u activation function set the y-axis, and set the y-axis values to 0."
2645,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,353.0,378.0,25.0," Now we multiply the y-axis value for each point by negative 0.1. For example, the original y-axis value for this point, when petal and sepal widths are both zero, is 1.6. And 1.6 times negative 0.1 equals negative 0.16, so the final point is here."
2646,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,379.0,405.0,26.0," Likewise, when we multiply all of the other y-axis coordinates by negative 0.1, we get this final blue bent surface. Now we do the exact same thing for the connections to the bottom node in the hidden layer. And we end up with this orange bent surface, where the bend occurs where the real-u activation function set the y-axis values to 0."
2647,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,405.0,439.0,34.0," Then we multiply each y-axis coordinate by 1.5 to get the final orange bent surface. Now we add the y-axis coordinates on the blue bent surface to the y-axis coordinates on the orange bent surface. For example, the y-axis coordinate for this blue point is negative 0.16. And we add the y-axis coordinate for this orange point 1.05. To get 0.89, the y-axis coordinate for this green point."
2648,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,439.0,471.0,32.0," Anyways, we do that for every single point, and ultimately, we end up with this green-crinkled surface. Now, the last thing we do is add the final bias 0 to each y-axis coordinate. And since adding 0 doesn't change the green-crinkled surface, this is the output for Satosa. Bam! Looking at the green-crinkled surface, we see that the value for Satosa is highest when the pedal width is close to 0."
2649,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,471.0,501.0,30.0," And the value for Satosa is lowest when the pedal width is close to 1. Note, remember that we scaled the inputs to be between 0 and 1. And thus, pedal width equals 0 does not imply that the pedal is 0 centimeters wide. Instead, 0 refers to the smallest width in the training dataset. Likewise, one means the largest width in the training dataset."
2650,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,501.0,531.0,30.0," Small bam. Now, to review the concepts so far, when we have two inputs, the neural network creates curved or bent surfaces, that are added together to make a new, crinkled surface. That, in this case, we can use to make predictions about whether or not the species of an iris is Satosa. For example, if we found this iris while walking in the woods, and the scaled pedal width was 0.5,"
2651,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,531.0,570.0,39.0," and the scaled sepal width was 0.37, then we can look at the y-axis value on the green-crinkled surface that corresponds to these measurements, and see that this particular iris is probably not Satosa because the y-axis value is closer to 0 than 1. And this is confirmed when we run the numbers through the neural network, and get 0.09. Bam. Now that we have a green-crinkled surface for Satosa, let's determine the output for the second species, first e-color."
2652,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,570.0,607.0,37.0," Just like before, we'll start with the connections to the top node in the hidden layer. And because the weights and biases are the same as before, we start out with the same blue-bent surface. However, because we will multiply the y-axis coordinates by 2.4, let's change the range of the y-axis from 0 to 2 to negative 6 to 6. Bam. Now, multiplying the y-axis coordinates by 2.4 gives us this final blue-bent surface."
2653,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,607.0,650.0,43.0," Now we create the orange-bent surface from the bottom node in the hidden layer, and multiply the y-axis coordinates on the orange-bent surface by negative 5.2. Now we add the y-axis coordinates from the two bent surfaces together to create this red-crinkled surface. Lastly, we add the final bias 0 to the y-axis coordinates on the red-crinkled surface. And that gives us the final surface for predicting if the iris species is versy-color. Now, I'll admit, it's hard to see what values for petal or sepal widths will give versy-color a high score on this red-crinkled surface."
2654,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,650.0,687.0,37.0," But when we change the y-axis scale from negative 6 to 6 to negative 0.5 to 1, we see that when petal width is close to 0.4, we will get a high value for versy-color. Double-bam. Now, just like we did for Satosa and versy-color, let's determine the crinkled surface for Virginia. Just like before, we start with the blue-bent surface from the top node in the hidden layer. But now we multiply the y-axis coordinates by negative 2.2."
2655,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,687.0,719.0,32.0," And just like before, we create the orange-bent surface from the bottom node in the hidden layer. But now we multiply the y-axis coordinates by 3.7. Now we add the y-axis coordinates from the two bent surfaces together and get this purple-crinkled surface. Lastly, we add the final bias 1 to the y-axis coordinates on the purple-crinkled surface. And that gives us the final surface for predicting if the iris species is virginica."
2656,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,720.0,756.0,36.0," Now, so we can see what's going on. Let's change the scale for the y-axis from negative 6 to 6 to 0 to 1. Now we see that when petal width is close to 1, then we will get a high score for virginica. Triple-bam. At long last, we have crinkled surfaces for Satosa, versy-color, and virginica. Now we can plug in the petal and seep a width from the iris we found and run the numbers through the neural network."
2657,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,756.0,788.0,32.0," And predict that this iris is versy-color because that output value 0.86 is closest to 1. That said, usually when there are two or more output nodes, the output values are sent to either something called argmax arg. Or something called softmax before a final decision is made. And we'll talk about argmax and softmax in the next stack quest in this series. Bam!"
2658,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,788.0,812.0,24.0," Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the stack quest study guides at statquest.org. There's something for everyone. Hey, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe."
2659,Neural Networks Pt. 4: Multiple Inputs and Outputs,https://www.youtube.com/watch?v=83LYR-1IcjA,83LYR-1IcjA,812.0,829.0,17.0," And if you want to support stack quest, consider contributing to my Patreon campaign. Becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, are just donate. The links are in the description below. Alright, until next time, quest on."
2660,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,0.0,35.04,35.04," arg, max, soft, max, stack quest. Hello, I'm Josh Stormer and welcome to StecQuest. Today we're going to talk about neural networks part five, arg, max, and soft max. Note, this stack quest assumes that you already understand the main ideas behind neural networks, the main ideas behind back propagation, and how neural networks work with multiple input and output nodes. If not, check out the quests. The links are in the description"
2661,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,35.04,70.32000000000001,35.28000000000001," below. In the stack quest on neural networks with multiple input and output nodes, we had a fancy neural network that used pedal and seatbel widths to predict Iris species. The neural network made predictions with crinkled surfaces for Satosa, Versy color, and Virginia. Then we plugged in the pedal and seatbel widths from an Iris we found in the woods, and ran the numbers through the neural network, and predicted that this Iris is Versy color,"
2662,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,70.32000000000001,113.28,42.96," because that output value 0.86 is closest to one. Bam, now let's see what happens when pedal width equals 0, and seat width equals 1. When we run the numbers through the neural network, the output values are 1.43 for Satosa, negative 0.4 for Versy color, and 0.23 for Virginia. So, one thing we notice is that the raw output values are not always values between 0 and 1. Sometimes a raw output value can be greater than 1, like in the case for Satosa with 1.43."
2663,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,114.24,152.32,38.08," And sometimes a raw output value can be less than 0, like the case for Versy color, with negative 0.4. This broad range of values makes the raw output harder to interpret than it needs to be. And this is one of the reasons that, when there is more than one output, like we have here, the raw output values are sent to either an arg max layer, or a soft max layer before a final decision is made. arg max, which sounds like something a pirate might say, arg max. Simply sets the largest value to 1,"
2664,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,152.32,183.68,31.360000000000014," and all of the other values to 0. In this example, Satosa has the largest value 1.43. So, the arg max function sets the final output value for Satosa to 1, and the final output values for Versy color and Virginia to 0. Thus, when we use arg max, the neural network's prediction is simply the output with a 1 in it. And that makes the output super easy to interpret. Bam."
2665,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,184.64,222.64,37.99999999999997," The only problem with arg max is that we can't use it to optimize the weights and biases in the neural network. This is because the output values from arg max are constants, 0 and 1. To see why this is a problem, let's plot the second largest output value, 0.23 on a graph. Since this is the second largest value, arg max will output 1 for any other output value that is greater than 0.23, and it will output 0 for any other output value that is less than 0.23."
2666,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,224.32,253.52,29.19999999999999," Because the slopes of these two lines are both 0, their derivatives are also 0. And that means if we wanted to find the optimal value for any of the weights and biases in the neural network, then we would end up plugging 0 into the chain rule for the derivative of arg max, and then the whole derivative would be 0. And if we plug 0 into gradient descent, we won't step towards the optimal parameter values."
2667,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,254.56,277.2,22.639999999999983," And that means we can't use arg max for back propagation. Want, want, want? So that leads us to the softmax function. When people want to use arg max for output, they often use softmax for training. However, first, let me say that softmax sounds like a brand of toilet paper."
2668,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,277.84,302.16,24.32000000000005," But since we're already using a roll of toilet paper to represent the soft plus activation function, let's use a picture of a soft teddy bear to represent the softmax function. Now let's see the softmax function in action. First, let's solve for the softmax output for Satosa. We raise E to the raw output value for Satosa,"
2669,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,303.20000000000005,335.12,31.91999999999996," and divide by the sum of E raised each raw output value. So, in this case, we plug 1.43 in for Satosa, negative 0.4 for Versicolor, and 0.23 for Virginia. And when we do the math, we get 0.69 as the softmax output value for Satosa. So let's put 0.69 here so we don't forget it."
2670,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,335.52,356.32,20.80000000000001," Now let's calculate the softmax output value for Versicolor. When we calculate the softmax value for Versicolor, the only thing that changes is the numerator. Now, instead of E raised to the raw output value for Satosa, we are using the raw output value for Versicolor."
2671,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,357.52,383.12,25.600000000000023," So let's plug in the raw output values, do the math, and we get 0.1 as the softmax output value for Versicolor. Lastly, let's calculate the softmax output value for Virginia. Just like before, the only thing that changes is the numerator. Now, E is raised to the raw output value for Virginia."
2672,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,384.24,408.16,23.920000000000016," So let's plug in the raw output values, do the math, and we get 0.21 as the softmax output value for Virginia. Bam! Now let's take a moment to look at the three softmax output values. First, notice that the largest raw output value, 1.43 for Satosa,"
2673,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,408.88,438.16,29.279999999999973," is paired with the largest softmax output value 0.69. Likewise, the second largest raw output value, 0.23 for Virginia, is paired with the second largest softmax output value 0.21. Lastly, the lowest raw output value, negative 0.4 for Versicolor, is paired with the lowest softmax output value 0.1."
2674,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,439.12,462.3200000000001,23.200000000000045," So we see that the softmax function preserves the original order or ranking of the raw output values. Bam! Second, notice that all three softmax output values are between 0 and 1. This is something that the softmax function ensures. Regardless of how many raw output values there are,"
2675,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,462.3200000000001,481.92,19.59999999999991," the softmax output values will always be between 0 and 1. Double Bam! Third, notice that if we add up all of the softmax output values, then the total is 1. That means that as long as the output values are mutually exclusive,"
2676,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,482.8,509.36,26.56," then the softmax output values can be interpreted as predicted probabilities. Note, I put the word probabilities in quotes because you should not put a lot of trust in their The reason you should not put a lot of trust in the accuracy of these predicted probabilities is that they are, in part, dependent on the weights and biases in the neural network."
2677,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,510.08,535.5999999999999,25.519999999999925," And the weights and biases in turn depend on the randomly selected initial values. And if we change the initial values, we can end up with different weights and biases that give us a neural network that is just as good at classifying the data, but give us different raw output values. And different raw output values give us different softmax output values."
2678,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,536.7199999999999,563.12,26.40000000000009," And that means that different randomly selected initial values for the weights and biases result in different predicted probabilities. In other words, the predicted probabilities don't just depend on the input values but also on the random initial values for the weights and biases. So don't put a lot of trust in the accuracy of these predicted probabilities."
2679,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,564.3199999999999,587.36,23.04000000000008," Small BAM. Now let's go back to the original neural network with the original predicted probabilities and look at the general form of the softmax equation. This I refers to an individual raw output value. For example, when I equals 1, then we are talking about the raw output value for Satosa."
2680,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,588.48,620.24,31.75999999999999," And that means we put Satosa here and we put the raw output value for Satosa in the numerator. In the denominator, we just have the sum of E raised to each raw output value. Now, remember that we started by talking about argmax, which is easy to interpret, but cannot be used for back propagation because its derivative is totally lame. In contrast, softmax has a derivative that can be used for back propagation."
2681,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,621.28,650.48,29.200000000000045," For example, if we have the softmax function for Satosa, then the derivative of the predicted probability with respect to the raw output value for Satosa is the predicted probability for Satosa, times 1 minus the predicted probability for Satosa. Where, in this case, the predicted probability for Satosa equal 0.69. And so when we do the math, we get 0.21."
2682,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,651.6,676.24,24.639999999999983," Note, some of you may wonder where this derivative came from. Some of you may not. For those who want to know, I've created a video that walks you through every single step. So check it out. Bam. Now, because the raw output values for Versicolor and Vergenico play a role in the softmax"
2683,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,676.24,699.68,23.43999999999994," output value for Satosa, we also need the derivatives of the predicted probability for Satosa with respect to Versicolor and Vergenico. The derivative of the predicted probability with respect to the raw output value for Versicolor is the negative predicted probability for Satosa, times the predicted probability for Versicolor."
2684,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,700.64,723.68,23.039999999999964," Where the predicted probability for Satosa equal 0.69 and the predicted probability for Versicolor equal 0.10. And when we do the math, we get negative 0.07. Lastly, the derivative with respect to Vergenica is the negative predicted probability for Satosa,"
2685,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,723.68,745.76,22.08000000000004," times the predicted probability for Vergenica. And when we do the math, we get negative 0.15. Thus, unlike the argmax function, which has a derivative equal to 0, or it's undefined, the derivative of the softmax function is not always 0 and we can use it for gradient descent."
2686,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,746.72,775.52,28.799999999999955," So we see why neural networks with multiple outputs often use softmax for training and use argmax, which has super easy to understand output to classify new observations. Triple BAM. Now, before we go, I need to mention that way back in the stat quest on back propagation main ideas, we use the sum of the squared residuals to determine how well the neural network fit the"
2687,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,775.92,800.16,24.24000000000001," data. However, when we use the softmax function, because the output values are predicted probabilities between 0 and 1, we often use something called cross-inchipy to determine how well the neural network fits the data. And we'll talk about cross-inchipy in the next stat quest in this series. BAM."
2688,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,800.48,815.8399999999999,15.3599999999999," Now it's time for some. Shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat quest study guides at statquest.org. There's something for everyone."
2689,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,817.36,838.64,21.279999999999973," Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below."
2690,Neural Networks Part 5: ArgMax and SoftMax,https://www.youtube.com/watch?v=KpKog-L9veg,KpKog-L9veg,839.28,842.64,3.3600000000000136," All right, until next time, quest on."
2691,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,0.0,24.8,24.8," If you're watching this, then your hardcore stat quest. Hello, I'm Josh Palmer and welcome to stat quest. Today we're going to talk about the softmax derivative, and we're going to go through it step by step. Note, this stat quest assumes that you already understand the main ideas behind softmax."
2692,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,24.8,49.400000000000006,24.600000000000005," If not, check out the quest. The link is in the description below. In the stat quest on softmax, we had this fancy neural network. That predicted if an iris was Satosa, Versicolor, or Virginia. And since the output values were all over the place, we ran them through a softmax layer."
2693,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,49.400000000000006,74.8,25.39999999999999," And I was like, here's the derivative of the softmax for Satosa with respect to the raw output value for Satosa, bam. Now let's solve for that derivative one step at a time. Well, first, remember that the softmax for Satosa, and thus this fraction, is equal to the predicted probability for Satosa."
2694,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,74.8,102.8,28.0," Now, the derivative of the predicted probability requires the ocean rule. Don't worry, I don't expect you to remember the quotient rule. I had to Google it, too. Since we are taking the derivative with respect to the raw output value for Satosa, the quotient rule tells us to take the derivative of the numerator with respect to the raw output value for Satosa."
2695,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,102.8,134.7,31.89999999999999," And the derivative of the numerator, E, raised to the raw output value for Satosa, with respect to the raw output value for Satosa, is E raised to the raw output value for Satosa. Then we just plug in the denominator. Then we subtract the derivative of the denominator with respect to the raw output value for Satosa. Which, again, is just E raised to the raw output value for Satosa, so we plug that in."
2696,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,134.7,162.2,27.5," And then we plug in the numerator. Lastly, we square the original denominator and plug that in. Now let's move the equation up a little bit, and shorten Satosa, Versicolor, and Vergenica, so the equations don't run off the screen. And now just do some algebra. First, since E to the S multiplies the rest of the numerator, we can split it out."
2697,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,162.2,193.2,31.0," Likewise, we can split this numerator into two terms. Now we notice that the first term is equal to the predicted probability for Satosa. So we can replace the first term with the predicted probability for Satosa. And this term has the same thing in the numerator and the denominator, so it is equal to 1. And the last term is also equal to the predicted probability for Satosa, so we plug that in."
2698,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,193.2,222.7,29.5," Bam. So we see that the derivative of the predicted probability for Satosa, with respect to the raw output value for Satosa, is the predicted probability for Satosa, times 1 minus the predicted probability for Satosa. Where, in this case, the predicted probability for Satosa equals 0.69. So, when we do the math, we get 0.21."
2699,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,222.7,267.7,45.0," Now, because the raw output values for Versy color and Versy color play a role in the softmax output value for Satosa, we also need the derivatives of the predicted probability for Satosa with respect to Versy color and Versy. So let's quickly look at how to calculate the derivative with respect to Versy color, and we'll leave Versy for homework. Just like before, remember that the softmax value for Satosa and thus, this fraction is equal to the predicted probability for Satosa. Likewise, the softmax value for Versy color and thus this fraction is equal to the predicted probability for Versy color."
2700,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,267.7,302.7,35.0," Now, just like before, the derivative of the predicted probability for Satosa requires the quotient rule. The big difference this time is that now we are taking the derivatives with respect to Versy color. This difference makes things easier because the derivative of the numerator, e, raised to the raw value for Satosa, with respect to Versy color, is 0, since Versy color is not in the numerator. And that means this whole first term is 0."
2701,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,302.7,334.7,32.0," Then we subtract the derivative of the denominator with respect to Versy color times the numerator, and divide everything by the denominator squared. Now let's shorten Satosa, Versy color, and Versy so the equations don't run off the screen. And now we just do some algebra. First, we split the numerator into two separate terms. And the first term is the same as the negative predicted probability for Satosa."
2702,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,334.7,372.7,38.0," And the second term is the same as the predicted probability for Versy color. Double bam. So we see that the derivative of the predicted probability for Satosa, with respect to the raw output value for Versy color, is the negative predicted probability for Satosa, times the predicted probability for Versy color, where the predicted probability for Satosa equals 0.69, and the predicted probability for Versy color equals 0.1. And when we do the math, we get negative 0.07."
2703,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,372.7,403.7,31.0," Likewise, the derivative with respect to Virginia is the negative predicted probability for Satosa, times the predicted probability for Virginia. And that gives us negative 0.15. Triple bam. Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat quest study guides at statquest.org."
2704,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,403.7,425.7,22.0," There's something for everyone. Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate."
2705,"The SoftMax Derivative, Step-by-Step!!!",https://www.youtube.com/watch?v=M59JElEPgIg,M59JElEPgIg,425.7,431.7,6.0," The links are in the description below. All right. Until next time, quest on."
2706,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,0.0,23.0,23.0," If you made an neural network on the surface of Mars, you'd be a long, long way away from me, that's the truth. Stat Quest. Hello, I'm Josh Starman. Welcome to Stat Quest. Today we're going to talk about neural networks part six, cross entropy."
2707,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,23.0,45.0,22.0," Note, this Stat Quest assumes that you already understand the main ideas behind neural networks, the main ideas behind back propagation, and softmax and argmax. If not, check out the quests. The links are in the description below. Now, before we get started with cross entropy,"
2708,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,45.0,67.0,22.0," let me remind you that in the Stat Quest on back propagation main ideas, we had a simple neural network with a single output that, that, in theory, could give us any output value. In cases like this, we commonly use the sum of the squared residuals to determine how well the neural network fits the data."
2709,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,67.0,97.0,30.0," Bam. Now, when we have a neural network with multiple output values, we often run the data through argmax to make the output easy to interpret. But because argmax has a terrible derivative, we can't use it for back propagation. So, in order to train the neural network, we use the softmax function. And the softmax output values are predicted probabilities between zero and one."
2710,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,97.0,126.0,29.0," And when the output is restricted to values between zero and one, we often use something called cross entropy to determine how well the neural network fits the data. Cross entropy is one of those things that sounds super fancy and complicated, but when it comes to neural networks, is super simple. To see how super simple it is, let's start with this super simple training data set."
2711,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,126.0,170.0,44.0," That has pedal and sepal widths, for known or observed, I respecies. Now let's plug in the pedal and seepowits for the first observed species, setosa. And run the numbers through the neural network and run the raw output values through the softmax function. Now, because we know the data are from setosa, the cross entropy is the negative log base E of the softmax output value for setosa, zero point five seven. In other words, we plug the predicted probability for the observed species into the cross entropy function."
2712,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,170.0,209.0,39.0," Note, if you have seen the cross entropy function before, this version may look different to you. The difference is because neural networks only need a simplified form of this general equation. In this summation, M is the number of output classes. In this case, M equals three because we have three output classes, setosa, versicolor, and virginica. Thus, if we expanded the summation, we would get one term for setosa, one term for versicolor, and one term for virginica."
2713,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,210.0,251.0,41.0," Now, because we know that the data from the first row comes from setosa, the observed probability that the data comes from setosa is one, and the observed probabilities that the data came from versicolor and virginica are both zero. And that means the terms for versicolor and virginica go away. And we are left with negative one times the log of the predicted probability for setosa. Anyway, going back to where we plugged in the predicted probability for setosa, when we do the math, we get zero point five six. Bam!"
2714,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,251.0,314.0,63.0," So let's add the predicted probability for setosa to the table, and the corresponding cross entropy value. Now let's plug in the pedal and seep a widths for the second observed species, virginica, and run the numbers through the neural network, and run the raw output values through the softmax function. Now, because we know the data are from virginica, we plug the predicted probability for virginica zero point five eight into the cross entropy equation. And the cross entropy value for virginica, the second row in the training data, is zero point five four. Likewise, we plug in the measurements on the third row, run the numbers through the neural network, and softmax, and because we know the data are from versicolor, we plug the predicted probability for versicolor zero point five two into the cross entropy equation."
2715,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,314.0,353.0,39.0," And the cross entropy for versicolor is zero point six five. Now, to get the total error for the neural network, all we do is add up the cross entropy values. In this case, we get 1.75 as the total error, and we can use back propagation to adjust the weights and biases and hopefully minimize the total error. Double-bowl. Now, at this point, you might be wondering, if I can calculate these probabilities for each observed species, then I can calculate residuals."
2716,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,353.0,382.0,29.0," The difference between the observed probabilities and the predicted probabilities. For example, for the first row in the data, the observed species is citosa. In thus, the observed probability that the pedal and sepal measurements came from Satosa is 1. And the predicted probability is 0.57. Thus, the residual is 0.43."
2717,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,382.0,409.0,27.0," And if we can calculate a residual, we can square it. And ultimately, that means we can calculate the sum of the squared residuals. You may be wondering why we don't just calculate the squared residuals. Instead of the cross entropy. Well, the first thing we do is remember that the softmax function only gives us values between 0 and 1."
2718,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,409.0,440.0,31.0," And if the prediction for Satosa is really good, it will be close to 1. And if the prediction is really terrible, it will be close to 0. In this case, the prediction for Satosa is kind of in the middle. However, we can just plug in values for the predicted probability from 0 to 1 into the cross entropy function and plot the output. The y-axis is the loss, which is a measure of how bad the prediction is."
2719,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,440.0,472.0,32.0," When we use cross entropy as the prediction gets worse and worse, the loss kind of explodes and gets really, really big. In contrast, if we plug in values for the predicted probability from 0 to 1 into the squared residual, then the change in loss between 0 and 1 is not as large as it is for cross entropy. And you may remember from, neural networks part 2, back propagation main ideas,"
2720,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,472.0,503.0,31.0," that the step size for back propagation depends in part on the derivatives of these functions. And the derivative, or slope of the tangent line for cross entropy, for a bad prediction, will be relatively large. Compared to the derivative for the same bad prediction with squared residuals. So, when the neural network makes a really bad prediction, cross entropy will help us take a relatively large step towards a better prediction."
2721,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,503.0,526.0,23.0," Because the slope of the tangent line will be relatively large. Triple B. Now, with all this talk about back propagation, you're probably dying to see how it is done with cross entropy. My friend, the news is good. The next stack quest is all about how to use cross entropy with back propagation."
2722,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,526.0,549.0,23.0," Bam! Now it's time for some shame or self-promotion. If you want to review statistics and machine learning offline, check out the stack quest study guides at stackquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting stack quest."
2723,Neural Networks Part 6: Cross Entropy,https://www.youtube.com/watch?v=6ArSys5qHAU,6ArSys5qHAU,549.0,570.0,21.0," If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time, quest on."
2724,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,0.0,45.0,45.0," Cross, entropy, derivatives, and back, propagation, so-called stat quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to talk about neural networks part 7, cross entropy derivatives, and back propagation. Note, this stat quest assumes that you already understand the main ideas behind neural networks and back propagation. The main ideas behind neural networks with multiple inputs and outputs and softmax, and the main ideas behind cross entropy."
2725,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,45.0,80.0,35.0," Now, consider this neural network. It takes pedal and simple width measurements as input, and by using a softmax layer at the end, outputs predicted probabilities of the species of iris we measured. And since we're using the softmax function for training, we evaluate how well the neural network fits the data with cross entropy. And that means if we want to optimize parameters with back propagation, we need to take the derivative of the equation for cross entropy,"
2726,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,80.0,122.0,42.0," with respect to the different weights and biases in the neural network. To demonstrate the basic principles behind how we do back propagation with cross entropy, we're going to work through an example that optimizes this bias, B sub 3. But let's start by reviewing how this neural network makes predictions to begin with. First, we put values from 0 to 1 into the inputs for pedal and sepal widths, and these values are modified by weights and biases, and run through an activation function to create this blue bent surface, which is scaled by the weight, negative 0.1."
2727,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,122.0,176.0,54.0," Likewise, we create an orange bent surface with the other activation function, which is scaled by the weight, 1.5. Now we add the blue and orange bent surfaces together to get this green crinkled surface. Lastly, we add a final bias term, B sub 3, and that gives us the final green crinkled surface. In other words, this blue bent surface plus this orange bent surface plus a bias creates a green crinkled surface, and the green crinkled surface represents the raw output for Satosa. Likewise, to create the raw output values for Versicolor, we add a blue bent surface to an orange bent surface, plus a bias to create a red crinkled surface."
2728,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,176.0,238.0,62.0," Finally, to create the raw output values for Virginia, we add a blue bent surface to an orange bent surface, plus a bias to get a purple crinkled surface. Now we run the raw output values through the soft max function, to get the final predicted probabilities for Satosa, Versicolor and Virginia. Bam. For example, if we had this training data with pedal and seat-al-width measurements for Satosa, then we would plug the measurements into the neural network and get these raw output values that are converted by the soft max function into these predicted probabilities. In Thus, 0.57 is the predicted probability for Satosa. Now, to evaluate how good the prediction is, we plug it into the equation for cross entropy, and we get 0.56."
2729,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,239.0,283.0,44.0," Now that we know how to calculate a cross entropy value, let's talk about how changes in his bias, B sub 3, can change the cross entropy value. First, remember that B sub 3 is the bias that finalizes the green crinkled surface. In Thus, B sub 3 finalizes the raw output for Satosa. And by changing B sub 3, and thus the raw output for Satosa, we will change the soft max value for Satosa, because the raw output for Satosa appears in the numerator and denominator. And it will change the soft max value for Versicolor and Virginia, because it appears in those denominators."
2730,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,284.0,330.0,46.0," Now, when we finally plug a predicted probability into the equation for cross entropy, because the observed species is Satosa, the corresponding predicted probability is for Satosa. So we plug this equation into the cross entropy. However, if we had measurements from Virginia, then the soft max equation for Virginia would give us the corresponding predicted probability. In other words, if we have different observed species in the data, then we can end up with different ways to calculate the cross entropy. And because these equations are all a little different, we end up with different derivatives of cross entropy with respect to the bias B sub 3."
2731,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,332.0,367.0,35.0," So in this stat quest, we'll derive these different derivatives for cross entropy. However, you may have noticed that the bottom two derivatives are the same, so I'm actually just going to derive one of these and leave the other for homework. So, let's get started with the derivative of the cross entropy for Satosa with respect to B sub 3. First, let's talk a little bit more about the predicted probability for Satosa. Remember, the predicted probability for Satosa is the output from the soft max function."
2732,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,367.0,401.0,34.0," In the output from the soft max function comes from the raw output values for Satosa, versicolor and Virginia. Of these three crinkled surfaces, Only the green crinkled surface is directly influenced by B sub 3. Remember, the green crinkled surface is the sum of the blue and orange crinkled surfaces, plus B sub 3. Now we want to use gradient descent to optimize B sub 3."
2733,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,401.0,450.0,49.0," And that means we need to take the derivative of the cross entropy with respect to B sub 3. And because the cross entropy is linked to B sub 3 by the predicted probability for Satosa and the raw output for Satosa, we can use the chain. To solve the derivative of the cross entropy with respect to B sub 3, The chain rule says that the derivative of the cross entropy with respect to B sub 3 is the derivative of the cross entropy with respect to the predicted probability for Satosa. Times the derivative of the predicted probability for Satosa with respect to the raw output for Satosa."
2734,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,450.0,497.0,47.0," Now let's clean our workspace and move these equations out of the way. Now we can solve for the derivative of the cross entropy with respect to the predicted probability for Satosa. By first plugging in the equation for the cross entropy, and then just plugging in the derivative of negative 1 times the natural log. And thus the derivative of the cross entropy with respect to the predicted probability for Satosa is negative 1 divided by the predicted probability for Satosa. Now let's solve for the second part, the derivative of the predicted probability for Satosa with respect to its raw output value."
2735,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,497.0,542.0,45.0," We start by plugging in the softmax equation for the predicted probability. And if you remember from neural networks part 5, argmax and softmax, or better yet from the softmax derivative step by step, this derivative is the predicted probability for Satosa times 1 minus the predicted probability for Satosa. If you don't remember this derivative, you can, 1, take my word for it, or 2, check out the quest. Either way, we plug it into the chain rule. Now let's solve for the final derivative, the derivative of the raw output for Satosa with respect to b sub 3."
2736,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,542.0,579.0,37.0," We start by plugging in the equation for the raw output for Satosa. Remember that the blue and orange-bent surfaces were created before we got to b sub 3. So the derivative of the blue-bent surface with respect to b sub 3 is 0, because the blue-bent surface is independent of b sub 3. And the derivative of the orange-bent surface with respect to b sub 3 is also 0. Lastly, the derivative of b sub 3 with respect to b sub 3 is 1."
2737,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,579.0,615.0,36.0," Now we just add everything up, and the derivative of the raw output value for Satosa with respect to b sub 3 is 1. So we multiply the other derivatives by 1. Technically, when the observed data is for Satosa, and thus the projected probability for Satosa is used to calculate the cross entropy, then this product is the derivative of the cross entropy with respect to b sub 3. However, it is usually simplified by multiplying everything by 1."
2738,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,615.0,651.0,36.0," And all that does is make the times 1 term go away. Poof. Then we cancel out these two predicted probabilities for Satosa, and multiply everything by this negative 1. And lastly, we move the negative 1 to the right, so this looks like everyone else's solution. And, at long last, when the predicted probability for Satosa is used to calculate the cross entropy, the derivative of the cross entropy with respect to b sub 3 is the predicted probability for Satosa minus 1."
2739,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,651.0,701.0,50.0," Bam. In summary, when the observed data is for Satosa, and we use the predicted probability, in other words, the softmax output for Satosa, to calculate the cross entropy, then the derivative of the cross entropy with respect to the bias b sub 3 is this, the predicted probability for Satosa minus 1. Now let's see what happens when we calculate the derivative of the cross entropy for Virginia with respect to b sub 3. Remember, the predicted probability for Virginia is the output from the softmax function, and the output from the softmax function comes from the raw output values for Satosa,"
2740,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,701.0,747.0,46.0," Versicolor and Virginia. Of these three crinkled surfaces, only the green crinkled surface is directly influenced by b sub 3. The green crinkled surface represents the raw output for Satosa, and is one of the inputs to the softmax function. And the green crinkled surface is the sum of the blue and orange crinkled surfaces plus b sub 3. Now, because the cross entropy is linked to b sub 3 by the predicted probability for Virginia, and the raw output for Satosa, we can use the chain rule to solve for the derivative of the cross entropy with respect to b sub 3."
2741,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,748.0,793.0,45.0," The chain rule says that the derivative of the cross entropy with respect to b sub 3 is the derivative of the cross entropy with respect to the predicted probability for Virginia, Times the derivative of the predicted probability for Virginia with respect to the raw output for Satosa. Times the derivative of the raw output for Satosa with respect to b sub 3. Now, just like before, we can solve for the derivative of the cross entropy with respect to the predicted probability for Virginia, by plugging in the equation for the cross entropy, and then just plugging in the derivative of negative 1 times the natural log."
2742,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,794.0,828.0,34.0," The derivative of the predicted probability for Virginia with respect to the raw output for Satosa is the negative predicted probability for Satosa times the predicted probability for Virginia. If you don't remember that, check out the quest. Now we just plug it into the chain rule. Lastly, the derivative of the raw output value for Satosa with respect to b sub 3 is 1, just like we got earlier. So we multiply the other derivatives by 1."
2743,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,828.0,877.0,49.0," So, technically, when the observed data is for Virginia, and thus the predicted probability for Virginia is used to calculate the cross entropy, this product is the derivative of the cross entropy with respect to b sub 3. However, it is usually simplified by multiplying everything by 1, poof. Then we cancel out these two predicted probabilities for Virginia, and multiply everything by negative 1. And at long last, when the predicted probability for Virginia is used to calculate the cross entropy, the derivative of the cross entropy with respect to b sub 3 is the predicted probability for Satosa."
2744,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,877.0,920.0,43.0," Double-bound. In summary, when the observed data is for Virginia, and we use the predicted probability, in other words, the softmax output for Virginia, to calculate the cross entropy, then the derivative of the cross entropy with respect to the bias b sub 3 is this, the predicted probability for Satosa. Note, before we move on, I want to mention that this predicted probability for Satosa comes from using the measurements for pedal and sepal widths in the second row of the training data. This difference isn't super important now, but we'll become important later when we do back propagation."
2745,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,920.0,969.0,49.0," Small ban. Now, when the observed measurements are for versy color, then following the same steps we use for Virginia, the derivative is always the same as the previous one. Now, when the observed measurements are for versy color, then following the same steps we use for Virginia, the derivative is also the predicted probability for Satosa. Note, before we move on, I want to remind everyone that b sub 3, the parameter we want to optimize, only directly influences the raw output for Satosa. And that is why these three derivatives are in terms of the predicted probability for Satosa."
2746,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,969.0,1025.84,56.83999999999992," In other words, if we had taken the derivatives with respect to b sub 4, which only directly influences the raw output for versy color, then by the same process we went through earlier, we would end up with different derivatives. Now, going back to the original derivatives with respect to b sub 3, let's use this training data and use these derivatives to optimize the bias b sub 3 with back propagation. First, let's set the bias b sub 3 to some starting value. In this case, we'll use negative 2. Now, just so we can make sure back propagation is improving things, let's calculate the total cross-dip"
2747,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1025.84,1088.84,63.0," and we can plot that on a graph with values for b sub 3 on the x-axis and total cross entropy on the y-axis. Note, if we just plugged in a bunch of values for b sub 3 and calculated the total cross entropy, then we would get this pink curve. Now let's optimize the bias b sub 3 with back propagation to find the value for b sub 3 at the low point in the curve. In other words, let's find the value for b sub 3 that minimizes the total cross entropy. And that starts with the derivative of the cross entropy with respect to b sub 3."
2748,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1088.84,1126.84,38.0," Now, since we have three observations and we'll make one prediction per observation, we expand the summation so that there's one term per prediction. Now let's focus on the prediction made for the first observation. Because the observed species is Satosa, we solve for the cross entropy using the predicted probability for Satosa. And the derivative of the cross entropy for Satosa with respect to b sub 3 is the predicted probability for Satosa minus 1. So we plug that in."
2749,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1126.84,1161.84,35.0," Now we plug the pedal and seep all measurements into the neural network, do the math, and plug the predicted probability for Satosa 0.15 into the equation. Bam! Now let's focus on the prediction for the second observation. Because the observed species is for genico, we solve for the cross entropy using the predicted probability for virginico. And the derivative of the cross entropy for virginica with respect to b sub 3 is the predicted probability for Satosa."
2750,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1161.84,1182.84,21.0," So we plug that in. Now we plug the pedal and seep all measurements into the neural network, do the math, and plug the predicted probability for Satosa 0.04 into the equation. Double bam! Now let's focus on the prediction for the third and final observation."
2751,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1182.84,1215.84,33.0," Because the observed species is for Satosa, we solve for the cross entropy using the predicted probability for virginico. And the derivative of the cross entropy for virginico with respect to b sub 3 is the predicted probability for Satosa. So we plug that in. Now we plug the pedal and seep all measurements into the neural network, do the math, and plug the predicted probability for Satosa 0.04 into the equation."
2752,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1215.84,1241.84,26.0," Now we just add up the derivatives and get negative 0.77. And thus, the slope of this tangent line is negative 0.77. Now we plug the slope into the gradient descent equation for step size, and in this example, we'll set the learning rate to 1. And that means the step size is negative 0.77."
2753,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1241.84,1271.84,30.0," Now we use the step size to calculate the new value for b sub 3, by plugging in the old value for b sub 3, negative 2, and the step size, negative 0.77. And the new value for b sub 3 is negative 1.23. Then we just repeat the process, each time using the new value for b sub 3, until the predictions no longer improve very much, or we reach a maximum number of steps,"
2754,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1271.84,1294.84,23.0," or we meet some other criteria. In this case, the predictions stop improving when b sub 3 equals negative 0.03, so we're done. Triple B. Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline,"
2755,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1294.84,1309.84,15.0," check out the statquest study guides at statquest.org. There's something for everyone. Whoay! We've made it to the end of another exciting statquest. If you like this statquest and want to see more, please subscribe."
2756,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1309.84,1323.84,14.0," And if you want to support statquest, consider contributing to my Patreon campaign, becoming a channel member by one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right."
2757,Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation,https://www.youtube.com/watch?v=xBEh66V9gZo,xBEh66V9gZo,1323.84,1326.84,3.0," Until next time, quest on!"
2758,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,0.0,23.94,23.94," Convolutional neural networks are used for image classification and other stuff. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about neural networks part eight. Image classification with convolutional neural networks."
2759,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,23.94,43.08,19.14," Note, this Stat Quest assumes that you are already familiar with the main ideas behind neural networks. The main idea is behind back propagation. The main idea is behind the RLU activation function. And you should know about neural networks with multiple inputs and outputs."
2760,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,43.08,64.64,21.56," If not, check out the quests. The links are in the description below. Now, imagine we have a friend named StatSquatch. In StatSquatch, once to play TickTackToe against their computer. Unfortunately, StatSquatch can't remember if you start with the letter X or the letter O."
2761,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,64.64,88.08,23.44," So each time StatSquatch plays TickTackToe against their computer, the computer has to figure out if StatSquatch drew the letter X or the letter O. The good news is that the computer can figure out if StatSquatch drew the letter X or O using a convolutional neural network. So, let's talk about the letter O."
2762,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,88.08,121.08,33.0," Because the letter O and the letter X are drawn on a computer screen, we can zoom in on the letter O and the letter X. And see that each image is just a bunch of pixels. And in this case, each pixel is represented by either a zero for a white pixel or a one for a black pixel. So let's walk through step by step how a computer can classify this image as the letter O and this image as the letter X."
2763,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,121.08,150.08,29.00000000000001," We will start with the image of the letter O. Now, because this image is so small, just six pixels by six pixels, it is possible to make a normal, everyday neural network that can correctly classify it. We simply convert this six by six grid of pixels into a single column of 36 input nodes, bit bit bit bit, and connect the input nodes to a hidden layer."
2764,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,150.08,182.08,32.0," So, here we have 36 connections from the 36 input nodes to this node in the hidden layer. Now, remember, each connection has a weight that we have to estimate with back propagation. So that means we need to estimate 36 weights in order to connect to this node. However, usually the first hidden layer has more than one node. And each additional node adds an additional 36 weights that we need to estimate."
2765,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,182.08,218.08,36.0," Like I said, because the original image is small, six by six, and black and white, something like this could work. However, if we had a larger image, like 100 pixels by 100 pixels, which is still pretty small compared to real world pictures, then we would end up having to estimate 10,000 weights per node in the hidden layer. So this method doesn't scale very well. Another problem is that it's not clear that this neural network was still performed well if the image is shifted by one pixel."
2766,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,218.08,259.08000000000004,41.00000000000003," For example, if this is the image we used for training, then it is not clear that the neural network will still recognize this letter O correctly if each pixel is shifted to the right by one. Lastly, even complicated images like this teddy bear, tend to have correlated pixels. For example, any brownish pixel in this image tends to be close to other brown pixels. And any white pixel tends to be near other white pixels. And it might be helpful if we can take advantage of the correlation that exists among each pixel."
2767,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,259.08000000000004,290.08,30.999999999999943," Thus, classification of large and complicated images is usually done using something called a convolutional neural network. Convolutional neural networks do three things to make image classification practical. One, they reduce the number of input nodes. Two, they tolerate small shifts in where the pixels are in the image, and three, take advantage of the correlations that we observe in complex images."
2768,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,290.08,328.08,38.0," So let's go back to our Tic-Tac-Togame, and see how a convolutional neural network can recognize this letter O. The first thing a convolutional neural network does is apply filter to the input image. In convolutional neural networks, a filter is just a smaller square that is commonly three pixels by three pixels. And the intensity of each pixel in the filter is determined by back propagation. In other words, before training a convolutional neural network, we start with random pixel values."
2769,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,328.08,357.08,29.0," And after training with back propagation, we end up with something more useful. To apply the filter to the input image, we overlay the filter onto the image, and then we multiply together each overlapping pixel, bit, bit, bit, and then we add each product together to get a final value, which in this case is three. Oh no, it's the dreaded terminology alert."
2770,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,357.08,380.08,23.0," In fancy math, lingo, we call this sum of products a dot product. By computing the dot product between the input and the filter, we can say that the filter is convolved with the input, and that's what gives convolutional neural networks their name. Small bam. Now we add a bias term to the output of the filter,"
2771,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,380.08,405.08,25.0," and put the final value into something called a feature map. Now, in this example, we slide the filter over one pixel. However, other convolutional neural networks might move over two or more pixels. But in this example, we just move over one pixel, and calculate the dot product of the filter and overlapping pixels in the image."
2772,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,405.08,429.08,24.0," Add the bias term, and put the final value into the feature map. Then we shift the filter over again, and repeat until we have filled up the feature map. Bip, bip, bip, bip, bip, bip, bip, bip. Bam, we filled up the whole feature map. To summarize, we started with an input image of the letter O,"
2773,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,429.08,447.08,18.0," and then applied a filter to it. In other words, we can evolve to the filter with the input, and add it a bias term to the values, and that gave us a feature map. Because each cell in the feature map corresponds to a group of neighboring pixels,"
2774,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,447.08,473.08,26.0," the feature map helps take advantage of any correlations there might be in the image. Bam, now, typically, we run the feature map through a RELU activation function. And that means that all of the negative values are set to zero, and the positive values are the same as before. In this case, that means everything gets set to zero except for these two points."
2775,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,473.08,497.08,24.0," So let's move this stuff over to make some room for the next step. Now we apply another filter to the new feature map. However, unlike before, we simply select the maximum value, and this filter usually moves in such a way that it does not overlap itself. Oh no, it's another dreaded terminology alert."
2776,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,497.08,529.08,32.00000000000006," When we select the maximum value in each region, we are applying max pooling. To see the effect that max pooling has in this example, let's go back to the input image. Here, we see that the upper left hand corner in the input image has an exact match with the filter. And this results in the highest possible value in the feature map. And then the max pooling step reduced this region to the one spot where the filter did the best job matching the input image."
2777,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,529.08,562.08,33.0," In other words, this region and the input image corresponds to this part of the feature map. And the max pooling step selected the spot where the filter did the best job matching the直 out image. Likewise, this region and the input image corresponds to this part of the feature map. And the max pooling step selected the spot where the filter did the best job matching the input image. So we see that max pooling select the spots where the filter did the best job matching the input image."
2778,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,562.08,595.08,33.0," Oh no, it's another terminology alert. Alternatively, we could calculate the average value for each region, and that would be called average or mean pooling. Now, going back to the max pooled layer, let's move and shrink things to give us more room. Now let's convert the pooled layer into a column of input nodes. Lastly, let's plug the input nodes into a normal everyday neural network."
2779,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,595.08,642.08,47.0," This neural network has four input nodes, a single hidden layer with a single node using the RLU activation function, and two output nodes, one for the letter O, and one for the letter X. So, given this image, we run it through the filter to create the feature map, then we run the feature map through a RLU activation function, then we select the max wall value in each area, and end up with these values in the input nodes. Now, we multiply the values in the input nodes by their associated weights, and add each term together, and then add the bias, and we end up with 0.34."
2780,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,642.08,688.08,46.0," And thus, the X-axis coordinate for the activation function is 0.34. Now, we plug 0.34 into the RLU activation function, and the output is 0.34, because 0.34 is greater than 0. Now, this connection from the hidden layer to the output for the letter O gives us 1 for the letter O, and the connection from the hidden layer to the output for the letter X gives us 0 for the letter X. So, when the input is a picture of the letter O, this convolutional neural network classifies it as a picture of the letter O. Bam!"
2781,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,688.08,723.08,35.0," Now, let's see what happens when the input is a picture of the letter X. Note, even though we have changed the input, the filter is the same as before, and we're doing max pooling just like before. And the neural network, with its weights and biases, is the same as before. So, just like before, we run the filter over the input to create the feature map. Now, we run the feature map through the RLU, and do max pooling."
2782,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,723.08,760.08,37.0," And the results of max pooling become the values for the input nodes. Now, we run the values in the input nodes through the neural network, and the result is 0 for the letter O, and 1 for the letter X. So, when the input is a picture of the letter X, this convolutional neural network classifies it as a picture of the letter X. Double-BAM! Now, remember that we said convolutional neural networks help reduce the number of inputs in the neural network."
2783,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,760.08,797.08,37.0," In this case, we started with a 6x6 image, or 36 potential inputs, and can press those down to just 4 inputs into the neural network. We also said that convolutional neural networks take correlations into account, and this is accomplished by the filter, which looks at a region of pixels, instead of just 1 at a time. Lastly, we said that convolutional neural networks can tolerate small shifts in where the pixels are in the image. So, let's see what happens when we shift the picture of the letter X, 1 pixel to the right."
2784,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,797.08,827.08,30.0," In other words, will this convolutional neural network still decide if the input is a picture of the letter X? Better, better, better, better, better, better. And we see that the output value for the letter X, 1.23, is much closer to 1 than the output value for the letter O, negative 0.2. So, this convolutional neural network decided that the input image is of the letter X. Triple-BAM!"
2785,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,827.08,862.08,35.0," Note, if we wanted to, we can make the output easier to interpret by running it through the softmax function, or the argmax function. Also, note, this is about as simple a convolutional neural network as you can get. However, no matter how fancy the convolutional neural network is, it's still based on filters, aka convolution, applying an activation function to the filter output, and pooling the output of the activation function."
2786,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,862.08,887.08,25.0," Now, the next time stat-squatch decides to play tick-tack-toe with their computer. Stats-squatch can start with the letter X, or the letter O, and a convolutional neural network. We'll figure it out. Bam! Now it's time for some shameless self-promotion."
2787,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,887.08,912.08,25.0," If you want to review statistics and machine learning offline, check out the stat-quest study guides at stat-quest.org. There's something for everyone. Whoay! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member,"
2788,Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),https://www.youtube.com/watch?v=HGwBXDKFk9I,HGwBXDKFk9I,912.08,922.08,10.0," buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time, quest on!"
2789,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,0.0,17.68,17.68," Recurrent, neural networks, you just don't roll them. Bam. Stack Quest. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about recurrent neural networks,"
2790,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,17.68,35.36,17.68," and they're going to be clearly explained. Lightning and grid are totally cool. Check them out when you've got some time. Note, this Stack Quest assumes that you were already familiar with the main ideas"
2791,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,35.36,50.84,15.480000000000004," behind neural networks, back propagation, and the real-you activation function. If not, check out the Quest. Also note, although basic or vanilla recurrent neural networks are awesome, they are usually thought of as a stepping stone"
2792,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,50.84,67.04,16.200000000000003," to understanding fancier things, like long, short-term memory networks and transformers, which we will talk about in future Stack Quest. In other words, every Quest worth taking takes steps, and this is the first step."
2793,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,67.04,82.04,15.0," So, with that said, let's say hi to Stack Quest. Hi. In Stack Quest says, hello, the other day I bought Stack an accompanying called Gitrich Quick. But the next day, their Stack price went down,"
2794,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,82.04,93.36,11.319999999999991," and I lost money. Won't war. So, I was thinking, maybe we could create a neural network to predict stock prices. Wouldn't that be cool?"
2795,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,93.36,109.44,16.08," That sure would be cool, Squatch. Unfortunately, the actual Stack Market is crazy complicated, and we'd probably both get in a lot of trouble if we offered advice on how to make money with it. But if we go to that mystical place called Statland,"
2796,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,109.44,124.04,14.600000000000009," things are much simpler and there are far fewer lawyers. So let's build a neural network that predict stock prices in Statland. However, first let's just talk about stock market data in general."
2797,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,124.04,140.64,16.60000000000001," When we look at stock prices, they tend to change over time. For example, the price of this stock went up for four days. Whoay, before going down. Won't war. Also, the longer a company has been traded"
2798,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,140.64,156.72,16.079999999999984," on the stock market, the more data will have for it. For example, we have more time points for the company represented by the blue line, than we have for the company represented by the red line. What that means is, if we want to use a neural network"
2799,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,156.72,172.32,15.599999999999994," to predict stock prices, then we need a neural network that works with different amounts of sequential data. In other words, if we want to predict the stock price for the blue line company on day 10, then we might want to use the data from all nine"
2800,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,172.32,187.28,14.95999999999998," of the preceding days. In contrast, if we wanted to predict the stock price for the red line company on day 10, then we would only have data for the preceding five days. So we need the neural network to be flexible"
2801,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,187.28,203.88,16.600000000000023," in terms of how much sequential data we use to make a prediction. This is a big difference compared to the other neural networks we've looked at in this series. For example, in neural networks clearly explained, we examined a neural network that made predictions"
2802,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,203.88,219.32,15.439999999999998," using one input value, no more and no less. And if you saw the stack quest on neural networks with multiple inputs and outputs, you saw this neural network that made predictions using two input values, no more and no less."
2803,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,220.44,234.6,14.159999999999997," And in the stack quest on deep learning image classification, you saw a neural network that made a prediction using an image that was six pixels by six pixels, no bigger and no smaller. However, now we need a neural network"
2804,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,234.6,249.36,14.76000000000002, that can make a prediction using the nine values we have for the blue company and make a prediction using the five values we have for the red company. The good news is that one way to deal with the problem of having different amounts of input values
2805,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,249.36,265.36,16.0," is to use a recurrent neural network. Just like the other neural networks that we've seen before, recurrent neural networks have weights, biases, layers, and activation functions. The big difference is that recurrent neural networks"
2806,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,265.36,280.72,15.359999999999957," also have feedback loops. And although this neural network may look like it only takes a single input value, the feedback loop makes it possible to use sequential input values like stock market prices collected over time"
2807,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,280.72,298.84,18.120000000000005," to make predictions. To understand how, exactly, this recurrent neural network can make predictions with sequential input values, let's run some of stat-lans stock market data through it. In stat-land, if the price of a stock is low for two days"
2808,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,298.84,316.48,17.639999999999986," in a row, then more often than not, the price remains low on the next day. In other words, if yesterday and today's stock prices low, then tomorrow's price should also be low. In contrast, if yesterday's price was low"
2809,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,316.48,333.0,16.519999999999982," and today's price is medium, then tomorrow's price should be even higher. And when the price decreases from high to medium, then tomorrow's price will be even lower. Lastly, if the price stays high for two days in a row,"
2810,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,333.0,348.44,15.439999999999998," then the price will be high tomorrow. Now that we see the general trends in stock prices in stat-land, we can talk about how to run yesterday and today's data through a recurrent neural network to predict tomorrow's price."
2811,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,348.44,365.32,16.879999999999995, The first thing we'll do is scale the prices so that low equals zero. Medium equals 0.5 and high equals one. Now let's run the values for yesterday and today through this recurrent neural network
2812,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,365.32,382.48,17.160000000000025," and see if it can correctly predict tomorrow's value. Now, because the recurrent neural network has a feedback loop, we can enter yesterday and today's values into the input sequentially. We'll start by plugging yesterday's value into the input."
2813,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,382.48,395.48,13.0," Now we can do the math just like we would for any other neural network. Beep, beep, beep, beep, beep, beep, beep, beep. At this point, the output from the activation function, the y-axis coordinate that will call y-sub-one,"
2814,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,395.48,412.96,17.47999999999996," can go two places. First, y-sub-one can go towards the output. And if we go that way and do the math, beep, beep, beep, beep. Then the output is the predicted value for today. However, we're not interested in the predicted value for today"
2815,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,412.96,431.92,18.95999999999998," because we already have the actual value for today. Instead, we want to use both yesterday and today's value to predict tomorrow's value. So for now, we'll ignore this output. And instead, focus on what happens with this feedback loop."
2816,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,431.92,447.76,15.840000000000032," The key to understanding how the feedback loop works is this summation. The summation allows us to add y-sub-one, time w-sub-2, which is based on yesterday's value, to the value from today times w-sub-one."
2817,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,447.76,467.04,19.279999999999973," In other words, the feedback loop allows both yesterday and today's values to influence the prediction. Hey, this feedback loop has got me all turned around. Is there an easier way to see how this works? Yes, there's an easier way to see what's going on."
2818,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,467.04,487.72,20.680000000000064," Instead of having to remember which value is in the loop and which value is in the input, we can unroll the feedback loop by making a copy of the neural network for each input value. Now, instead of pointing the feedback loop to the sum in the first copy, we can point it to the sum in the second copy."
2819,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,487.72,506.96,19.239999999999952," By unrolling the recurrent neural network, we end up with a new network that has two inputs and two outputs. The first input is for yesterday's value. And if we do the math straight through to the first output like we did earlier, we get the predicted value for today."
2820,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,506.96,527.36,20.400000000000038," However, as we saw earlier, we can ignore this output. The second input is for today's value. And the connection between the first activation function and the second summation allows both yesterday and today's values to influence the final output, which gives us the predicted value"
2821,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,527.36,547.0,19.639999999999983," for tomorrow. Now, when we put yesterday's value into the first input, and we do the math just like before, then we follow the connection from the first activation function to the summation in the second copy of the neural network."
2822,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,547.0,571.08,24.08000000000004," Now we put today's value into the second input and keep doing the math. And that gives us the predicted value for tomorrow, zero, which is consistent with the original observation. In other words, the recurrent neural network"
2823,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,571.08,585.36,14.279999999999973," correctly predicted tomorrow's value. Bam! Likewise, when we run yesterday and today's values for the other scenarios through the recurrent neural network, we predict the correct values for tomorrow."
2824,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,586.44,599.28,12.839999999999918, Double bam! This recurrent neural network performs great with two days worth of data. But what if we have three days of data? When we want to use three days of data to make a prediction
2825,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,599.28,614.12,14.840000000000032," about tomorrow's price like this, then we just keep unrolling the recurrent neural network until we have an input for each day of data. Then we plug the values into the inputs, always from the oldest to the newest."
2826,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,614.12,627.24,13.120000000000005," In this case, that means we start by plugging in the value for the day before yesterday. Then we plug in yesterday's value, and then we plug in today's value. And when we do the math, the last output"
2827,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,627.24,640.5200000000001,13.280000000000086," gives us the prediction for tomorrow. Bam! Note, regardless of how many times we unroll a recurrent neural network, the weights and biases are shared across every input."
2828,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,640.5200000000001,660.84,20.319999999999936," In other words, even though this unrolled network has three inputs, the weight, W sub 1, is the same for all three inputs. And the bias, B sub 1, is also the same for all three inputs. Likewise, all of the other weights and biases are shared."
2829,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,660.84,676.32,15.480000000000018," So, no matter how many times we unroll a recurrent neural network, we never increase the number of weights and biases that we have to train. Triple Bam! Okay, now that we've talked about what makes basic recurrent"
2830,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,676.32,693.12,16.799999999999955," network so cool, let's briefly talk about why they are not used very often. One big problem is that the more we unroll a recurrent neural network, the harder it is to train. This problem is called the vanishing slash exploding gradient"
2831,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,693.12,704.9200000000001,11.800000000000068," problem, which is also known as the, hey, wait, where the gradient go? Kaboom! Problem. In our example, the vanishing slash exploding gradient"
2832,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,704.9200000000001,719.9200000000001,15.0," problem has to do with the weight along the squiggle that we copy each time we unroll the network. Note, to make it easier to understand the vanishing slash exploding gradient problem, we're going to ignore the other weights and biases in this network"
2833,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,719.9200000000001,735.72,15.799999999999956," and just focus on W sub 2. Also, just to remind you, when we optimize neural networks with back propagation, we first find the derivatives or gradients for each parameter. We then plug those gradients into the gradient"
2834,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,735.72,747.9200000000001,12.200000000000044," that we just sent algorithm to find the parameter values that minimize a loss function, like the sum of the squared residuals. Bam! Now, even though the vanishing slash exploding gradient"
2835,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,747.9200000000001,762.9200000000001,15.0," problem starts with vanishing, we're going to start by showing how a gradient can explode. Kaboom! In our example, the gradient will explode when we set W sub 2 to any value larger than 1."
2836,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,762.92,782.88,19.96000000000004," So let's set W sub 2 equal to 2. Now, the first input value, input sub 1, will be multiplied by 2 on the first squiggle and then multiplied by 2 on the next squiggle and again on the next squiggle and again on the last squiggle."
2837,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,782.88,799.88,17.0," In other words, since we unroll the recurrent neural network four times, we multiply the input value by W sub 2, which is 2 raised to the number of times we unrolled, which is 4. And that means the first input values amplified"
2838,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,799.88,821.1999999999999,21.319999999999936," 16 times before it gets to the final copy of the network. Now, if we had 50 sequential days of stock market data, which, to be honest, really isn't that much data, then we would unroll the network 50 times. And 2 raised to the 50 power is a huge number,"
2839,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,821.2,837.08,15.879999999999995," and this huge number is why they call this an exploding gradient problem. Kaboom. If we tried to train this recurrent neural network with back propagation, this huge number would find its way into some of the gradients."
2840,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,837.08,852.84,15.759999999999993," And that would make it hard to take small steps to find the optimal weights and biases. In other words, in order to find the parameter values that give us the lowest value for the loss function, we usually want to take relatively small steps."
2841,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,852.84,867.48,14.639999999999986," Bam, however, when the gradient contains a huge number, then we'll end up taking relatively large steps. And instead of finding the optimal parameter, we'll just bounce around a lot. Qua, waaam."
2842,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,868.6,885.7199999999999,17.11999999999989," One way to prevent the exploding gradient problem would be to limit W sub 2 to values less than 1. However, this results in the vanishing gradient problem. Hey, wait, where did the gradient go? To illustrate the vanishing gradient problem,"
2843,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,885.7199999999999,904.88,19.16000000000008," let's set W sub 2 to 0.5. Now, just like before, we multiply the first input by W sub 2 raised to the number of times we unroll the network. So if we have 50 sequential input values, that means multiplying input sub 1 by 0.5"
2844,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,904.88,923.88,19.0," raised to the 50th power. And 0.5 raised to the 50th power is a number super close to 0. Because this number is super close to 0, this is called the vanishing gradient problem. Now, when optimizing a parameter, instead of taking steps"
2845,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,923.88,941.24,17.360000000000014," that are too large, we end up taking steps that are too small. And as a result, we end up hitting the maximum number of steps we're allowed to take before we find the optimal value. Waw, waw. Hey, Josh, these vanishing slash exploding gradients"
2846,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,941.24,951.2,9.959999999999924, are a total bummer. Is there anything we could do about them? Yes. And we'll talk about a popular solution called long short-term memory networks
2847,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,951.2,962.8,11.600000000000025," in the next stack quest. Kabam. Now it's time for some. Shameless self-promotion. If you want to review statistics and machine learning offline,"
2848,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,962.8,979.8,17.0," check out my book, the Stackwest Illustrated Guide to Machine Learning at Stackwest.org. It's over 300 pages of total awesomeness. Hooray, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more,"
2849,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,979.8,991.48,11.680000000000064," please subscribe. And if you want to support Stackwest, consider contributing to my Patreon campaign. Becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate."
2850,"Recurrent Neural Networks (RNNs), Clearly Explained!!!",https://www.youtube.com/watch?v=AsNTP8Kwu80,AsNTP8Kwu80,991.48,996.92,5.439999999999941," The links are in the description below. All right, until next time, Quest On."
2851,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,0.0,24.2,24.2," Long, short, term, memories. I've got them both and so does this network, hooray. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about long, short term memory, LSTM,"
2852,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,24.2,36.2,12.000000000000004," and it's going to be clearly explained. Let it in. Yeah. Going to deploy a model in just a few days, not once. Yeah."
2853,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,36.2,54.6,18.4," This Stat Quest is also brought to you by the letters A, B and C. A, always B, B, C, curious. Always B, curious. Note, this Stat Quest assumes you are already familiar with recurrent neural networks and the vanishing exploding gradient problem."
2854,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,54.6,78.12,23.520000000000003," If not, check out the quest. Also note, although long, short term memory is totally awesome, it is also a stepping stone to learning about transformers, which we will talk about in future Stat Quest. In other words, today we're taking the second step in our quest. Now, in the Stat Quest on Basic, vanilla recurrent neural networks, we saw how we can use"
2855,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,78.12,103.4,25.279999999999987," a feedback loop to unroll a network that works well with different amounts of sequential data. However, we also saw that when we plug in the numbers, when the weight on the feedback loop is greater than 1, and in this example, the weight is 2, then when we do the math, we end up multiplying the input by the weight, which in this case is 2, raised to the"
2856,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,103.4,121.4,18.0," number of times we unroll the network. And thus, if we had 50 sequential data points, like 50 days of stock market data, which isn't really that much. Then we would raise 2 by 50. And 2 to the 50th power is a huge number."
2857,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,121.4,142.95999999999998,21.559999999999988," And this huge number would cause the gradient, which we need for gradient descent, to explode. Kaboom. Alternatively, we saw that if the weight on the feedback loop was less than 1, and now we have it set to 0.5, then we'll end up multiplying the input value by 0.5 raised to the 50th"
2858,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,142.96,159.16,16.19999999999999," power. And 0.5 raised to the 50th power is a number super close to 0. And this number super close to 0 would cause the gradient, which we need for gradient descent, to vanish. Poof."
2859,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,159.16,178.32,19.16," In summary, basic vanilla recurrent neural networks are hard to train because the gradients can explode, kaboom, or vanish. Poof. The good news is that it doesn't take much to extend the basic vanilla recurrent neural network so that we can avoid this problem."
2860,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,178.32,198.44,20.120000000000005," So today, we're going to talk about long short term memory, LSTM, which is a type of recurrent neural network that is designed to avoid the exploding slash vanishing gradient problem. Pray. The main idea behind how long short term memory works is that instead of using the same"
2861,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,198.44,223.56,25.120000000000005," feedback loop connection for events that happened long ago, and events that just happened yesterday to make a prediction about tomorrow, long short term memory uses two separate paths to make predictions about tomorrow. One path is for long term memories, and one is for short term memories. Bam."
2862,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,223.56,247.84,24.28," Now that we understand the main idea behind long short term memory that it uses different paths for long and short term memories, let's talk about the details. The bad news is that compared to a basic vanilla recurrent neural network, which unrolls from a relatively simple unit, long short term memory is based on a much more complicated unit."
2863,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,247.84,260.36,12.52000000000001," Holy smokes. This looks really complicated. Don't worry, Squatch. We'll go through this one step at a time so that you can easily understand each part. Bam."
2864,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,260.36,283.56,23.19999999999999," Note. Unlike the networks we've used before in this series, long short term memory uses sigmoid activation functions, and tan-h activation functions. So let's quickly talk about sigmoid and tan-h activation functions. In a nutshell, the sigmoid activation function takes any x-axis coordinate and turns it into"
2865,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,283.56,319.24,35.68000000000001," a y-axis coordinate between 0 and 1. For example, when we plug in this x-axis coordinate 10 into the equation for the sigmoid activation function, we get 0.99995 as the y-axis coordinate. And if we plug in this x-axis coordinate negative 5, then we get 0.01 as the y-axis coordinate. In contrast, the tan-h or hyperbolic tangent activation function takes any x-axis coordinate"
2866,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,319.24,350.16,30.920000000000016," and turns it into a y-axis coordinate between negative 1 and 1. For example, if we plug this x-axis coordinate 2 into the equation for the tan-h activation function, we get 0.96 as the y-axis coordinate. And if we plug in this x-axis coordinate negative 5, we get negative 1 as the y-axis coordinate. So now that we know that the sigmoid activation function turns any input into a number between"
2867,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,350.16,377.08,26.91999999999996," 0 and 1, and the tan-h activation function turns any input into a number between negative 1 and 1, let's talk about how the long-short term memory unit works. First, this green line that runs all the way across the top of the unit is called the cell state and represents the long-term memory. Although the long-term memory can be modified by this multiplication, and then later by"
2868,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,377.08,405.76,28.680000000000007," this addition, you'll notice that there are no weights and biases that can modify it directly. This lack of weights allows the long-term memories to flow through a series of unrolled units without causing the gradient to explode or vanish. Now this pink line, called the hidden state, represents the short-term memories. And as we can see, the short-term memories are directly connected to weights that can modify"
2869,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,405.76,426.76,21.0," them. To understand how the long- and short-term memories interact and result in predictions, let's run some numbers through this unit. First, for the sake of making the math interesting, let's just assume that the previous long-term memory is 2, and the previous short-term memory is 1, and let's set the input"
2870,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,426.76,451.2,24.44," value to 1. Now that we have plugged in some numbers, let's do the math to see what happens in the first stage of the long-short-term memory unit. We'll start with the short-term memory, 1, times its weight, 2.7. Then we multiply the input, 1, by its weight, 1.63, and then we add those two terms together."
2871,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,451.2,481.16,29.95999999999998," And lastly, we add this bias, 1.62. To get 5.95, an x-axis coordinate for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation function, and we get the y-axis coordinate, 0.997. Lastly, we multiply the long-term memory, 2, by the y-axis coordinate, 0.997, and the result"
2872,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,481.16,503.72,22.56," is 1.99. So this first stage of the long-short-term memory unit reduced the long-term memory by a little bit. In contrast, if the input to the LSTM was a relatively large negative number, like negative 10, then after calculating the x-axis coordinate, the output from the sigmoid activation"
2873,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,503.72,527.12,23.400000000000038," function will be 0. And that means the long-term memory would be completely forgotten, because anything multiplied by 0 is 0. Thus, because the sigmoid activation function turns any input into a number between 0 and 1, the output determines what percentage of the long-term memory is remembered."
2874,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,527.12,547.52,20.399999999999977," To summarize, the first stage in a long-short-term memory unit determines what percentage of the long-term memory is remembered. Bam! Oh no, it's the dreaded terminology alert. Even though this part of the long-short-term memory unit determines what percentage of the long-term"
2875,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,547.52,569.68,22.159999999999968," memory will be remembered. It is usually called the forget gate, small bam. Now that we know with the first part of the LSTM unit does, it determines what percentage of the long-term memory will be remembered. Let's go back to when the input was one, and talk about what the second stage does."
2876,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,569.68,591.2,21.520000000000092," In a nutshell, the block on the right combines the short-term memory and the input to create a potential long-term memory. Then a block on the left determines what percentage of that potential memory to add to the long-term memory. So let's plug the numbers in and do the math to see how a potential memory is created and"
2877,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,591.2,613.3199999999999,22.11999999999989," how much of it is added to the long-term memory. Starting with the block furthest to the right, we multiply the short-term memory and the input by the respective weights. Then we add those values together. Then add a bias term to get 2.03, the input value for the tan-h activation function."
2878,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,613.3199999999999,642.8399999999999,29.519999999999985," Now we plug 2.03 into the equation for the tan-h activation function and we get the y-axis coordinate 0.97. Remember, the tan-h activation function turns any input into a number between negative 1 and 1. And in this case, when the input to the LSTM is 1, then after calculating the x-axis coordinate, the tan-h activation function gives us an output close to 1."
2879,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,642.8399999999999,672.88,30.04000000000008," In contrast, if the input to the LSTM was negative 10, then after calculating the x-axis coordinate, the output from the tan-h activation function would be negative 1. Going back to when the input to the LSTM was 1, we have a potential memory 0.97 based on the short-term memory and the input. Now the LSTM has to decide how much of this potential memory to save."
2880,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,672.88,697.76,24.879999999999995," And this is done using the exact same method we used earlier when we determined what percentage of the long-term memory to remember. In other words, after multiplying the short-term memory and the input by weights and adding those products together and adding a bias, we get 4.27, the x-axis input value for the sigmoid activation function."
2881,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,697.76,721.52,23.75999999999999," Now we plug the x-axis coordinate into the equation for the sigmoid activation function, and we get the y-axis coordinate 1.0. That means the entire potential long-term memory is retained because multiplying it by 1 doesn't change it. Note, if the original input value was negative 10, then the percentage of the potential"
2882,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,721.52,745.0,23.480000000000015," memory to remember would be 0. So we would not add anything to the long-term memory. Now going back to when the original input value was 1, we add 0.97 to the existing long-term memory, and we get a new long-term memory 2.96. Double-band."
2883,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,745.0,766.0,21.0," Oh no, it's the dreaded terminology alert. Even though this part of the long-short-term memory unit determines how we should update the long-term memory, it's usually called the input gate. Tiny-band. Now that we have a new long-term memory, we're ready to talk about the final stage in the"
2884,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,766.0,791.6,25.600000000000023," LSTL. This final stage updates the short-term memory. We start with the new long-term memory and use it as input to the tan-h activation function. In after plugging 2.96 into the tan-h activation function, we get 0.99. 0.99 represents a potential short-term memory."
2885,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,791.6,820.36,28.75999999999999," Now the LSTLm has to decide how much of this potential short-term memory to pass on. And this is done using the exact same method we used two times earlier. When we determined what percentage of the original long-term memory to remember, and when we determined what percentage of the potential long-term memory to remember. In all three cases, we use a sigmoid activation function to determine what percent the LSTL"
2886,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,820.36,850.2,29.840000000000032," remember. In this case, when we do the math, we get 0.99. And we create the new short-term memory by multiplying 0.99 by 0.99 to get 0.98. This new short-term memory, 0.98, is also the output from this entire LSTM unit. Oh no, it's the dreaded terminology alert again."
2887,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,850.2,872.12,21.91999999999996," Because the new short-term memory is the output from this entire LSTM unit, this stage is called the output gate. And, at long last, the common terminology seems reasonable to me. Triple-bound. Now that we understand how all three stages in a single LSTM unit work, let's see them"
2888,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,872.12,894.0,21.879999999999995," in action with real data. Here, we have stock prices for two companies, company A and company B. On the Y-axis, we have the stock value. And on the X-axis, we have the day the value is recorded. Note, if we overlap the data from the two companies, we see that the only differences"
2889,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,894.0,919.2,25.200000000000045," occur on day one and on day five. On day one, company A is at zero. And company B is at one. And on day five, company A returns to zero, and company B returns to one. On all of the other days, days two, three and four, both companies have the exact same values."
2890,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,919.2,942.24,23.039999999999964," Given this sequential data, we want the LSTM to remember what happened on day one, so it can correctly predict what will happen on day five. In other words, we're going to sequentially run the data from days one through four through an unrolled LSTM. And see if it can correctly predict the values for day five for both companies."
2891,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,942.24,970.88,28.639999999999983," So let's go back to the LSTM and initialize the long and short-term memories to zero. Now, because this single LSTM unit is taking up the whole screen, let's shrink it down to this smaller simplified diagram. Now, if we want to sequentially run company A's values from days one through four through this LSTM, then we'll start by plugging the value for day one, which is zero into the"
2892,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,970.88,992.56,21.67999999999995," input. Now, just like before, we do the math. And after doing the math, we see that the new or updated long-term memory is negative zero.20. And the new, updated short-term memory is negative zero.13."
2893,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,992.56,1017.48,24.920000000000076, So we plug in negative zero.2 for the updated long-term memory and negative zero.1 for the updated short-term memory. Now we unroll the LSTM using the updated memories and plug the value from day two to zero.5 into the input. Then the LSTM does its math using the exact same weights and biases as before.
2894,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1017.48,1037.48,20.0," And we end up with these updated long and short-term memories. Note, if you can't remember the stack quest on recurrent neural networks very well, the reason the LSTM reuses the exact same weights and biases is so that it can handle data sequences of different lengths. Small BAM."
2895,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1037.48,1065.04,27.559999999999945," Anyway, we unroll the LSTM again and plug in the value for day three. Then the LSTM does the math, again using the exact same weights and biases. And gives us these updated memories. Then we unroll the LSTM one last time and plug in the value for day four. And the LSTM does the math, again using the exact same weights and biases."
2896,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1065.04,1089.96,24.920000000000076," Then gives us the final memories. And the final short-term memory, 0.0, is the output from the unrolled LSTM. And that means the output from the LSTM correctly predicts company A's value for day five. Bam. Now that we have shown that the LSTM can correctly predict the value on day five for company A,"
2897,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1090.0,1112.72,22.720000000000027," let's show how the same LSTM with the same weights and biases can correctly predict the value on day five for company B. Note, remember on day one through four, the only difference between the companies occurs on day one. And that means the LSTM has to remember what happened on day one in order to correctly predict"
2898,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1112.72,1133.0,20.279999999999973," the different output values on day five. So let's start by initializing the long and short-term memories to zero. Now let's plug in the value for day one from company B, one. And the LSTM does the math, just like before, using the exact same weights and biases. Bam."
2899,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1133.0,1164.36,31.360000000000127," After doing the math, we see that the updated long-term memory is 0.5. And the updated short-term memory is 0.28. So we plug in 0.5 for the updated long-term memory and 0.3 for the updated short-term memory. Now we unroll the LSTM and do the math with the remaining input values. And the final short-term memory, one.0, is the output from the unrolled LSTM."
2900,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1164.36,1185.48,21.11999999999989," And that means the output from the LSTM correctly predicts company B's value for day five. Double-band. In summary, using separate paths for long-term memories and short-term memories, long-short-term memory networks avoid the exploding vanishing gradient problem."
2901,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1185.48,1211.12,25.639999999999876," And that means we can unroll them more times to accommodate longer sequences of input data than a vanilla recurrent neural network. At first I was scared of how complicated the LSTM was, but now I understand. Now it's time for some, shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat-quest PDF"
2902,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1211.12,1232.88,21.76000000000022," study guides in my book, the stat-quest illustrated guide to machine learning at stat-quest.org. There's something for everyone. Hooray, we've made it to the end of another exciting stat-quest. If you like this stat-quest and want to see more, please subscribe. And if you want to support stat-quest, consider contributing to my Patreon campaign, becoming"
2903,"Long Short-Term Memory (LSTM), Clearly Explained",https://www.youtube.com/watch?v=YCzL96nL7j0,YCzL96nL7j0,1232.88,1244.56,11.679999999999836," a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time, Quest ON."
2904,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,0.0,25.6,25.6," If you want to turn words into numbers, and you want those numbers to make sense, then use word embeddings, and similar words will have similar numbers who raised that quest. Hello, I'm Josh Starmer and welcome to Stack Quest."
2905,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,25.6,40.72,15.119999999999996," Today we're gonna talk about word embedding and word to Vek, and they're gonna be clearly explained. Lightning makes using the cloud just as easy to use as your laptop, and that's cool."
2906,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,40.72,54.16,13.440000000000005," This Stack Quest is also brought to you by the letters A, B, and C. A, always, B, B, C, curious. Always, B, curious. Also, I wanna give a special thanks to Alex Lavani"
2907,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,54.16,67.2,13.04," and the students at Boston University's Spark for letting me test this Stack Quest out on them. Note, this Stack Quest is focused on how neural networks can be used to create word embeddings, and assumes that you're already familiar"
2908,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,67.2,79.75999999999999,12.559999999999988," with the basic ideas of neural networks and back propagation, as well as the main ideas about the soft max function and cross entropy. If not, check out the quests. Words are great."
2909,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,79.75999999999999,97.92,18.16," We can use them to communicate all kinds of cool ideas. For example, hey, norm, isn't Stack Quest awesome? It sure is, squash. Unfortunately, a lot of machine learning algorithms, including neural networks don't work well with words."
2910,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,97.92,116.08,18.16000000000001," Hey, neural network, isn't Stack Quest awesome? So, if we wanna plug words into a neural network or some other machine learning algorithm, we need a way to turn the words into numbers. 24, 0.3, 5.1."
2911,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,116.08,130.96,14.88000000000001," Yes, Stack Quest is awesome. Bam. One super easy way to convert words into numbers is to just assign each word to a random number. For example, if someone just saw the hit movie"
2912,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,130.96,147.08,16.120000000000005," troll two and said, troll two is great, we can just give each word a random number. Note, for the purposes of this Stack Quest, we're going to treat troll two as a single word. Anyway, if the next person said,"
2913,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,147.08,162.48000000000002,15.400000000000006," troll two is awesome. Then we could reuse the random numbers that we already selected for troll two and is and assign a new random number to the new word awesome. In theory, this is fine."
2914,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,162.48000000000002,179.16,16.67999999999998," But it means that even though great and awesome means similar things at are used in similar ways, they have very different numbers associated with them. 4.2 and negative 32.1. And that means the neural network"
2915,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,179.16,197.08,17.919999999999987, will probably need a lot more complexity and training because learning how to correctly process the word great won't help the neural network correctly use the word awesome. So it would be nice if similar words that are used in similar ways could be given similar numbers.
2916,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,197.08,212.72000000000003,15.640000000000043," So that learning how to use one word will help learn how to use the other at the same time. And because the same words can be used in different contexts or made plural or used in some other way, it might be nice to assign each word more than one number"
2917,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,212.72000000000003,229.76,17.039999999999992," so that the neural network can more easily adjust to different contexts. For example, the word great can be used in a positive way, like stat Quest is great. And it can also be used in a sarcastic, negative way, like"
2918,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,229.76,242.92,13.159999999999997, my cell phone's broken. Great. And it would be nice if we had one number that could keep track of the positive ways that great is used and a different number to keep track of the negative ways.
2919,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,242.92,258.12,15.200000000000015," Hey Josh, deciding what words are similar and used in similar contexts sounds like a lot of work. And using more than one number per word to account for different contexts sounds like even more work. Don't worry, Squatch."
2920,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,258.12,272.32,14.199999999999989," The good news is that we can get a super simple neural network to do all of the work for us. Boom. To show how we can get a super simple neural network to figure out what numbers should go with different words,"
2921,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,272.32,289.16,16.840000000000032," let's imagine we have two phrases. Troll 2 is great, and Jim Cotta is great. Anyway, both phrases have two words. Troll 2 and Jim Cotta in similar contexts because someone in a not going to name names"
2922,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,289.16,307.52,18.35999999999996," thinks that these two terrible movies are great. So in order to create a neural network to figure out what numbers we should associate with each word, the first thing we do is create end puts for each unique word. In this case, we have four unique words in the training data"
2923,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,307.52,326.48,18.95999999999998," so we have four end puts. Now we connect each end put to at least one activation function. Note, this activation function uses the identity function. And thus, the end put value is the same as the output value. In other words, this activation function"
2924,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,326.48,343.64,17.160000000000025," doesn't do anything except give us a place to do addition. The number of activation functions corresponds to how many numbers we want to associate with each word. And the weights on those connections will, ultimately, be the numbers that we associate with each word."
2925,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,343.64,364.76,21.120000000000005," Now, in this example, we want to associate two numbers with each word. So that means we'll use two activation functions. And the weights on the connection to the second activation function will be another number associated with each word. However, like always, these weights start out with random values"
2926,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,364.76,383.16,18.400000000000038," that will optimize with back propagation. Now, in order to do back propagation, we have to make predictions. So we'll use the end put word to predict the next word in the phrase. So if the phrase is, troll two is great."
2927,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,383.16,405.88,22.71999999999997," Then we can use the word troll two to predict the word is. In other words, if the end put word is troll two, and we indicate that by putting a one into the troll two end put, and zeros into all of the other end puts, then we want the output for the next word is to have the largest value."
2928,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,405.88,420.96,15.08000000000004," And if the end put word is, is, which means that the end put for is is one, and all of the other end puts are zero, then we want the output for the next word, great, to have the largest value."
2929,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,420.96,437.92,16.95999999999998," Lastly, if the end put word is Jim Cata, which means that the end put for Jim Cata is one, and all of the other end puts are zero, then we want the output for the next word is to have the largest value. In order to make these predictions,"
2930,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,437.92,456.4,18.480000000000015," we connect the activation functions to outputs, and we add weights to those connections with random initialization values. And then we run the outputs through the softmax function, because we have multiple outputs for classification. And that means we can use the cross entropy loss function"
2931,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,456.4,471.92,15.519999999999982," for back propagation. Again, that goal is to train this neural network so that it correctly predicts the next word in a phrase. And now, before training, if we plug the word troll 2 into the end put,"
2932,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,471.92,485.96,14.04000000000002," and do the math, we get lucky and correctly predict the next word is, but just barely. However, when we plug the word is into the end put, and do the math,"
2933,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,485.96,501.88,15.920000000000016," then we fail to correctly predict the next word, great, and instead predict is, won't, won't. So we need to train this neural network. However, before we optimize all of the weights,"
2934,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,501.88,519.0,17.120000000000005," remember that I said that these weights would be the numbers associated with each word. And since in this example, we have two weights for each word, we can plot each word on a graph that has the weight values to the top activation function"
2935,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,519.0,541.5999999999999,22.59999999999991," on the x-axis, and the weight values to the bottom activation function on the y-axis. For example, the word troll 2 goes here, because its top weight is 0.38, and its bottom weight is 0.42. Likewise, the word is goes here,"
2936,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,541.6,558.64,17.039999999999964," and the word great goes here. And jemqata goes here. Now, with this graph, we see that the words troll 2 and jemqata are currently no more similar to each other as they are to any of the other words."
2937,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,558.64,574.24,15.600000000000025," However, because both words appear in the same context in the training data, we hope that back propagation will make their weights more similar. So we start with these weights, and we end up with these new weights."
2938,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,574.24,589.28,15.039999999999964," The new weights on the connections from the inputs to the activation functions are the word embeddings. And when we plot the words using the new weights, which are now the embeddings, we see that troll 2 and jemqata are now relatively close to each other"
2939,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,589.28,605.4,16.120000000000005," compared to the other words in the training data. Bam. Now that we've trained the neural network, we can see how well it predicts the next word. For example, if we plug the word troll 2 into the input"
2940,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,605.4,623.68,18.279999999999973," and do the math, then the output for the next word is is 1 and everything else is 0. And that is exactly what we wanted. And when we plug the word is into the input and do the math, then the output for the next word"
2941,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,623.68,643.12,19.44000000000005," and both phrases, great is 1 and everything else is 0. Lastly, when we plug the word jemqata into the input and do the math, then the output for the next word is is 1 and everything else is 0. Bam."
2942,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,643.12,661.52,18.399999999999977," Now, before we talk about a popular word embedding tool word to VEC, let's summarize what we've learned so far. First, rather than just a sign random numbers to words, we can train a relatively simple neural network to assign numbers for us."
2943,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,661.52,674.72,13.200000000000044, The advantage of using a neural network is that it can use the context of words in the training data set to optimize weights that can be used for the embeddings. And this can result in similar words ending up
2944,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,674.72,690.56,15.839999999999918," with similar embeddings. Lastly, having similar words with similar embeddings means training a neural network to process language is easier because learning how one word is used helps learn how similar words are used."
2945,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,690.56,706.0,15.440000000000056," Double bam. Now, so far, we've shown we can train a neural network to predict the next word in each phrase. But just predicting the next word doesn't give us a lot of context to understand each one."
2946,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,706.0,723.04,17.039999999999964," So, now let's learn about the two strategies that word divac, a popular method for creating word embeddings, uses to include more context. The first method, called continuous bag of words, increases the context by using the surrounding words"
2947,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,723.04,741.32,18.280000000000086," to predict what occurs in the middle. For example, the continuous bag of words method could use the words troll two and great to predict the word that occurs between them is. The second method, called skip gram, increases the context"
2948,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,741.32,762.6800000000001,21.360000000000014," by using the word in the middle to predict the surrounding words. For example, the skip gram method could use the word is to predict the surrounding words troll two, great, and Jim Cotta. Lastly, before we're done, just know that in practice, instead of using just two activation functions"
2949,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,762.6800000000001,779.04,16.3599999999999," to create two embeddings per word, people often use 100 or more activation functions to create a lot of embeddings per word. And instead of using two sentences for training, they use the entire Wikipedia."
2950,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,779.04,802.72,23.680000000000064," Thus, instead of just having a vocabulary of four words and phrases, word divac might have a vocabulary of about three million words and phrases. Thus, the total number of weights in this neural network that we need to optimize is three million words and phrases, times at least 100, the number of weights each word"
2951,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,802.72,820.88,18.159999999999968," has going to the activation functions. Times two, for the weights that get us from the activation functions to the outputs, for a total of 600 million weights. So training can be slow. However, one way that word divac speeds things up"
2952,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,820.88,839.76,18.879999999999995," is to use something called negative sampling. Negative sampling works by randomly selecting a subset of words we don't want to predict for optimization. For example, say like we wanted the word artvark to predict the word A. That means that only the word artvark"
2953,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,839.76,858.48,18.720000000000027, has a one in it and all of the other words have zeros. And that means we can ignore the weights coming from every word but artvark because the other words multiply their weights by zero. That alone removes close to 300 million weights from this optimization step.
2954,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,858.48,873.68,15.199999999999932," However, we still have 300 million weights after the activation functions. So, because we want to predict the word A, then we don't want to predict artvark abandoned and all of the other words."
2955,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,873.68,894.3599999999999,20.67999999999995," So, for this example, let's imagine word divac randomly selects abandoned as a word we don't want to predict. Note, in practice, word divac would select between two and 20 words that we don't want to predict. However, in this example, we just select one abandoned."
2956,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,894.3599999999999,912.04,17.680000000000064," So now word divac only uses the output values for A and abandon. And that means for this round of back propagation, we can ignore the weights that lead to all of the other possible outputs. So, in the end, out of the 600 million total weights"
2957,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,912.04,929.92,17.879999999999995," in this neural network, we only optimize 300 per step. And this is one way that word divac can efficiently create lots of word embeddings for each word in a large vocabulary. Triple B. Now it's time for some."
2958,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,929.92,946.96,17.040000000000077," Shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat quest PDF study guides and my book. The stat quest illustrated guide to machine learning at statquest.org. There's something for everyone."
2959,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,946.96,964.4,17.43999999999994," Hooray, we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie,"
2960,"Word Embedding and Word2Vec, Clearly Explained!!!",https://www.youtube.com/watch?v=viZrOnJclY0,viZrOnJclY0,964.4,971.0,6.600000000000023," or just donate the links are in the description below. All right, until next time, quest on."
2961,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,0.0,24.0,24.0," Tenses, do it for the data, tenses, do it for the speed, tenses, do it whenever you feel the need. Steadquist. Hello, I'm Josh Starmer and welcome to Steadquist. Today we're going to talk about Tensers for neural networks and they're going to be clearly explained."
2962,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,24.0,44.0,20.0," This stack quest is sponsored by Lightning and Grid.AI. With Lightning, you can design, build, and scale models with ease, focus on the business and research problems that matter to you. Lightning takes care of everything else. And with Grid, you can use the cloud to seamlessly train hundreds of models from your"
2963,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,44.0,61.72,17.72," laptop with a single command. No code changes necessary. For more details, follow the links in the pinned comment below. One thing that makes Tensers a little confusing is that different people use the word Tenser differently."
2964,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,61.72,82.28,20.56," People in the math and physics community define Tenser one way and people in the machine learning community define Tenser a different way. In this stack quest, we're going to focus on the way Tenser is used in the machine learning community. Within the machine learning community, Tensers are used in conjunction with neural"
2965,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,82.32000000000001,104.52,22.200000000000003," networks, so we need to talk about neural networks. Note, if you're not already familiar with neural networks, feel free to check out the quests, the links are in the description below. Anyway, neural networks can do a lot of things. For example, in the stack quest, neural networks part one inside the black box, we had a"
2966,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,104.52,134.28,29.75999999999999," simple neural network that had a single input, drug dosage, and used that single value to predict a single output, the efficacy of the dosage. Then, in the stack quest on back propagation, we saw that even with this super simple neural network, and this super simple training data with only three data points, we still had to do a lot of math for the neural network to fit this squiggle to the data."
2967,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,134.28,162.0,27.72," Ugg, math. Then, in the stack quest, neural networks part four, multiple inputs and outputs, we had a fancier neural network that had two inputs that corresponded to two different flower measurements, and had three outputs that predicted which Irish species these two measurements came from, and then we saw how the neural network does a lot of math to make those predictions."
2968,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,162.04,187.84,25.80000000000001," Ugg, more math. Then, in the stack quest, neural networks part eight, image classification with convolutional neural networks, we had a super fancy neural network. That had a six pixel by six pixel image, so 36 pixels in all, as the input. And two outputs that predicted whether the image was of an X or an O."
2969,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,187.84,214.4,26.56," We walked through a whole lot of math that we needed to do in order to make those predictions. Triple Ugg, so much math. Note, even though this neural network needs to do a whole lot of math, it's still relatively simple compared to the types of neural networks that are used in practice. For example, the input to this convolutional neural network is a relatively small six pixels"
2970,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,214.44,248.48,34.03999999999999," by six pixels black and white image. However, in practice, usually the input images much larger, like 256 by 256, and that means the input has 65,536 pixels, and usually the image is color instead of black and white. And color images are usually split into three color channels, red, green, and blue. And since the neural network treats each channel separately, that basically triples the number"
2971,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,248.48,269.68,21.200000000000017," of pixels we have to do math on. So now we're up to three times 65,536, which equals 196,688 pixels that we have to do a lot of math on. And this is just one image, and usually we need to do a ton of images to train the neural network."
2972,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,269.68,291.4,21.71999999999997," So that means we have to do a ton of math on a ton of images. And if we want to apply a neural network to video, which is basically a series of images, then we have even more math. Ugg. The good news is, is that all this math is what tensors were designed for."
2973,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,291.4,316.16,24.760000000000048," Bam. Now let's talk about what tensors are. In the perspective of someone who is creating a neural network, tensors are ways to store the input data, which in this example consists of three color channels for every single frame. But as we saw earlier, the input can also be super simple and consist of a single value."
2974,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,316.16,340.12,23.95999999999998," And tensors also store the weights and biases that make up the neural network. So from the perspective of someone creating a neural network, tensors can seem really boring. Wah, wah. For example, the input value for this neural network is just a single value, which in most programming languages we'd call a scalar."
2975,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,340.12,368.16,28.04000000000002," However, to make things seem more exciting, we can use fancy terminology and call the input, which is just a single value, a zero dimensional tensor. And a neural network takes two input values like this one. Then, in most programming languages, we would say we store the inputs in an array. However, using tensor talk, we will call this a one dimensional tensor."
2976,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,368.16,396.64,28.47999999999996," Likewise, when the input is a single image, most programming languages would call it a matrix, but we'll call it a two dimensional tensor. And when the input is video, most programming languages would call this a multi-dimensional matrix or a multi-dimensional array, or for you Python people and ND array. However, using tensor talk, we will call it an end-dimensional tensor."
2977,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,396.64,419.28,22.640000000000043," So just like I said earlier, from the perspective of someone creating a neural network, sensors can seem really boring because all we have done is rename things that already exist. So what's the big deal? Well, unlike normal scalars, arrays, matrices, and end-dimensional matrices, tensors"
2978,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,419.28,444.76,25.47999999999996," were designed to take advantage of hardware acceleration. In other words, tensors don't just hold data in various shapes like these, but they also allow for all the math that we have to do with the data to be done relatively quickly. Usually, tensors and the math they do are sped up with special chips called graphics processing units, GPUs."
2979,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,444.76,468.2,23.44," But there are also tensor processing units, GPUs that are specifically designed to work with tensors and make neural networks run relatively quickly. Double-bowm. Note, one thing I hinted at early on, but didn't dive into the details about, is that one of the things we do with neural networks is estimate the optimal weights and biases"
2980,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,468.2,491.56,23.360000000000014," with back propagation. And if you saw the stack quest on back propagation, you'll know that we have to derive a bunch of derivatives and do a whole lot of the change. Well, one more cool thing about tensors is that they take care of back propagation for you with automatic differentiation."
2981,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,491.56,511.0,19.44," This means you can pretty much create the fanciest neural network ever and the hard part, figuring out the derivatives will be taken care of by the tensors. Triple-bowm. In summary, there are two types of tensors. One type is used by mathematicians and physicists."
2982,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,511.0,529.7199999999999,18.719999999999917, We did not talk about these today. The other type of tensor is used in neural networks. This is the type we talked about. Tensors for neural networks hold the data and the weights and biases. And are designed for hardware acceleration so that neural networks can do all the math they
2983,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,529.7199999999999,544.68,14.960000000000036, need to do in a relatively short period of time. And they take care of back propagation with automatic differentiation. Bam. Now it's time for some. Shameless self-promotion.
2984,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,544.68,562.8399999999999,18.159999999999968," If you want to review statistics and machine learning offline, check out the statquest study guides at statquest.org. There's something for everyone. Ray, we've made it to the end of another exciting statquest. If you like this statquest and want to see more, please subscribe."
2985,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,562.8399999999999,577.2399999999999,14.399999999999975," And if you want to support statquest, consider contributing to my Patreon campaign, becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. All right."
2986,"Tensors for Neural Networks, Clearly Explained!!!",https://www.youtube.com/watch?v=L35fFDpwIM4,L35fFDpwIM4,577.2399999999999,579.4,2.160000000000082," Until next time, quest on."
2987,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,0.0,22.6,22.6," The Stat Quest Introduction to Patorch is here. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about the Stat Quest Introduction to Puy Torch. This Stat Quest is sponsored by Lightning and Grid.ai."
2988,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,22.6,43.6,21.0," Lightning and Grid are awesome. You can do cool stuff in the cloud. Hooray. Note, this Stat Quest assumes that you already understand the main ideas behind neural networks. The main ideas behind how neural networks are fit to data with back propagation."
2989,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,43.6,61.6,18.0," The main ideas behind the real you activation function, and how tensors are used in neural networks. If not, check out the quests. Lastly, you can download all of the code in this Stat Quest for free. The details are in the pinned comment below."
2990,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,61.6,81.6,19.999999999999996," In neural networks part 3, the real you activation function in action, we started with a simple data set. That showed whether or not different drug doses were effective against a virus. The low and high doses were not effective. But the medium dose was effective."
2991,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,81.6,100.6,19.0," Then we talked about how this neural network used weights and biases to slice, flip, and stretch the real you activation functions into new and exciting shapes. To fit this pointy thing to the dataset. Bam. Hey look, it's Stat's Quatch."
2992,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,100.6,118.6,18.0," Hey Josh, this neural network is awesome. Can we code it in Puy Torch? Yes, however, before we start coding, let's label the weights. And the biases. Now we need to make sure we have the necessary Python modules installed."
2993,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,118.6,146.6,28.0," In this tutorial, we will be using Puy Torch to create the neural network, and map plot lib and c-borne to draw awesome graphs. If you need help installing any of these, check the pinned comment below. Now that we have installed everything that will need to implement this neural network, let's get coding. The first thing we do is import the Python modules that we will use."
2994,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,146.6,174.6,28.0," First, we'll import Puy Torch, which is actually called Torch, because that is what it was originally called before it was ported to Puython. We'll use Torch to create tensors to store all of the numerical values, including the raw data, and the values for each weight and bias. Then we will import Torch.nn, which we will use to make the weight and bias tensors part of the neural network."
2995,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,174.6,203.6,29.0," Then we import Torch.nn.functional, which gives us the activation functions. Then we import SGD, which is short for stochastic gradient descent, to fit the neural network to the data. The next two things we import, map plot lib and c-borne, are all just to draw nice looking graphs. Note, the c-borne package is traditionally imported as sns, which stands for Samuel Norman C-borne,"
2996,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,203.6,223.6,20.0," a fictional character in the drama The West Wing. The dude that wrote c-borne is just a big fan of the West Wing, and names his stuff after it. Strange but true, bam. Now let's build this neural network. With Puy Torch, creating a new neural network means creating a new class."
2997,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,223.6,247.6,24.0," So we start by creating a new class. That, in this example, we call basic nn. And basic nn will inherit from a Puy Torch class called module. Now we create an initialization method for the new class. And the first thing we do is call the initialization method for the parent class, nn.module."
2998,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,247.6,282.6,35.00000000000003," Then we initialize the weights and biases in our neural network. We'll start with weight W sub 00, which is set to 1.70. So we create a new variable called W00, and make it a neural network parameter. Making this weight a parameter for the neural network gives us the option to optimize it. Now, since weight W sub 00 is 1.70, we initialize the new parameter with a tensor set to 1.7."
2999,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,282.6,321.6,39.0," Note, since this is a tensor, the neural network can take advantage of the accelerated arithmetic and automatic differentiation that it provides. Lastly, because we don't need to optimize this weight, we'll set requires underscore grad, which is short for requires gradient to false. Likewise, we create new variables for the bias B sub 00 and the weight W sub 01. And then we create variables for the remaining weights and biases. Bam, we have created neural network parameters for each weight and bias."
3000,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,321.6,359.6,38.0," Now we need to connect them to the input, the activation functions, and ultimately to the output. In other words, we need a way to make a forward pass through the neural network that uses the weights and biases that we just initialized. So we do that by creating a second method inside basic nn called forward. So we can see what's going on. Let's move the code for forward to the top of the screen. Now, the first thing we want to do is connect the input to the activation function on top."
3001,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,359.6,390.6,31.0," So we create a new variable, input to top rlu that is equal to. The input times the weight W sub 00 plus the bias B sub 00. Then we pass input to top rlu to the rlu activation function with f dot rlu. Psst, remember earlier we imported torched.nn dot functional as f. So that is where the rlu function comes from."
3002,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,390.6,417.6,27.0," Then we save the output of the rlu in top rlu output. Now we scale top rlu output by the weight W sub 01 and save the result. In scale top rlu output. Likewise, we connect the input to the bottom rlu and scale the activation functions output. Then we add the top and bottom scaled values to the final bias."
3003,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,417.6,440.6,23.0," And use the sum as the input to the final rlu to get the output value. Lastly, the forward function returns the output. Thus, given an input value, the forward function does a forward pass through the neural network to calculate and return the output value. Bam."
3004,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,440.6,473.6,33.0," Now, if we look at the entire class that we created, basic nn, we see two methods, nn and forward. nnit creates an initializes the weights and biases. And forward does a forward pass through the neural network by taking an input value and calculating the output value with the weights, biases, and activation functions. Wow, this is a lot of code. How do we know it works and doesn't have any bugs? Good questions, watch?"
3005,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,473.6,507.6,34.0," We can verify that the code works by plugging in a bunch of values between 0 and 1 that represent different doses. And see if the output from forward results in this bent shape that fits the training data. So the first thing we need to do is create a sequence of input doses. And we do that with this command. Here, we use the pi torch function, ln space, to create a tensor with a sequence of 11 values between and including 0 and 1."
3006,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,507.6,542.6,35.0," And we store the tensor in a variable called input doses. Note, we can print out and admire the input doses by just typing the variable name in put doses. Now, the idea is to run these input values through our neural network. So we'll make a neural network that will call model from the class we just created basic nn. Note, we are naming the actual neural network model because that is the standard variable name used when coding with pi torch."
3007,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,542.6,577.6,35.0," Thus, from here on out, I'm going to use the term model and neural network interchangeably. If this freaks you out, check out the stack quest on models. Anyway, now we can pass the input doses to the model, which, by default, calls the forward method that we wrote earlier. And we save the output from the neural network in a variable we cleverly named output values. And now that we have both the input values to the neural network and the output values, we can use them to draw this graph."
3008,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,577.6,608.6,31.0," That has different drug doses on the x-axis, and they're predicted effectiveness on the y-axis. First, we set the C-born style to white grid so the graph looks cool, and then we use line plot to draw graph of the data. On the x-axis, we put the original input doses. And on the y-axis, we put the corresponding output values. And then we make the line green and wide enough to easily see."
3009,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,608.6,641.6,33.0," Lastly, we set the y in x-axis labels. And that code gives us this graph. The graph tells us that the neural network recreated earlier, basic and n, does exactly what we expected. In other words, earlier, we showed that when we put input values between zero and one into this neural network, the output was this bent shape, which is the same as the graph we drew with our code. Double-bowl."
3010,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,641.6,680.6,39.0," Now that we can create a neural network and pie torch and graph what it can do, can we pretend that we don't already know the optimal value for b sub final is negative 16? Sure, thanks, Quatch. We'll just set b sub final to zero. And we can now use pie torch to optimize b sub final with back propagation. The first thing we'll do is make a copy of the original class we created basic nn. And change the name of the copy from basic nn to basic nn underscore train, because we want to train this neural network."
3011,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,680.6,731.6,51.0," Then we change the initial value for final bias to zero point zero, and we set requires underscore grad, which, remember, is short for requires gradient to true. Setting requires grad to true is what tells pie torch that this parameter should be optimized. We can verify that setting b sub final to zero results in a neural network that no longer fits the training data by drawing a graph of the neural networks output like we did before. Only this time, we create the model from basic nn underscore train instead of basic nn. And, because final underscore bias now has a gradient, we call the touch on the output values to create a new tensor that only has the values."
3012,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,731.6,780.6,49.0," In other words, because cborn doesn't know what to do with the gradient, we strip it off with detach. The original graph we true for basic nn shows effectiveness equals one when the dose equals 0.5, which is correct. In contrast, the graph or basic nn underscore train shows effectiveness equals 17 when dose equals 0.5, which is way too high. And that means we need to train the neural network to optimize b sub final, which means we need to create this training data. All we have to do to create training data is create one tensor called inputs with the three input doses, zero 0.5 and one."
3013,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,780.6,812.6,32.0," And another tensor called labels that has the observed output values, zero, one and zero. Now we are ready to optimize the last bias, b sub final. Unfortunately, this next step requires a lot of code, but don't worry, we'll go through it one step at a time. Also, spoiler alert. Later in this series on how to implement neural networks, we'll see how pi torch lightning makes this code a lot simpler."
3014,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,813.6,853.6,40.0," Anyway, the first thing we do is create an optimizer object that will use to cast a gradient descent, SGD, to optimize b sub final. Psst, remember, we imported the SGD class from the torch dot opt-impactage way back at the start. So, in order to optimize b sub final, we pass model dot parameters to SGD, which will optimize every parameter that we set requires grad equal to true. We also set the learning rate to 0.1. In a bid, we're going to use our new optimizer to optimize final bias."
3015,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,853.6,884.6,31.0," But first, just so we can see how gradient descent improves the value for final bias, will print the current value. Psst, the stir function converts the tensor value into a string so we can print it with other text. And now we are ready to code the for loop that does gradient descent. Note, if you're not already familiar with gradient descent and stochastic gradient descent, check out the quests. Oh no, it's the dreaded terminology alert."
3016,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,884.6,925.6,41.0," Each time our optimization code sees all of the training data is called an e-pop. So, in this example, every time we run all three points from our training data through the model, we call that an e-pop. Thus, we start our optimization code with a for loop that counts the number of e-pawks. And we set it so that we will run all three data points from the training data through the model up to 100 times. Now we create an initialize a variable called total laws that will store the laws, which is a measure of how well the model fits the data."
3017,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,925.6,980.6,55.0," For example, if our unoptimized model fit the training data really poorly like this, and we had this really large residual, the difference between what the model predicts and what we know is true, then the laws would be relatively large. In contrast, if the model fit the training data a little better, and we had a smaller residual, then the laws would be relatively small. Thus, for each e-pop, we will use total laws to keep track of how well the model fits the data. Now we start a nested for loop that runs each data point from the training data through the model and calculates the total laws. In this case, that means the for loop starts with the first point in the training data, and determines its input or dose, and its known label or effectiveness."
3018,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,980.6,1014.6,34.0," Then it runs that dose through the model to get a predicted output. And then we calculate the laws between the predicted value and the known label with a loss function. In this case, we are calculating the squared residual where the residual is the difference between the output and the known value. That said, you can code any loss function that you want to use, like the absolute value loss, or you can choose from among the many loss functions, like MSE loss, or cross-inchipy loss that come with pi torch."
3019,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1014.6,1054.6,39.999999999999886," Anyways, in this example, the predicted and known value for the first point is zero. So the squared residual is zero minus zero squared, which equals zero. Now we use law-stott backward to calculate the derivative of the loss function with respect to the parameter or parameters we want to optimize. In this example, that means calculating the derivative of the squared residual with respect to b sub final and plugging in the predicted and known values. Note, if any of this part is freaking you out, check out the stack quest on back propagation."
3020,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1054.6,1090.6,36.0," Lastly, we add the squared residual to total loss so we can keep track of how well the model fits all of the data. Now we go back to the start of the nested for loop, and select the input and label for the second point in the training data set. And then we run the second point through the model to get a predicted output. And then we calculate the loss, the squared residual between the predicted and observed values. Next, we use law-stott backward to calculate the derivative of the loss function with respect to b sub final and,"
3021,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1090.6,1120.6,30.0," and this is really important, law-stott backward adds that to the previous derivative. In other words, law-stott backward remembers the derivative that we calculated for the first point, and adds the new derivative that we calculated for the second point. Thus, law-stott backwards accumulates the derivatives each time we go through the nested loop. To be honest, the first time I saw this, it blew my mind."
3022,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1120.6,1151.6,31.0," This is because every time we go through the nested loop, we create a brand new loss variable here, and I couldn't figure out how the new loss could add to what the last one computed. However, it turns out that we create loss from the output value, which in turn comes from the model, and the model keeps track of the derivatives. Anyway, the main point is that law-stott backward accumulates the derivatives each time we go through the nested loop,"
3023,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1151.6,1181.6,30.0," and we need to keep this in mind. In contrast, total loss does not automatically accumulate, so we add the new squared residual to total loss. Then we go through the loop one last time for the last point in the training dataset. And that means we calculate the squared residual for the last point. And, when we call loss.backward, we add the derivative for the last point to the other two derivatives."
3024,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1181.6,1209.6,28.0," And, lastly, we add the squared residual to total loss. Bam, we made it through the nested loop. And now that we're done with the nested loop, we check to see if total loss is really small. If so, that means the model fits the training data really well, and we can stop training. So, if total loss is really small, we print out the number of e-packs we've gone through so far,"
3025,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1209.6,1246.6,37.0," and break out of the optimization loop to stop training. Otherwise, if total loss is not small, then we take a small step towards a better value for B sub final using optimizer.step. Note, just like loss has access to the derivatives in the model when we call loss.backward, optimizer.step also has access to the derivatives stored in model and can use them to step in the correct direction. Now we need zero out the derivatives that we're storing in model, and we do that with optimizer.zero grad."
3026,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1246.6,1281.6,35.0," Note, if we don't zero out the derivatives, then the next time we enter this nested loop and call loss.backward, we'll add the new derivatives to the old derivatives from the previous step. And that would be bad. Lastly, we print out the current e-pock and the current value for final bias so we can see how final bias changes each time through the loop. Now we've made it through the entire optimization loop, and we just repeat it until total loss is small or we go through 100 e-packs."
3027,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1281.6,1317.6,36.0," Anyway, now that we have gone through the optimization loop, we print out the final value for the final bias. Bam! Now, when we run this block of code, we see the value for final bias before we optimize zero, and that value, as we saw before, gives us this graph of the output. Then we see the values that final bias takes on during each step of gradient descent. And after 34 steps, the total loss is tiny."
3028,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1317.6,1353.6,36.0," And the optimal value for final bias is negative 16.0019, which is pretty close to negative 16, the optimal value we used originally. Hey, can we draw one last graph to verify that the optimized model fits the training data? Sure thing, Squatch? We can verify that the optimized model fits the training data by graphing it with this code, which is the same as what we used before, except now we don't create a new model. Instead, we just use the one we optimize."
3029,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1353.6,1374.6,21.0," And this is what we get, which shows that the neural network does exactly what we expect. Triple bam! Now it's time for some. Shameless self-promotion. If you want to review statistics and machine learning offline, check out the StacQuest study guides at statquest.org."
3030,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1374.6,1390.6,16.0," There's something for everyone. Whoay! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign, becoming a channel member,"
3031,The StatQuest Introduction to PyTorch,https://www.youtube.com/watch?v=FHdlXe1bSe4,FHdlXe1bSe4,1390.6,1401.6,11.0," buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below. All right, until next time, quest on."
3032,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,0.0,31.6,31.6," Pytouch plus lightning is the coolest thing around Stacquist. Hello, I'm Josh Starman. Welcome to Stacquist. Today we're going to talk about an introduction to coding neural networks with pie, torch and lightning. Lightning lets you do awesome stuff with neural networks. Yeah. This Stacquist is also brought to you by the letters A, B and C."
3033,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,31.6,67.12,35.52," A always B, B, C, curious. Always B, curious. Also, I want to give a special ban to one of my co-workers at Lightning, Adrian Valshley, who helped me create this tutorial. Note, this Stacquist assumes that you have already seen the Stacquist introduction to pie torch, if not, check out the quest. Lastly, you can download all of the code in this Stacquist for free, the details are in the pinned comment below. In the Stacquist introduction to pie torch,"
3034,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,67.12,102.08,34.959999999999994," we started with this super simple data set, and then we coded this super simple neural network. To fit a pointy thing to the data. To do this, we created a class that contained code for the weights and biases, and for running data through the neural network. And then, completely separate from that class, we wrote code to optimize the neural network with back propagation. Bam? Well, even though the code worked like we expected, we had to come up with our own learning"
3035,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,102.08,135.04,32.959999999999994," rate for gradient descent. And, generally speaking, figuring out a good learning rate isn't always easy. So it would be nice if there was a tool that could find a good learning rate for us. Also, it would be nice if the code for training the neural network were easier to read and write. Lastly, we have to make significant changes to this code if we had GPUs or TPUs to accelerate learning. For example, this code runs fine on my laptop, but if we wanted to accelerate it with"
3036,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,135.04,169.92,34.879999999999995," one or more GPUs, we'd have to make a lot of changes. The good news is that we can do all of these things and more when we combine pie torch with lightning. So let's go back through the code and show how we can do things easier and improve it with lightning. Bam! First, just like before, we import torch to create tensors to store all of the numerical values, including the raw data and the values for each weight and bias. Then we import torch.inn to make the weight and bias"
3037,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,169.92,206.0,36.08000000000001," tensors part of the neural network and torch.inn.functional for the activation functions. Then we import SGD so we can use stochastic gradient descent to fit the neural network to the data. So far, everything is the same as when we use pie torch without lightning. But now we import lightning as L to make training easier to code. And now we need tensor data set and data loader from torch.utils.data, which will ultimately make our lives easier when we start working with"
3038,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,206.0,239.28,33.28," larger data sets. And just like before, we'll graph our output with matte plot, lib and c-borne. Now let's build this neural network. Note, if all we want to do is create a pre-trained neural network and run data through it, then everything is the exact same as before, except now when we create the class, we'll name it basic lightning, and we'll inherent from lightning module instead of in-in dot module, which is what we did when we used pie torch without"
3039,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,239.36,272.16,32.80000000000001," lightning. Other than that, creating this pre-trained neural network and running data through it is just like before. In other words, just like before, we create an initialization method for the new class, and the first thing we do is call the initialization method for the parent class, lightning module. And just like before, we create the weights and biases for the network. By creating a new parameter, initialized with a tensor set to the value for the weight or bias."
3040,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,272.96000000000004,306.88,33.91999999999996," And we do that for each weight and bias in the network. Bam. Now, just like before, we need a way to make a forward pass through the neural network that uses the weights and biases that we just initialized. So we create a second method inside the class called forward. So we can see what's going on, let's move the code for forward to the top of the screen. Again, just like we did when we used pie torch without lightning, we create a new variable input to top rau-u that is equal to,"
3041,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,307.52,343.6,36.08000000000004," the input times the weight, w sub 0, 0 plus the bias, b sub 0, 0. Then we pass input to top rau-u to the rau-u activation function with f dot rau-u. And scale the rau-u output by the weight w sub 0 1. Likewise, we connect the input to the bottom rau-u and scale the activation functions output. Then we add the top and bottom scale values to the final bias and use the sum as the input to the final rau-u to get the output value."
3042,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,344.48,375.76,31.279999999999973," Lastly, the forward function returns the output. So just like before, we have created a new class that initialized the weights and biases and does a forward pass through the neural network. Now, I don't know about you, but every time I write a block of code, even when it's mostly the same as something I wrote before, I like to test it to make sure it works as expected. So let's test the code by plugging in a bunch of values between 0 and 1 that represent different"
3043,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,375.76,409.84,34.079999999999984," doses and see if the output from forward results in this bent shape that fits the training data. Again, just like before, we can create a sequence of numbers between 0 and 1 using the lens space function from pi torch. We store the tensor in a variable called input doses and we can print out and admire the input doses by just typing the variable name input doses. Now, in order to run these input values through our neural network, we make a neural network"
3044,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,409.84,440.8,30.96000000000004," that will call model from the class we just created basic lightning. Then we pass the input doses to the model, which by default calls the forward method that we wrote earlier. And we save the output from the neural network in a variable that we cleverly named output values. And now that we have both the input values to the neural network and the output values, we can use them to draw this graph. So we set the C-born style to white grid so the graph"
3045,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,440.8,471.68,30.87999999999994," looks cool. And then we use line plot to draw graph of the data. Lastly, we set the y and x-axis labels. And that code gives us this graph. The graph tells us that the neural network recreated earlier, basic lightning, does exactly what we expected. Hey, so far pretty much everything has been a review of basic pie torch. When will we start doing cool stuff with lightning? Right now, squatch."
3046,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,472.88,506.56,33.67999999999995," So we can demonstrate how lightning makes it easier to train a model will set b sub final to 0. And make a copy of the original class we created, basic lightning, and change the name of the copy to basic lightning train because we want to train this neural network. Then we change the initial value for final bias to 0.0 and we set requires grad, which, remember, is short for requires gradient to true. And because we will use a lightning function to"
3047,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,506.56,540.32,33.760000000000105," improve the learning rate for us, we add a new variable, learning rate to store the value. Note, for now we set learning rate equal to 0.01, but this is just a placeholder value, and the actual value does not matter right now. Now, just like we did before, we can verify that this neural network no longer fits the training data by drawing a graph of the neural network's output. Only this time, we are using basic lightning train instead of basic lightning,"
3048,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,541.2800000000001,575.6800000000001,34.39999999999998," and because final bias now has a gradient, we call detach on the output values to create a new tensor that only has the values. The graph shows effectiveness equal 17 when dose equals 0.5, which is way too high. And that means we need to optimize b sub final. So we create the training data by creating a tensor called inputs with three input doses, 0, 0.5 and 1. And another tensor called labels that has the known values,"
3049,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,575.6800000000001,609.0400000000001,33.360000000000014," 0, 1 and 0. However, now that we are using lightning, we need to wrap the training data in a data loader. So we can bind the inputs and the labels into a tensor data set called data set, and then we use the tensor data set to create a data loader called data loader. Data loaders are super useful when we have a lot of data because, one, they make it easy to access the data in batches. This is super useful when we have more data than memory to store it."
3050,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,609.84,645.12,35.27999999999997," Two, they make it easy to shuffle the data each e-pock, and three, they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging. Okay, now that we have our training data wrapped up in a data loader, we are ready to optimize b sub final. Now, if you remember last time when we used pie torch without lightning, we optimized b sub final with a whole lot of code. The first thing we did was create an optimizer object that used to cast a gradient descent"
3051,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,645.12,678.64,33.51999999999998," sgd to optimize b sub final. Then we coded four loops to calculate the derivatives needed for stochastic gradient descent. Specifically, we coded a loop that went through the full training data set 100 times or e-pocks. Then, for each element in the training data, we calculated the value predicted by the neural network. Then we calculated the laws, which in this case was the square difference between the predicted value and the known value. Then we called law. backward to"
3052,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,678.64,707.36,28.720000000000027," calculate the derivative of the loss function with respect to the parameter we wanted to optimize. Lastly, after we calculated the derivatives for all three points in the training data, we took a small step towards an optimal value for b sub final with optimizer dot step. And zeroed out the gradients with optimizer dot zero grad, so that we could start another e-pock. All in all, we had to write quite a bit of code to train the neural network."
3053,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,708.4,738.16,29.75999999999999," Now let's combine pie torch with lightning to simplify this code a whole bunch. Well, wait, hold on a second. There's one more thing I need to review about the pie torch code. When we only use pie torch, before we wrote this code to optimize b sub final, we created a class to keep track of the weights and biases and a forward function to run data through the neural network. And then separately, we wrote the code to optimize b sub final."
3054,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,738.96,768.24,29.279999999999973," In contrast, when we add lightning, we put all of the code relating to the neural network in the same place. So, when we create the class basic lightning train from lightning module, we create the a knit method that contains the weights and biases for the neural network and the learning rate. The forward method to run data through the neural network and a new method called configure optimizers that sets up the method we want to use to"
3055,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,768.24,797.6,29.360000000000014," optimize the neural network. And just like before, we'll use stochastic gradient to set. However, this time we're setting the learning rate to a variable that will improve in just a bit. Then we create another new method called training step, which takes a batch of training data from the data loader that we created and the index for that batch. The training step function calculates the laws, which just like before is the sum of the"
3056,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,797.6,829.2,31.600000000000023," squared residuals. Now that we've added configure optimizers and training step to our class, we're ready to optimize the neural network. So, since we just modified the class by adding two new methods, the first thing we do is make a new model. Then we create a lightning trainer, which we will first use to find a good value for the learning rate and then we will use it to optimize or train the model. Here we are setting the maximum number of e-pucks to 34,"
3057,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,829.2,859.76,30.559999999999945," because we know from before that 34 e-pucks is enough to fit the model to the data. But if it were not, the good news is that we don't have to start from zero and try again, because lightning lets us add additional e-pucks right where we left off. Now that we have the trainer, we will use it to find and improve the learning rate by calling tuner.lr find. In this case, we're passing lr find the model, the training data, data loader,"
3058,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,860.32,896.0,35.67999999999995," the minimum learning rate, 0.001, the maximum learning rate, 1, and we're telling it to not stop early. In other words, by default, lr find will create 100 candidate learning rates between the minimum and maximum values, and by setting early stop threshold to none, we will test all of them. Anyway, we store the output from lr find in lr find results. And we can access and improve learning rate by calling suggestion on the results."
3059,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,896.96,930.08,33.120000000000005," Now, just for fun, we can print out the new learning rate that we stored in new lr to see what it is. And we see that the new learning rate is 0.00214. And, lastly, we can set the learning rate variable in our model to the new learning rate. Now that we have found an improved learning rate for stochastic gradient descent, let's train the model. To train the model and optimize b sub-final, we simply use the trainer to call the fit function."
3060,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,930.8,958.64,27.840000000000032," Fit requires the model and the training data, which we named data loader. When we call the fit function with our model and training data, the trainer will then call our models configure optimizers function. And, in this case, that means configuring a stochastic gradient descent optimizer using the new learning rate that we just set. Then the trainer calls our model's training step function to calculate the loss."
3061,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,959.6,993.28,33.67999999999995," Then, without us having to do anything, the trainer will call optimizer.0 grad, so that each epoch starts with a fresh gradient. Lost. backward to calculate the new gradient and optimizer.step to take a step towards the optimal values for the parameters. And then it calls training step again and repeats for each epoch that we requested. In other words, the big training loop that we had to code when we use pie torch without lightning"
3062,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,993.92,1028.64,34.72000000000014," is reduced to just coding the loss in the training step function when we use pie torch with lightning. Bam! Now, to verify that we correctly optimize B sub final, we can print out its new value. And we get negative 16.0098. Hey, can we draw one last graph to verify that the optimized model fits the training data? Yes! We can verify that the optimized model fits the training data by graphing it with this code,"
3063,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1029.2800000000002,1060.64,31.359999999999676," which is the same as what we used before, except now we don't create a new model, and instead just use the one we optimized. And this is what we get, which shows that the neural network does exactly what we expect. Double bam! What if we want to train our neural network on GPUs, or use some other fancy accelerator? On a very basic computer, like a laptop, you might only have a single processor that does all of the work called a central processing unit,"
3064,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1060.64,1093.36,32.72000000000003," or CPU. In this case, our neural network, and specifically, the tensors that represent the weights and biases would be on the CPU, as well as the tensors that represent the training data. And when all of the tensors, the ones for the weights and biases and the ones for the data, are in the same place on the CPU, then we just do the math for back propagation to train the neural network. However, training a neural network on a single CPU, which might only have a few"
3065,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1093.36,1127.52,34.16000000000008," computing cores, is usually relatively slow. With this simple neural network and this simple data set, the speed really doesn't matter. But if we had a fancier neural network with tons of weights and biases, and a ton of data, then running everything on a single CPU might be too slow to train in a reasonable amount of time. So, when we need speed, we often train neural networks on one or more graphics processing units, or GPUs, which can have 10 times, or even 100 times"
3066,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1127.52,1160.8,33.27999999999997," more computing cores. And when we use pie torch without lightning, then we have to manually move the tensors to the GPUs, and keeping track of what tensors are where can get pretty complicated. And that means we can't easily test our code on our laptop with a single CPU, and then ported to a system with a lot of GPUs without having to change the code a bunch. In contrast, when we use pie torch with lightning, we can let lightning automatically detect"
3067,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1160.8,1188.96,28.16000000000008," if GPUs are available by setting accelerator to auto when we create the trainer object. And we can let lightning determine how many GPUs are available by setting devices to auto. Now, with lightning, we can test our code on our laptop with a single CPU, and then move it to a fancy computing environment with a bunch of GPUs without having to change the code. Triple BAM!"
3068,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1190.0,1206.8,16.799999999999955," And don't forget, you can download all of the code in this stack quest for free, the details are in the pinned comment below. Now it's time for some, shameless self-promotion. If you want to review statistics and machine learning offline,"
3069,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1206.8,1230.24,23.44000000000005," check out the stack quest PDF study guides in my book, the stack quest illustrated guide to machine learning at stackquest.org. There's something for everyone. Hooray, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more, please subscribe. And if you want to support stack quest, consider contributing to my Patreon campaign,"
3070,Introduction to Coding Neural Networks with PyTorch and Lightning,https://www.youtube.com/watch?v=khMzi6xPbuM,khMzi6xPbuM,1230.24,1242.88,12.6400000000001," becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate, the links are in the description below. All right, until next time, Quest ON!"
3071,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,0.0,25.0,25.0," Long short term memory with five torch plus lightning is cool. Stat Quest. Hello, I'm Josh Starmer and welcome to Stat Quest. Today we're going to talk about long short term memory with PyTorch plus lightning. Pretty self-ful awesomeness and everything being just a little bit easier."
3072,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,25.0,44.0,19.0," Lightning, yeah. This Stat Quest is also brought to you by the letters A, B and C. A, always B, B, C, curious. Always B, curious. Also, I want to give a special ban to one of my co-workers at Lightning,"
3073,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,44.0,67.0,23.0," Adrian Vowshley, who helped me create this tutorial. Note, this Stat Quest assumes that you are already familiar with long short term memory. If not, check out the quest. This Stat Quest also assumes that you have already seen the Stat Quest introduction to coding neural networks with PyTorch plus lightning. If not, check out the quest."
3074,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,67.0,91.0,24.0," Lastly, you can download all of the code in this Stat Quest for free. The details are in the pinned comment below. In the Stat Quest on long short term memory, we had stock prices from two different companies, company A and company B. On the Y axis, we had the stock value. And on the X axis, we had the day the value was recorded."
3075,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,91.0,121.0,30.0," Note, if we overlap the data from the two companies, we see that the only differences occur on day one and on day five. On day one, company A is at zero, and company B is at one. And on day five, company A returns to zero, and company B returns to one. On all of the other days, days two, three and four, both companies have the exact same values."
3076,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,121.0,159.0,38.0," Given this sequential data, we wanted an LSTM to remember what happened on day one in order to correctly predict what will happen on day five. In other words, we ran this sequential data from days one through day four through an unrolled LSTM. To predict the values for day five for both companies. So, for company A, we initialized the long and short term memories on a pre-trained LSTM unit, and then unrolled the LSTM unit to accommodate this sequential data from the first four days."
3077,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,159.0,196.0,37.0," And the final short term memory, zero point zero, was the output from the unrolled LSTM. And that meant that the output from the LSTM correctly predicted company A's value for day five. Likewise, we unrolled the LSTM unit to accommodate this sequential data from the first four days of data from company B. And the final short term memory, 1.0, was the output from the unrolled LSTM. And that meant that the output from the LSTM correctly predicted company B's value for day five."
3078,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,196.0,229.0,33.0," Now that we've seen how an LSTM unit works in theory, let's code it with pie torch plus lightning. In this stackwest, we're going to code this LSTM unit from scratch. That means we'll make the tensors for all of the weights and biases. And then we'll code the first stage that determines what percentage of the long term memory the LSTM should remember, and then we'll code the second stage that creates a new potential long term memory and determines what percentage should be remembered."
3079,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,229.0,264.0,35.0," And then we'll code the third stage that creates a new short term memory and determines what percentage should be remembered. Then we'll use the data from company A and company B to train every single weight and bias. And along the way, we'll learn some cool tricks that lightning gives us to make training easy and fun. Lastly, we'll replace our homemade LSTM with the nn dot LSTM function from pie torch. And we'll use what we learn the first time to quickly and easily train the LSTM."
3080,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,264.0,298.0,34.0," Bam! First, just like always, we import torch to create tensors to store all of the numerical values including the raw data and the weights and biases. Then we import torch dot nn to make the weight and bias tensors part of the neural network. And after we learn how to build an LSTM by hand, we'll also use it to try pie torch's LSTM function. And we also need torch dot nn dot functional for the activation functions."
3081,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,298.0,326.0,28.0," Then we import atom to fit the neural network to the data. Note, in the previous pie torch stack quest tutorials, we've used stochastic gradient descent SGD. Adam is a lot like SGD, but not quite as stochastic. As a result, Adam tends to find the optimal values faster than SGD. Now we import lightning as L to make training easier to code."
3082,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,326.0,353.0,27.0," And that means we need tensor data set and data loader from torch.utils.data. Bam! Now let's build train and use this long short term memory unit. When we create a neural network and pie torch, we always start by defining a new class. Now, because we're coding our own LSTM by hand, we'll call this LSTM by hand."
3083,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,353.0,382.0,29.0," And in order to make training super easy, we'll inherit from lightning module. Then, just like we always do, we create an initialization method for the new class. This class will create an initialize all of the weight and bias tensors that we need to implement an LSTM unit. Then we will create a method called LSTM under score unit. That does the LSTM math."
3084,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,382.0,405.0,23.0," Then we create a new method called forward. That makes a forward pass through the unrolled LSTM unit. Then we'll create a method to configure the optimizer we want to use. Lastly, we'll need a method called training under score step. To calculate the loss, which, like before, will be the sum of the squared residuals."
3085,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,405.0,430.0,25.0," We'll also use training under score step to log the progress we're making during training. This will help us determine if we have done enough training and can stop. Let's start by coding the init method. The first thing we do is call the initialization method for the parent class, lightning module. This gives us access to some methods that will use later."
3086,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,430.0,470.0,40.0," Now we need to create an initialize the weights and biases for the network. However, unlike the previous tutorials we've done, this time we're going to use a normal distribution to randomly select an initialization value for each weight. For example, given this normal distribution with mean equal to zero and standard deviation equal to one, we'll use it to generate random numbers for the weights. The shape of this curve shows that there is a relatively high likelihood of generating random numbers close to zero. And a relatively low likelihood of generating random numbers far from zero."
3087,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,470.0,513.0,43.0," Now, in order to use this normal distribution to generate random numbers for us, we create a new tensor called mean and set it to zero point zero, the mean of the normal distribution. And we create a tensor called STD and set it to 1.0, the normal distributions standard deviation. Now we create a parameter for the first weight in the LSTM and use Torch.normal to initialize it to a number random number. To a number randomly generated from a normal distribution with mean equal to zero and standard deviation equal to one. When I did this, my first weight was randomly set to 0.34."
3088,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,513.0,541.0,28.0," Then the second weight was randomly set to 0.13. Note, for both weights we set requires underscore grad to true because we want to optimize them. Now we create a parameter for the first bias. And we'll initialize the bias to zero instead of a random number. However, we still want to optimize it so we set requires underscore grad equal to true."
3089,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,541.0,588.0,47.0," Likewise, we create an initialized parameters for all of the other weights and biases in the LSTM. Now that we have all of the parameters initialized, we're ready to use them to do the math inside the LSTM unit. We'll start by defining a method called LSTM underscore unit, which takes an input value, the current long term memory value, and the current short term memory value. Now we can use the parameters we defined in the init method to determine what percent of the long term memory should be remembered in the first stage of the LSTM. So we create a variable called long remember percent."
3090,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,588.0,622.0,34.0," And then we multiply the short term memory by its associated weight, and then add that to the input value multiplied by its associated weight. And then we add that to the bias. And finally, run the whole sum through a sigmoid activation function. Now let's code the second stage in the LSTM where we create a potential long term memory and determine what percentage should be remembered. First, let's calculate the percent of the potential long term memory that we want to remember."
3091,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,622.0,654.0,32.0," Just like before, we multiply the short term memory and the input by their corresponding weights. And then add both terms to the bias. And then run everything through a sigmoid activation function. Likewise, we calculate the potential long term memory, except this time we run everything through a tan h activation function. Now we can update the long term memory by first scaling it to the percentage that we are supposed to remember."
3092,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,654.0,697.0,43.0," And then adding a percentage of the potential long term memory. And that gives us a new or updated long term memory that we save in updated long memory. Now let's code the third stage of the LSTM where we create a new short term memory and determine what percentage is sent to the output. Just like before, we calculate the percentage to remember and we use that percentage to scale a potential short term memory, the tan h of the updated long term memory, to create a new short term memory. Lastly, we return the updated long and short term memories."
3093,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,697.0,726.0,29.0," In summary, LSTM under score unit contains the code to do all this. The first stage calculates the percentage of the long term memory to remember. The second stage creates a new potential long term memory and determines what percentage of it to remember. Then we update the long term memory. Then, in the third stage, we create a new short term memory and determine what percentage to remember."
3094,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,726.0,761.0,35.0," And, lastly, we return the updated long and short term memories. Bam. Now that we're done writing LSTM under score unit, which does the LSTM math, we can create the forward method to make a forward pass through the unrolled LSTM. For the forward method, the input is an array that contains the first four days of stock market values from either company A or company B. We start by initializing the long and short term memories to zero."
3095,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,761.0,803.0,42.0," Then we put the four days worth of stock market values into individual variables, one per day. Now, given the value for day one, we can pass the variables day one, long underscore memory, and short underscore memory, to LSTM under score unit to do the math. And LSTM under score unit will return the updated long and short term memories. Now, with the stock value from day two and the updated long and short term memories, we call LSTM under score unit again. And LSTM under score unit will return the updated long and short term memories."
3096,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,803.0,851.0,48.0," Then we call LSTM unit with the value from day three and the updated long and short term memories. Lastly, we return the final short term memory as the output. Now that we have the forward method, which can make a forward pass through an unrolled LSTM, we're ready to configure the optimizer. And configuring the optimizer, in this case atom, it's so easy we can just replace the pseudo code with the real code. Now let's talk about the training underscore step function, which we'll use to calculate the loss and log the training progress."
3097,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,851.0,886.0,35.0," The training step method starts out exactly like what we used in the stat quest introduction to coding neural networks with pi torch plus lightning. It takes a batch of training data for one of the two companies and the index for that batch. And then uses the forward method to make a prediction with the training data. And then calculates the loss, which in this case is the sum of the squared residuals. The big difference now, however, is that we are logging the loss so that we can review it later."
3098,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,886.0,930.0,44.0," When we call the log method, which is part of the lightning module that we are inheriting from, then lightning will create a new file in a directory called lightning underscore logs and store whatever we want to keep track of in it. In this case, we're storing the loss using the label train underscore loss. Now that we know how to keep track of the loss after each training step, it might be cool if we can also keep track of the predictions for company A and company B. First, we can use label underscore i to figure out which company we just made a prediction for. If label underscore i is zero, then we just made a prediction for company A."
3099,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,930.0,963.0,33.0," And we can keep track of that predicted value output underscore i by logging under the label out underscore zero. And if label underscore i is not zero, then we just made a prediction for company B. And we can keep track of that predicted value output underscore i by logging under the label out underscore one. Now that we've logged everything we want to keep track of, we can have training underscore step return the loss. Bam."
3100,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,963.0,992.0,29.0," Now, at long last, we've made it through all of the code needed to create an LSTM by hand. We created an initialized the weight and bias tensors in the init method. We did the LSTM math in LSTM underscore unit. We then wrote the forward method to make a forward pass through the unrolled LSTM. Configured the atom optimizer with configure underscore optimizers."
3101,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,992.0,1020.0,28.0," And last but not least, calculated the loss and logged our progress with training underscore step. Bam. Now let's use the new class we just wrote to create a new LSTM that will call model. And just for fun, let's look at the initial predictions made with random weight values. So we start by just printing out a nice little note that says what we want to do."
3102,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1020.0,1054.0,34.0," And then we use the model to make and print a prediction for company A by passing it a tensor containing the values from days one through four. And remember, just like we saw in the previous tutorials, the model will return a tensor that contains the value we want, the prediction as well as a gradient. So we have to call detach to strip off the gradient. And we see the prediction for company A, negative 0.04, isn't horrible and is relatively close to the observed value zero."
3103,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1054.0,1091.0,37.0," Now we make and print out the prediction for company B by plugging in its data for days one through four. And in contrast to what we saw for company A, the prediction for company B, negative 0.04, is horrible compared to the observed value one. Whoa, whoa. So given that the prediction for company B was horrible, we need to train the model. So we start out by creating the training data called inputs from the values from days one through four from both companies."
3104,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1091.0,1119.0,28.0," In this case, the training data consists of two arrays. The first array contains the values from days one through four from company A, and the second array contains the values from days one through four from company B. Then we wrap the arrays in a tensor and save it. Then we create a tensor called labels. And the labels are what we want the LSTM to predict for each company."
3105,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1119.0,1152.0,33.0," In this case, we want the LSTM to predict zero for company A and one for company B. Now we combine the inputs and the labels into a tensor data set called data set. And then we use the tensor data set to create a data loader called data loader. Remember, data loaders are super useful when we have a lot of data because, one, they make it easy to access the data in batches. Two, they make it easy to shuffle the data each e-pock."
3106,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1152.0,1195.0,43.0," And three, they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging. Okay, now that we have our training data wrapped up in a data loader, we are ready to optimize the weights and biases. We start by creating a lightning trainer called trainer and tell it to train for at most 2000 e-pocks. Which means we will do back propagation for every weight and bias using the data from both companies for 2000 times at most. Now we call the trainer's fit method and pass it to LSTM called model and the training data called data loader."
3107,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1195.0,1220.0,25.0," Hey, squash, this might take a minute to run, so if you need a snack, now it would be a good time to get one. Good idea, Norm. Anyway, once it's done, we can print out the predictions just like we did before. And the prediction for company A, 0.43, is pretty bad since it should be zero. This prediction is actually worse than what we started with."
3108,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1220.0,1249.0,29.0," What, what? The good news is that the prediction for company B is, while not awesome, a little better than before. So it seems like we need to do more training. However, since company A's prediction got worse, it would be nice if we could be confident that more training will actually improve both predictions. The good news is that we wrote the loss and the predictions to log files in training under score step."
3109,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1249.0,1289.0,40.0," And that means we can use tensor board to draw cool graphs that tell us what happened during training and give us a sense of whether or not we should try to train more. To launch tensor board to draw graphs from the log files, go to the file menu and select new. And from the sub menu, select terminal. Now navigate to the directory that contains lightning under score logs. Once there, copy this command, tensor board dash dash log derr equals lightning underscore logs, and paste it into the terminal."
3110,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1289.0,1320.0,31.0," After you hit return, tensor board will start and print a URL at the bottom. Copy this URL, click to open up a new tab and paste the URL. And after you hit return, you'll see the graphs that tensor board drew. Note, in order to see all of the graphs, you have to click on the tabs. Anyway, the graph on the left shows the loss values on the y-axis for each step in training on the x-axis."
3111,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1320.0,1358.0,38.0," Note, each epoch consists of two steps, one for the data from company A and another for the data from company B. As we can see, the loss went down a lot at the start, and then kept going down for the rest of training but at a lower rate. Now, at the end of training, the loss hasn't flattened out and looks like it is still going down. And that suggests that maybe we should do more training. The graph in the middle tells us about how the predictions the output values for company A changed during training."
3112,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1359.0,1383.0,24.0," The goal for company A is to predict zero. And we started out with an okay prediction. But then the prediction got really bad after about 1000 steps. But it looks like the prediction for company A has been improving ever since. And, at the end of training, the prediction hadn't flattened and looks like it could keep going down."
3113,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1383.0,1410.0,27.0," And that suggests that maybe we should do more training. Likewise, the graph on the right, the prediction for company B, suggests we should keep training because it looks like it could get closer to the goal value. 1. So let's do 1000 more epochs and see if things get better. One cool thing about using lightning is that we can add more training epochs without having to start over."
3114,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1410.0,1445.0,35.0," This is because lightning saves checkpoints and they allow us to add additional epochs at any point during training. Since we want to add additional training epochs to where we left off, we first get the path to the most recent checkpoints from the lightning trainer we named trainer. We're best underscore model underscore path refers to the most recent checkpoints. And we save that path in a new variable called path to best checkpoint. Now we create a new lightning trainer which we will call trainer just like the old one."
3115,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1445.0,1479.0,34.0," However, unlike the old one, we set max epochs to 3000 because we want to add 1000 more epochs to the 2000 we have already done. Now we call the trainers fit method. And just like before, we pass in the LSTM called model and the training data called data loader. However, now we also set checkpoint path to the last checkpoint so we can start training right where we left off. And that's all we need to do to add more epochs to training."
3116,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1479.0,1510.0,31.0," Bam! Once training is over, we can print out the new predictions just like before. And we see the prediction for company A is getting closer to zero which is good. And the prediction for company B is getting closer to one which is also good. We can also reload the tensorboard browser page to see the updated graphs of the log files. And we see that the loss is still going down which is good."
3117,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1511.0,1544.0,33.0," The prediction for company A is getting closer to zero and the prediction for company B is getting closer to one. Lastly, none of these graphs have flattened out suggesting that the predictions might improve if we do more training. So let's train for 2000 more epochs. First, we get the path to the most recent checkpoint and then we create a new lightning trainer and set max epochs to 5000 because we want to add 2000 more epochs to the 3000 we have already done."
3118,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1544.0,1575.0,31.0," And then we call the fit method with the LSTM, the training data, and the path to the most recent checkpoint. And when we print out the new predictions, we see that the prediction for company A is really close to zero. And the prediction for company B is really close to one. Bam! Now we reload the browser page with tensorboard and we see that each curve has flattened out suggesting we've done enough training."
3119,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1575.0,1601.0,26.0," So we're all done training. Double bam! Now that we know how to create an LSTM from scratch and then train it until it makes good predictions, let's learn how to make our lives a lot easier by just using the PyTorch function in n.lstm. So let's create a new class called Lightning LSTM that inherits from Lightning Module."
3120,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1601.0,1628.0,27.0," Now, just like always, we define the init method. And just like always, let's call the parents init function. However, this time, all we have to do to create an LSTM is called inn.lstm. Input size refers to the number of features or variables we have in the training data. In this example, we only have one feature, the value for each company."
3121,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1628.0,1655.0,27.0," So we set input size equal to one. Hit in size refers to the number of output values we want. And in this example, we just want one output value, the prediction for day five. Note, although we are using the output from the LSTM to make the predictions, it is common to feed the output from the LSTM into the inputs of another neural network like this one."
3122,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1655.0,1683.0,28.0," In this case, the neural network has three inputs. So if we wanted to connect an LSTM's output to it, we would need three output values. So, in this case, we would set hidden size equal to three. And in n.lstm would add extra weights and biases to the LSTM so that it could create three outputs. Anyways, this is all we need to do in the init method."
3123,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1683.0,1713.0,30.0," The forward method is also a little bit simpler. We start by transposing the input. The values from one of the companies for days one through four from being a single row to being a single column with view. We start by specifying how many rows we want, and in order to create a single column with four values, we want one row per value. However, instead of just putting a four here, which would work just fine with our data,"
3124,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1713.0,1746.0,33.0," we can put land input instead, which will create one row per data point, regardless of how many there are. Then we specify how many columns we want. And since we only have one feature and only want one column, we set that to one. Then we pass the transpose data to the LSTM, and save the output values as LSTM underscore out. Note, LSTM out contains the short term memory values from each LSTM unit that we unrolled."
3125,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1746.0,1783.0,37.0," In this case, that means it has four values, because we needed to unroll the LSTM four times for each of the four input values. So we extract the prediction from the last LSTM unit, which is the last element in the array by using negative one as the index. Lastly, we return the prediction. Now, in theory, we could leave configure underscore optimizers exactly the way it was before. However, in order to demonstrate how quickly atom can converge on optimal weights and biases, we've set the learning rate to 0.1,"
3126,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1783.0,1812.0,29.0," which is much larger than the default value we used before, 0.001. Lastly, the training step method is the exact same as what we had before. It calculates the loss and logs our progress. Now we've made it through all the code needed to create an LSTM using NN.LSTM. We created an initialized the LSTM unit in the init method within NN.LSTM."
3127,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1812.0,1847.0,35.0," Made a forward pass through the unrolled LSTM with forward, configured atom by setting the learning rate to 0.1 in configure underscore optimizers. And, last but not least, calculated the loss and logged our progress with training underscore step. Now we're ready to create the model and print out the predictions using the initialized weights and biases, because you never know you might get lucky. And because both of the predictions are pretty far from ideal, let's train the model."
3128,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1847.0,1875.0,28.0," So we create our Lightning Trainer with L.Trainer. Only this time, because we increase the learning rate, which means we take bigger steps towards the optimal weights and biases each epoch, we set it to only do 300 epochs. Also, by default, Lightning updates the log files every 50 steps. However, since we're only doing 300 epochs, we'd only update the log files 12 times,"
3129,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1875.0,1904.0,29.0," and with just 12 points on our graphs and tensor board, they would look lame. So, in order to update the log files more often and draw graphs that are awesome, we tell the trainer to update the log files every two steps or every epoch. Now, just like before, we just call the Fit method and pass it the model and the training data, and print out the predictions using the optimized weights and biases."
3130,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1904.0,1934.0,30.0," And now, after training, we see that the predictions for both companies are really good. The prediction for company A is super close to zero, and the prediction for company B is close to one. Lastly, we can view the log files with tensor board by refreshing the browser. However, because we trained our hand-made L.T.M for 5,000 epochs, but we only trained the nn.L.T.M for 300 epochs, we'll make sure that only version 3 log files,"
3131,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1934.0,1959.0,25.0," the most recent log files, are displayed, so that the x-axis e-pox is scaled properly. Anyway, these are the graphs we end up with. Each graph shows that the results have stabilized because each curve has flattened out. And that suggests that even though we only did 300 epochs, we're done training. Triple B.M."
3132,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1959.0,1982.0,23.0," Now it's time for some shameless self-promotion. If you want to review statistics and machine learning offline, check out the StacQuest PDF study guides and my book. The StacQuest illustrated guide to machine learning at StacQuest.org. There's something for everyone. Hooray! We've made it to the end of another exciting StacQuest."
3133,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1982.0,1999.0,17.0," If you like this StacQuest and want to see more, please subscribe. And if you want to support StacQuest, consider contributing to my Patreon campaign. Becoming a channel member, buying one or two of my original songs or a T-shirt or a hoodie, or just donate. The links are in the description below."
3134,Long Short-Term Memory with PyTorch + Lightning,https://www.youtube.com/watch?v=RHGiXPuo_pI,RHGiXPuo_pI,1999.0,2003.0,4.0," All right. Until next time, Quest ON."
