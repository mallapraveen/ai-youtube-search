url,id,title,start,end,duration,text
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",0.0,23.04,23.04," Wandering around, around them, forest, I won't get lost because of stat quest.  Hello, I'm Josh Starmer and welcome to stat quest.  Today we're going to be starting part one of a series on random forests, and we're  going to talk about building and evaluating random forests."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",23.04,47.92,24.880000000000003," Note, random forests are built from decision trees, so if you don't already know about  those, check out my stat quest and be-fuck.  Decision trees are easy to build, easy to use, and easy to interpret.  But in practice, they are not that awesome.  To quote from the elements of statistical learning, aka the Bible of Machine Learning, trees"
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",47.92,70.84,22.92000000000001," have one aspect that prevents them from being the ideal tool for predictive learning, namely  in accuracy.  In other words, they work great with the data used to create them, but they are not flexible  when it comes to classifying new samples.  The good news is that random forests combine the simplicity of decision trees with flexibility"
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",70.84,91.75999999999999,20.919999999999987," resulting in a vast improvement in accuracy.  So let's make a random forest.  Step one, create a bootstrap dataset.  Imagine that these four samples are the entire dataset that we are going to build a tree  from.  I know it's crazy small, but just pretend for now."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",91.75999999999999,114.08,22.320000000000007," To create a bootstrap dataset that is the same size as the original, we just randomly  select samples from the original dataset.  The important detail is that we're allowed to pick the same sample more than once.  This is the first sample that we randomly select.  So it's the first sample in our bootstrap dataset."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",114.08000000000001,138.84000000000003,24.76000000000002," This is the second randomly selected sample from the original dataset.  So it's the second sample in our bootstrap dataset.  Here's the third randomly selected sample.  So here it is in the bootstrap dataset.  Lastly, here's the fourth randomly selected sample.  Note, it's the same as the third.  And here it is."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",138.84,162.35999999999999,23.519999999999982," Bam, we've created a bootstrap dataset.  Step two for creating a random forest is to create a decision tree using the bootstrap  dataset, but only use a random subset of variables or columns at each step.  In this example, we will only consider two variables or columns at each step."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",162.4,179.28000000000003,16.880000000000024," Note, we'll talk more about how to determine the optimal number of variables to consider later.  Thus, instead of considering all four variables to figure out how to split the root node,  we randomly select two."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",179.28000000000003,203.48,24.19999999999996," In this case, we randomly selected good blood circulation and blocked arteries as candidates for the root node.  Just for the sake of the example, assume that good blood circulation did the best job separating the samples.  Since we used good blood circulation, I'm going to gray it out so that we focus on the remaining variables."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",203.56,226.44,22.879999999999995," Now we need to figure out how to split samples at this node.  Just like for the root, we randomly select two variables as candidates instead of all three remaining columns.  And we just build the tree as usual, but only considering a random subset of variables at each step.  Double bound."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",226.44,250.71999999999997,24.279999999999973," We built a tree, one, using a bootstrap dataset, and two, only considering a random subset of variables at each step.  Here's the tree we just made.  Now, go back to step one and repeat.  Make a new bootstrap dataset and build a tree considering a subset of variables at each step."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",250.79999999999998,275.68,24.880000000000024," Ideally, you do this hundreds of times, but we only have space to show six.  But you get the idea.  Using a bootstrap sample and considering only a subset of variables at each step results in a wide variety of trees.  The variety is what makes random forests more effective than individual decision trees.  Sweet."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",275.68,296.52,20.839999999999975," Now that we've created a random forest, how do we use it?  Well, first we get a new patient.  We've got all the measurements.  And now we want to know if they have heart disease or not.  So we take the data and run it down the first tree that we made."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",296.64,319.96,23.319999999999993," Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, Boo, The first tree says yes.  The patient has heart disease.  And we keep track of that here.  Now we run the data down the second tree that we made.  The second tree also says yes.  And we keep track of that here."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",319.96,342.24,22.28000000000003," And then we repeat for all the trees we made.  After running the data down all of the trees in the random forest, we see which option  received more votes.  In this case, yes, received the most votes.  So we will conclude that this patient has heart disease.  Bam."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",342.24,363.64,21.399999999999977," Oh no, terminology alert.  Boo's strapping the data, plus using the aggregate to make a decision, is called bagging.  OK, now we've seen how to create and use a random forest.  How do we know if it's any good?"
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",363.64,387.28,23.639999999999986," Remember when we created the bootstrapped data set?  We allowed duplicate entries in the bootstrapped data set.  As a result, this entry was not included in the bootstrapped data set.  Typically, about one third of the original data does not end up in the bootstrapped data set."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",387.32,411.35999999999996,24.039999999999964," Here's the entry that didn't end up in the bootstrapped data set.  If the original data set were larger, we'd have more than just one entry over here.  This is called the out of bag data set.  If it were up to me, I would have named it the out of boot data set, since it's the  entries that didn't make it into the bootstrapped data set."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",411.35999999999996,431.91999999999996,20.560000000000002," Unfortunately, it's not up to me.  Since the out of bag data was not used to create this tree, we can run it through and  see if it correctly classifies the sample as no heart disease.  In this case, the tree correctly labels the out of bag sample, no."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",431.91999999999996,455.47999999999996,23.560000000000002," Then we run this out of bag sample through all of the other trees that were built without  it.  This tree incorrectly labeled the out of bag sample, yes.  These trees correctly labeled the out of bag sample, no.  Since the label with the most votes wins, it is the label that we assign this out of  bag sample."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",455.48,478.84,23.359999999999957," In this case, the out of bag sample is correctly labeled by the random forest.  We then do the same thing for all of the other out of bag samples for all of the trees.  This out of bag sample was also correctly labeled.  This out of bag sample was incorrectly labeled."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",478.84,501.52,22.680000000000007," Et cetera, et cetera, et cetera.  Ultimately, we can measure how accurate our random forest is by the proportion of out of  bag samples that were correctly classified by the random forest.  The proportion of out of bag samples that were incorrectly classified is the out of  bag error."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",501.56,522.9200000000001,21.36000000000007," OK, we now know how to 1 build a random forest, 2 use a random forest, and 3 estimate the  accuracy of a random forest.  However, now that we know how to do this, we can talk a little more about how to do this."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",522.9200000000001,544.24,21.319999999999936," Remember when we built our first tree and we only use two variables, columns of data,  to make a decision at each step.  Now we can compare the out of bag error for a random forest built using only two variables  per step, to a random forest built using three variables per step."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",544.24,568.48,24.24000000000001," Then we test a bunch of different settings and choose the most accurate random forest.  In other words, 1 we build a random forest, and then 2 we estimate the accuracy of a random  forest.  Then we change the number of variables used per step, and we do this a bunch of times and then  choose the one that is the most accurate."
https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",568.48,590.2,21.720000000000027," Typically, we start by using the square of the number of variables, and then try a few  settings above and below that value.  Triple BAM  Hooray, we've made it to the end of another exciting stat quest.  Toon in next week, and we'll talk about how to deal with missing data and how to cluster  the samples."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,0.0,24.560000000000002,24.560000000000002," Random Force, Part 2, Eipit, who reads true, Stankwist.  Hello, I'm Josh Starmer and welcome to Stankwist.  Today we're doing Random Force Part 2, and we're going to focus on missing data in sample  clustering.  To be honest, the sample clustering aspect of Random Force is my favorite part, so I'm really excited  we're going to cover it.  Here's our data set."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,24.560000000000002,48.72,24.159999999999997," We've got data for four separate patients.  However, for patient number four, we've got some missing data.  Random forests consider two types of missing data.  One, missing data in the original dataset used to create the random forest, and two, missing  data in a new sample that we want to categorize."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,48.72,67.0,18.28," We'll start with this one.  So we want to create a random forest from this data.  However, we don't know if this patient has blocked arteries or their weight.  The general idea for dealing with missing data in this context is to make an initial guess"
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,67.0,91.75999999999999,24.75999999999999," that could be bad, and then gradually refine the guess until it is hopefully a good guess.  Because this person did not have heart disease, the initial and possibly bad, guess for the  blocked arteries value is just the most common value for blocked arteries found in the  other samples that do not have heart disease."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,91.75999999999999,114.88,23.120000000000005," Among the people that do not have heart disease, no is the most common value for blocked  arteries.  It occurs in two out of two samples.  So no is our initial guess.  This weight is numeric.  Our initial guess will be the median value of the patients that did not have heart disease."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,114.92,136.48,21.559999999999988," In this case, the median value is 167.5.  Here's our new dataset with the filled in missing values.  Now we want to refine these guesses.  We do this by first determining which samples are similar to the one with missing data."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,136.48,155.35999999999999,18.879999999999995, So let's talk about how to determine similarity.  Step 1.  Build a random forest.  Step 2.  Run all of the data down all of the trees.  We'll start by running all of the data down the first tree.
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,155.35999999999999,177.95999999999998,22.599999999999994," Do-vid-u-vid-u-vid-u-vid-u-vid-u-vid-u-vid-u-vid-u-vidä½“.  Notice that sample 3 and sample 4 both ended up at the same leaf node.  That means they're similar, at least that cell similarity is defined in random forests."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,177.96,200.00000000000003,22.04000000000002," We keep track of similar samples using a proximity matrix.  The proximity matrix has a row for each sample, and it has a column for each sample.  Because sample 3 and sample 4 ended up in the same leaf node, we put a 1 here."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,200.0,220.44,20.439999999999998," We also put a 1 here, since this position also represents samples 3 and 4.  Because no other pair of samples ended in the same leaf node, our proximity matrix looks  like this after running the samples down the first tree.  Now we run all of the data down the second tree."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,220.84,242.84,22.0," No, samples 2, 3 and 4 all ended up in the same leaf node.  This is what the proximity matrix looked like after running the data down the first tree."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,242.84,267.08,24.23999999999998," And after the second tree, we add 1 to any pair of samples that ended up in the same leaf node.  And samples 3 and 4 ended up in the same node together again.  And sample 2 also ended up in that same node.  Now we run all of the data down the third tree.  And here's the updated proximity matrix."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,267.08,287.28,20.19999999999999," Only samples 3 and 4 ended up in the same leaf node.  Ultimately, we run the data down all the trees and the proximity matrix fills in.  Then we divide each proximity value by the total number of trees.  In this example, assume we had 10 trees."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,287.28,308.44,21.160000000000025," Now we use the proximity values for sample 4 to make better guesses about the missing data.  For blocked arteries, we calculate the weighted frequency of yes and no using proximity values  as the weights.  Yes, occurs in one third of the samples."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,308.96,330.44,21.480000000000018, The weighted frequency for yes is the frequency of yes times the weight for yes.  The weight for yes equals the proximity of yes divided by all of the proximity.  The proximity for yes is the proximity value for sample 2.  The only one with yes.
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,330.44,352.44,22.0," And we divide that by the sum of the number of the number of the proximity values.  So the weight for yes is 0.1.  Thus, the weighted frequency for yes is 0.03.  The weighted frequency for no is the frequency of no, which is 2-thirds, times the weight for no.  Samples 1 and 3 both have no."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,353.44,376.44,23.0," With that in mind, we can plug in the values for the proximity of no, divided by all proximity.  Thus, the weight for no is 0.9.  And we can plug in the values for the proximity of no divided by all proximity.  Thus, the weight for no is 0.9."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,376.44,397.44,21.0," And the weighted frequency for no is 0.6.  No has a way higher weighted frequency, so we'll go with it.  In other words, our new improved and revised guests based on the proximity is no for blocked arteries."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,397.44,419.44,22.0," For weight, we use the proximity to calculate a weighted average.  In this case, the weighted average equals sample 1's weight.  Sample 1's weighted average weight.  Sorry if there's any confusion between a patient's weight or a sample's weight and the weight used in the weighted average."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,420.44,442.44,22.0," To calculate that weight, we start with the proximity for sample 1, divided by the sum of the proximity.  So sample 1's weighted average weight is 0.1.  Here's the weighted value for sample number 2, who weighs 180."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,442.44,466.44,24.0," Here's the weighted average value for sample number 3, who weighs 2.10.  Ultimately, the weighted average of weight is 198.5.  And remember, the weights that we used in the weighted average were based on proximity.  Now that we've revised our guesses a little bit, we do the whole thing over again."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,466.44,490.44,24.0," We build a random forest, run the data through the trees, recalculate the proximity and recalculate the missing values.  We do this six or seven times until the missing values converge.  I.e. no longer change each time we recalculate.  Bam.  Now it's time for an inner loot of awesomeness."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,491.44,514.44,23.000000000000057," Let me show you something super cool we can do with the proximity matrix.  This is the proximity matrix before we divided each value by 10, the number of trees in the pretend random forest.  Just for the sake of easy math, imagine if samples 3 and 4 ended up in the same leaf node in all 10 trees."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,515.44,534.44,19.0," Now we have a 10 here and here.  After dividing by 10, the number of trees in the forest, we see that the largest number in the proximity matrix is 1.  1 in the proximity matrix means the samples are as close as close can be."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,534.44,553.44,19.0, That means 1 minus the proximity values equals distance.  Closes can be equals no distance between and not close equals far away.
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,554.44,577.44,23.0," This is a distance matrix and that means we can draw heat map with it.  If you don't know what a heat map is, check out the stack quest.  I think this is super cool because it means that no matter what the data are, ranks, multiple choice, numeric, etc.  If we can use it to make a tree, we can draw heat map or an MDS plot to show how the samples are related to each other."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,577.44,599.44,22.0," This is awesome.  Dremble, bam.  I think this is a very good example of a tree.  I think this is a very good example of a tree.  I think this is a very good example of a tree.  I think this is a very good example of a tree.  Dremble, bam.  Okay, enough fun stuff.  Let's get back to the missing data problem."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,600.44,623.44,23.0," At long last, we'll talk about the second method.  This is when we have missing data in a new sample that we want to categorize.  Imagine we had already built a random forest with existing data and wanted to classify this new patient.  So we want to know if they have heart disease or not.  But we don't know if they have blocked arteries."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,623.44,647.44,24.0, So we need to make a guess about blocked arteries so we can run the patient down all the trees in the forest.  The first thing we do is create two copies of the data.  One that has heart disease and one that doesn't have heart disease.  Then we use the iterative method we just talked about to make a good guess about the missing values.
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,647.44,668.44,21.0, These are the guesses that we came up with.  Then we run the two samples down the trees in the forest.  And we see which of the two is correctly labeled by the random forest the most times.  This option was correctly labeled yes in all three trees.
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,668.44,690.44,22.0," This option was only correctly labeled no in one tree.  This option wins because it was correctly labeled more than the other option.  Bam.  We filled in the missing data and we've classified our sample.  Hey, we've made it to the end of another exciting stat quest."
https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,StatQuest: Random Forests Part 2: Missing data and clustering,690.44,707.44,17.0," If you like this stat quest and want to see more, please subscribe.  And if you want to support stat quest, consider contributing to my Patreon campaign.  Becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate.  The links are in the description below."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,0.0,19.0,19.0," You don't need a ukulele to do statistics, but it makes it more fun.  Hello, I'm Josh Starmer and welcome to Stack Quest.  Today we're going to talk about how to build, use and evaluate random forests in R."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,19.0,42.0,23.0," This stack quest builds on two stack quests that I've already created that demonstrate the theory behind random forests.  So if you're not familiar with it, check them out.  Just so you know, you can download all the code that I described in this tutorial using the link in the description below.  The first thing we do is load in GGplot 2 so we can draw fancy graphs."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,42.0,64.0,22.0," And when I do that, our prints out a little message, it's no big deal.  Then we load CalPlot which just improves some of GGplot 2's default settings.  It overrides the GG save function and that's fine with me, so no worries here.  The last library we need to load is random forest."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,65.0,84.0,19.0," So we can make random forests.  It also prints out some stuff in red, but it's no big deal we can move on from here.  For this example, we're going to get a real data set from the UCI machine learning repository.  Specifically, we want the heart disease data set."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,85.0,107.0,22.0," So we make a variable called URL and set it to the location of the data we want for our random forest.  And this is how we read the data set into R from the URL.  The head function shows us the first six rows of data.  Unfortunately, none of the columns are labeled.  Wow, wow."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,108.0,128.0,20.0," So we name the columns after the names that were listed on the UCI website.  The UCI website actually lists a whole lot of information about this data.  So it's worth checking out if you haven't done that already.  Hey.  Now when we look at the first six rows with the head function, things look a lot better."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,129.0,144.0,15.0," However, the stir function, which describes the structure of the data, tells us that some of the columns are messed up.  Sex is supposed to be a factor where zero represents female and one represents male."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,144.0,168.0,24.0," CP, aka chest pain, is also supposed to be a factor where levels one through three represent different types of pain and four represents no chest pain.  CA and thaw are correctly called factors, but one of the levels is question mark when we need it to be an NA.  So we've got some cleaning up to do."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,169.0,188.0,19.0," The first thing we do is change the question marks to NA's.  Then, just to make the data easier on the eyes, we convert the zeros in sex to F for female.  And the ones to M for male.  Lastly, we convert the column into a factor."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,190.0,210.0,20.0," Then we convert a bunch of other columns into factors since that's what they're supposed to be.  See the UCI website or the sample code on the stat quest blog for more details.  Since the CA column originally had a question mark in it, rather than NA, our thinks it's a column of strings."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,211.0,231.0,20.0," We correct that assumption by telling our it's a column of integers.  And then we convert it to a factor.  Then we do the exact same thing for thaw.  The last thing we need to do to the data is make HD aka heart disease, a factor that is easy on the eyes."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,232.0,254.0,22.0," Here I'm using a fancy trick with if else to convert the zeros to healthy and the ones to unhealthy.  We could have done a similar trick for sex, but I wanted to show you both ways to convert numbers towards.  Once we're done fixing up the data, we can check that we've made the appropriate changes with the stir function."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,254.0,277.0,23.0," Sex is now a factor with levels F and M.  And everything else looks good too.  Hey, we're done with the boring part. Now we can have some fun.  Since we're going to be randomly sampling things, let's set the seed for the random number generator so that we can reproduce our results."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,277.0,298.0,21.0, Now we impute values for the NA's and the data set with RF impute.  The first argument to RF impute is HD tilde dot.  And that means we want the HD aka heart disease column to be predicted by the data in all of the other columns.
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,299.0,319.0,20.0," Here's where we specify which data set to use. In this case, there's only one data set and it's called data.  Here's where we specify how many random forests RF impute should build to estimate the missing values.  In theory, 4 to 6 iterations is enough."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,319.0,336.0,17.0," Just for fun, I set this parameter, itter equal to 20, but it didn't improve the estimates.  Lastly, we save the results. The data set with imputed values instead of NA's as data dot imputed."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,337.0,361.0,24.0," After each iteration, RF impute prints out the out of bag, OOB, error rate.  This should get smaller if the estimates are improving. Since it doesn't, we can conclude that our estimates are as good as they're going to get with this method.  Here's where we actually build a proper random forest using the random forest function."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,361.0,378.0,17.0," Just like when we imputed values for the NA's, we want to predict HD aka heart disease using all of the other columns in the data set.  However, this time we specify data dot imputed as the data set."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,379.0,395.0,16.0," We also want random forest to return the proximity matrix, who will use this to cluster the samples at the end of the stat quest.  Lastly, we save the random forest and associated data like the proximity matrix as model."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,395.0,417.0,22.0," To get a summary of the random forest and how well it performed, we can just type model on the command prompt and then hit enter.  Here's what gets printed to the screen.  The first thing is the original call to random forest.  Next, we see that the random forest was built to classify samples."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,418.0,440.0,22.0," If we had used the random forest to predict weight or height, it would say regression.  And if we had omitted the thing, the random forest was supposed to predict entirely, it would say unsupervised.  Then it tells us how many trees are in the random forest. The default value is 500."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,441.0,462.0,21.0," Later, we will check to see if 500 trees is enough for optimal classification.  Then it tells us how many variables or columns of data were considered at each internal node.  Classification trees have a default setting of the square root of the number of variables."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,462.0,479.0,17.0," Regression trees have a default setting of the number of variables divided by 3.  Since we don't know if 3 is the best value, we'll fiddle with this parameter later on.  Here's the out of bag, OOB, error estimate."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,479.0,496.0,17.0," This means that 83.5% of the OOB samples were correctly classified by the random forest.  Lastly, we have a confusion matrix.  There were 141 healthy patients that were correctly labeled healthy."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,497.0,517.0,20.0," Prey, there were 27 unhealthy patients that were incorrectly classified as healthy.  There were 23 healthy patients that were incorrectly classified on healthy."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,517.0,535.0,18.0," Lastly, there were 112 unhealthy patients that were correctly classified on healthy.  Prey, to see if 500 trees is enough for optimal classification, we can plot the error rates."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,535.0,559.0,24.0," Here, we created data frame that formats the error rate information so that GG plot 2 will be happy.  This is kind of complicated, so let me walk you through it.  For the most part, this is all based on a matrix within model called ER.rate.  This is what the ER.rate matrix looks like."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,560.0,572.0,12.0," There's one column for the out of bag error rate.  One column for the healthy error rate, i.e. how frequently healthy patients are misclassified."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,572.0,592.0,20.0, Each row reflects the error rates at different stages of creating the random forest.  The first row contains the error rates after making the first tree.
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,592.0,614.0,22.0, The second row contains the error rates after making the first two trees.  The last row contains the error rates after making all 500 trees.  So what we're doing here is making a data frame that looks like this.  There's one column for the number of trees.
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,614.0,638.0,24.0, There's one column for the type of error.  And one column for the actual error value.  And here's the call to GG plot.  Bam.  The blue line shows the error rate when classifying unhealthy patients.  The green line shows the overall out of bag error rate.
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,639.0,660.0,21.0," The red line shows the error rate when classifying healthy patients.  In general, we see the error rates decrease when our random forest has more trees.  If we added more trees with the error rate go down further.  To test this hypothesis, we make a random forest with 1000 trees."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,661.0,680.0,19.0, The out of bag error rate is the same as before.  And the confusion matrix shows that we didn't do a better job classifying patients.  And we can plot the error rates just like before.  Double bam.
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,680.0,701.0,21.0," And we see that the error rates stabilize right after 500 trees.  So adding more trees didn't help, but we would not have known this unless we used more trees.  Now we need to make sure we are considering the optimal number of variables at each internal node in the tree."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,701.0,719.0,18.0," We start by making an empty vector that can hold 10 values.  And then we create a loop that tests different numbers of variables at each step.  Each time we go through the loop, I increase by 1.  It starts at 1 and ends after 10."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,720.0,744.0,24.0," In this line, we are building a random forest using I to determine the number of variables to try at each step.  Specifically, we are setting Mtri equals I and I equals values between 1 and 10.  This is where we store the out of bag error rate after we build each random forest that uses a different value for Mtri."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,745.0,768.0,23.0," This is a bit of complex code.  Here's what's going on.  Temp.model contains a matrix called Er.rate, just like model did before.  And we want to access the value in the last row and in the first column.  Ie, the out of bag error rate when all 1000 trees have been made."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,768.0,791.0,23.0," Now we can print out the out of bag error rates for different values for Mtri.  The third value corresponding to Mtri equals 3, which is the default in this case, has the lowest out of bag error rate.  So the default value was optimal, but we wouldn't have known that unless we tried other values."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,791.0,813.0,22.0," Lastly, we want to use the random forest to draw an Mds plot with samples.  This will show us how they are related to each other.  If you don't know what an Mds plot is, don't freak out, just check out the stack quest on.  We start by using the dist function to make a distance matrix from 1 minus the proximity matrix."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,813.0,835.0,22.0," Then we run CMD scale on the distance matrix.  CMD scale stands for classical multi-dimensional scaling.  Then we calculate the percentage of variation in the distance matrix that the X and Y axes account for.  Again, see the other stack quest for details."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,835.0,853.0,18.0, Then we format the data for Gg plot.  And then we draw the graph with Gg plot.  Triple BAM.  Unhealthy samples are on the left side.  Healthy samples are on the right side.
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,854.0,875.0,21.0," I wonder if patient 253 was misdiagnosed, and actually has heart disease.  Note, the X axis accounts for 47% of the variation in the distance matrix.  The Y axis only accounts for 14% of the variation in the distance matrix."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,875.0,897.0,22.0," That means that the big differences are along the X axis.  Lastly, if we got a new patient and didn't know if they had heart disease and they clustered down here,  we'd be pretty confident that they had heart disease.  Her A, we've made it to the end of another exciting stack quest."
https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,StatQuest: Random Forests in R,897.0,906.0,9.0," If you like this stack quest and want to see more of them, please subscribe.  And if you have any suggestions for future stack quests, well, put them in the comments below."
