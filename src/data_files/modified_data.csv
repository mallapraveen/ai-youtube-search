title,url,id,start,end,text,duration
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,0.0,27.96," Wandering around, around them, forest, I won't get lost because of stat quest. Hello, I'm Josh Starmer and welcome to stat quest. Today we're going to be starting part one of a series on random forests, and we're going to talk about building and evaluating random forests. Note, random forests are built from decision trees, so if you don't already know about those, check out my stat quest and be-fuck.",27.96
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,27.96,53.64," those, check out my stat quest and be-fuck. Decision trees are easy to build, easy to use, and easy to interpret. But in practice, they are not that awesome. To quote from the elements of statistical learning, aka the Bible of Machine Learning, trees have one aspect that prevents them from being the ideal tool for predictive learning, namely in accuracy.",25.68
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,53.64,74.88," in accuracy. In other words, they work great with the data used to create them, but they are not flexible when it comes to classifying new samples. The good news is that random forests combine the simplicity of decision trees with flexibility resulting in a vast improvement in accuracy. So let's make a random forest.",21.239999999999995
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,74.88,91.75999999999999," So let's make a random forest. Step one, create a bootstrap dataset. Imagine that these four samples are the entire dataset that we are going to build a tree from. I know it's crazy small, but just pretend for now. To create a bootstrap dataset that is the same size as the original, we just randomly",16.879999999999995
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,91.75999999999999,114.08000000000001," To create a bootstrap dataset that is the same size as the original, we just randomly select samples from the original dataset. The important detail is that we're allowed to pick the same sample more than once. This is the first sample that we randomly select. So it's the first sample in our bootstrap dataset. This is the second randomly selected sample from the original dataset.",22.32000000000002
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,114.08000000000001,133.68," This is the second randomly selected sample from the original dataset. So it's the second sample in our bootstrap dataset. Here's the third randomly selected sample. So here it is in the bootstrap dataset. Lastly, here's the fourth randomly selected sample. Note, it's the same as the third.",19.599999999999994
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,133.68,156.39999999999998," Note, it's the same as the third. And here it is. Bam, we've created a bootstrap dataset. Step two for creating a random forest is to create a decision tree using the bootstrap dataset, but only use a random subset of variables or columns at each step. In this example, we will only consider two variables or columns at each step.",22.71999999999997
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,156.39999999999998,188.12," In this example, we will only consider two variables or columns at each step. Note, we'll talk more about how to determine the optimal number of variables to consider later. Thus, instead of considering all four variables to figure out how to split the root node, we randomly select two. In this case, we randomly selected good blood circulation and blocked arteries as candidates for the root node. Just for the sake of the example, assume that good blood circulation did the best job separating the samples.",31.720000000000027
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,188.20000000000002,224.24," Just for the sake of the example, assume that good blood circulation did the best job separating the samples. Since we used good blood circulation, I'm going to gray it out so that we focus on the remaining variables. Now we need to figure out how to split samples at this node. Just like for the root, we randomly select two variables as candidates instead of all three remaining columns. And we just build the tree as usual, but only considering a random subset of variables at each step. Double bound.",36.03999999999999
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,224.24,250.79999999999998," Double bound. We built a tree, one, using a bootstrap dataset, and two, only considering a random subset of variables at each step. Here's the tree we just made. Now, go back to step one and repeat. Make a new bootstrap dataset and build a tree considering a subset of variables at each step. Ideally, you do this hundreds of times, but we only have space to show six.",26.559999999999974
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,250.79999999999998,275.68," Ideally, you do this hundreds of times, but we only have space to show six. But you get the idea. Using a bootstrap sample and considering only a subset of variables at each step results in a wide variety of trees. The variety is what makes random forests more effective than individual decision trees. Sweet. Now that we've created a random forest, how do we use it?",24.880000000000024
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,275.68,296.52," Now that we've created a random forest, how do we use it? Well, first we get a new patient. We've got all the measurements. And now we want to know if they have heart disease or not. So we take the data and run it down the first tree that we made. Boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo.",20.839999999999975
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,296.76,308.68," Boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo, boo. The first tree says yes. The patient has heart disease. And we keep track of that. Here. Now we run the data Confederate section.",11.920000000000016
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,308.68,320.0, Now we run the data Confederate section. The second tree that we made. The second tree also says yes. And we keep track of that. Here. And then we repeat for all the trees we made.,11.319999999999993
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,320.0,339.84000000000003," And then we repeat for all the trees we made. After running the data down all of the trees in the random forest, we see which option received more votes. In this case, yes, received the most votes. So we will conclude that this patient has heart disease. Bam.",19.840000000000032
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,339.84000000000003,360.35999999999996," Bam. Oh no, terminology alert. Boots strapping the data, plus using the aggregate to make a decision, is called bagging. OK. Now we've seen how to create and use a random forest. How do we know if it's any good?",20.519999999999925
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,360.35999999999996,387.40000000000003," How do we know if it's any good? Remember when we created the Boots strapped data set? We allowed duplicate entries in the Boots strapped data set. As a result, this entry was not included in the Boots strapped data set. Typically, about one third of the original data does not end up in the Boots strapped data set. Here's the entry that didn't end up in the Boots strapped data set.",27.040000000000077
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,387.40000000000003,408.20000000000005," Here's the entry that didn't end up in the Boots strapped data set. Just. If the original data set were larger, we'd have more than just one entry over here. This is called the Out of Bag Data Set. If it were up to me, I would have named it the Out of Boot Data Set, since it's the entries that didn't make it into the Boots strapped data set.",20.80000000000001
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,408.2,431.96," entries that didn't make it into the Boots strapped data set. Unfortunately, it's not up to me. Since the Out of Bag Data was not used to create this tree, we can run it through and see if it correctly classifies the sample as no hard disease. In this case, the tree correctly labels the Out of Bag sample, no. Then we run this Out of Bag sample through all of the other trees that were built without",23.75999999999999
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,431.96,453.92," Then we run this Out of Bag sample through all of the other trees that were built without it. This tree incorrectly labeled the Out of Bag sample, yes. These trees correctly labeled the Out of Bag sample, no. Since the label with the most votes wins, it is the label that we assign this Out of Bag sample.",21.960000000000036
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,453.92,478.84," sample. In this case, the Out of Bag sample is correctly labeled by the random forest. We then do the same thing for all of the Out of Bag samples for all of the trees. This Out of Bag sample was also correctly labeled. This Out of Bag sample was incorrectly labeled. Etc.",24.91999999999996
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,478.84,493.91999999999996," Etc. Etc. Ultimately, we can measure how accurate our random forest is by the proportion of Out of Bag samples that were correctly classified by the random forest. The proportion of Out of Bag samples that were incorrectly classified is the Out of Bag",15.079999999999984
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,493.91999999999996,514.28," The proportion of Out of Bag samples that were incorrectly classified is the Out of Bag error. Okay. We now know how to 1 build a random forest, 2 use a random forest, and 3 estimate the accuracy of a random forest. However, now that we know how to do this, we can talk a little more about how to do this.",20.360000000000014
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,514.68,544.0799999999999," However, now that we know how to do this, we can talk a little more about how to do this. Remember when we built our first tree and we only use two variables, columns of data, to make a decision at each step. Now we can compare the Out of Bag error for a random forest built using only two variables per step, to a random forest built using three variables per step. And we test a bunch of different settings and choose the most accurate random forest.",29.399999999999977
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,544.08,568.4000000000001," And we test a bunch of different settings and choose the most accurate random forest. In other words, 1 we build a random forest, and then 2 we estimate the accuracy of a random forest. Then we change the number of variables used per step, and we do this a bunch of times and then choose the one that is the most accurate. Typically, we start by using the square of the number of variables, and then try a few",24.32000000000005
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,568.4000000000001,589.24," Typically, we start by using the square of the number of variables, and then try a few settings above and below that value. Triple BAM Hooray, we've made it to the end of another exciting stat quest. Toon in next week, and we'll talk about how to deal with missing data and how to cluster the samples.",20.839999999999918
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,589.24,593.36," the samples. All right, until then, quest on.",4.1200000000000045
"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",https://www.youtube.com/watch?v=J4Wdy0Wc_xQ,J4Wdy0Wc_xQ,590.24,593.36," All right, until then, quest on.",3.1200000000000045
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,0.0,20.400000000000002," Random Force, Part 2, Eipit, who reads true, Stankwist. Hello, I'm Josh Starmer and welcome to Stankwist. Today we're doing Random Force Part 2, and we're going to focus on missing data in sample clustering. To be honest, the sample clustering aspect of Random Force is my favorite part, so I'm really excited we're going to cover it.",20.400000000000002
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,20.400000000000002,37.6," we're going to cover it. Here's our data set. We've got data for four separate patients. However, for patient number four, we've got some missing data. Random forests consider two types of missing data. One, missing data in the original dataset used to create the random forest, and two, missing",17.2
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,37.6,61.839999999999996," One, missing data in the original dataset used to create the random forest, and two, missing data in a new sample that we want to categorize. We'll start with this one. So we want to create a random forest from this data. However, we don't know if this patient has blocked arteries or their weight. The general idea for dealing with missing data in this context is to make an initial guess",24.239999999999995
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,61.839999999999996,91.75999999999999," The general idea for dealing with missing data in this context is to make an initial guess that could be bad, and then gradually refine the guess until it is hopefully a good guess. Because this person did not have heart disease, the initial and possibly bad, guess for the blocked arteries value is just the most common value for blocked arteries found in the other samples that do not have heart disease. Among the people that do not have heart disease, no is the most common value for blocked",29.919999999999995
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,91.75999999999999,108.52," Among the people that do not have heart disease, no is the most common value for blocked arteries. It occurs in two out of two samples. So no is our initial guess. This weight is numeric. Our initial guess will be the median value of the patients that did not have heart disease.",16.760000000000005
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,108.52,136.48," Our initial guess will be the median value of the patients that did not have heart disease. In this case, the median value is 167.5. Here's our new dataset with the filled in missing values. Now we want to refine these guesses. We do this by first determining which samples are similar to the one with missing data. So let's talk about how to determine similarity.",27.959999999999994
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,136.48,164.12," So let's talk about how to determine similarity. Step one, build a random forest. Step two, run all of the data down all of the trees. We'll start by running all of the data down the first tree. Dovity-dovity-dovity-dovity-dovity-dovity-dovity-dovity-dov. Notice that sample three and sample four both ended up at the same leaf node.",27.640000000000015
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,164.12,200.0," Notice that sample three and sample four both ended up at the same leaf node. That means they're similar, at least that cell similarity is defined in random forests. We keep track of similar samples using a proximity matrix. The proximity matrix has a row for each sample, and it has a column for each sample. Because sample three and sample four ended up in the same leaf node, we put a one here. We also put a one here, since this position also represents samples three and four.",35.879999999999995
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,200.0,236.95999999999998," We also put a one here, since this position also represents samples three and four. Because no other pair of samples ended in the same leaf node, our proximity matrix looks like this after running the samples down the first tree. Now we run all of the data down the second tree. Noot, samples two, three and four all ended up in the same leaf node. This is what the proximity matrix looked like after running the data down the first tree.",36.95999999999998
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,236.95999999999998,262.84," This is what the proximity matrix looked like after running the data down the first tree. And after the second tree, we add one to any pair of samples that ended up in the same leaf node. And samples three and four ended up in the same node together again. And sample two also ended up in that same node. Now we run all of the data down the third tree. And here's the updated proximity matrix.",25.879999999999995
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,262.84,287.24," And here's the updated proximity matrix. Only samples three and four ended up in the same leaf node. Ultimately, we run the data down all the trees and the proximity matrix fills in. Then we divide each proximity value by the total number of trees. In this example, assume we had 10 trees. Now we use the proximity values for sample four to make better guesses about the missing data.",24.400000000000034
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,287.24,312.59999999999997," Now we use the proximity values for sample four to make better guesses about the missing data. For blocked arteries, we calculate the weighted frequency of yes and no using proximity values as the weights. Yes, occurs in one third of the samples. No occurs in two thirds of the samples. The weighted frequency for yes is the frequency of yes times the weight for yes.",25.359999999999957
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,312.59999999999997,344.0, The weighted frequency for yes is the frequency of yes times the weight for yes. The weight for yes equals the proximity of yes divided by all of the proximity. The proximity for yes is the proximity value for sample two. The only one with yes. And we divide that by the sum of the proximities for sample four. So the weight for yes is 0.1.,31.400000000000034
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,344.0,367.71999999999997," So the weight for yes is 0.1. Thus, the weighted frequency for yes is 0.03. The weighted frequency for no is the frequency of no, which is two thirds, times the weight for no. The samples one and three both have no. With that in mind, we can plug in the values for the proximity of no divided by all",23.71999999999997
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,367.71999999999997,389.67999999999995," With that in mind, we can plug in the values for the proximity of no divided by all proximities. Thus, the weight for no is 0.9. And the weighted frequency for no is 0.6. No has a way higher weighted frequency so we'll go with it. In other words, our new improved and revised guests based on the proximities is no for",21.95999999999998
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,389.68,414.16," In other words, our new improved and revised guests based on the proximities is no for blocked arteries. For weight, we use the proximities to calculate a weighted average. In this case, the weighted average equals sample ones weight, sample ones weighted average weight. Sorry if there's any confusion between a patient's weight or a sample's weight and",24.480000000000018
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,414.24,437.44000000000005," Sorry if there's any confusion between a patient's weight or a sample's weight and the weight used in the weighted average. To calculate that weight, we start with the proximity for sample one divided by the sum of the proximities. So sample ones weighted average weight is 0.1. Here's the weighted value for sample number two who weighs 180.",23.200000000000045
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,437.44000000000005,467.47999999999996," Here's the weighted value for sample number two who weighs 180. Here's the weighted average value for sample number three who weighs 2.10. Ultimately, the weighted average of weight is 198.5. And remember, the weights that we used in the weighted average were based on proximities. Now that we've revised our guesses a little bit, we do the whole thing over again. We build a random forest, run the data through the trees, recalculate the proximities",30.039999999999907
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,467.48,487.36," We build a random forest, run the data through the trees, recalculate the proximities and recalculate the missing values. We do this six or seven times until the missing values converge. IE no longer change each time we recalculate. Bam! Now it's time for an inner loot of awesomeness.",19.879999999999995
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,487.36,511.96000000000004," Now it's time for an inner loot of awesomeness. Let me show you something super cool we can do with the proximity matrix. This is the proximity matrix before we divided each value by 10, the number of trees in the pretend random forest. Just for the sake of easy math, imagine if samples 3 and 4 ended up in the same leaf node in all 10 trees.",24.600000000000023
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,511.96000000000004,534.96," node in all 10 trees. Now we have a 10 here and here. After dividing by 10, the number of trees in the forest, we see that the largest number in the proximity matrix is 1. 1 in the proximity matrix means the samples are as close as close can be. That means 1 minus the proximity values equals distance.",23.0
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,534.96,569.76," That means 1 minus the proximity values equals distance. This can be equals no distance between and not close equals far away. This is a distance matrix and that means we can draw heat map with it. If you don't know what a heat map is, check out the stack quest. And we can also draw an MDS plot with it. If you don't know what an MDS plot is, well, check out the stack quest.",34.799999999999955
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,569.76,591.0," If you don't know what an MDS plot is, well, check out the stack quest. I think this is super cool because it means that no matter what the data are, ranks, multiple choice, numeric, etc. If we can use it to make a tree, we can draw heat map or an MDS plot to show how the samples are related to each other. This is awesome.",21.24000000000001
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,591.0,604.08," This is awesome. Drip all bound. OK, enough fun stuff. Let's get back to the missing data problem. At long last, we'll talk about the second method. This is when we have missing data in a new sample that we want to categorize.",13.080000000000041
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,604.08,628.88, This is when we have missing data in a new sample that we want to categorize. Imagine we had already built a random forest with existing data and wanted to classify this new patient. So we want to know if they have heart disease or not. But we don't know if they have blocked arteries. So we need to make a guess about blocked arteries so we can run the patient down all the trees in the forest.,24.799999999999955
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,628.88,648.6, in the forest. The first thing we do is create two copies of the data. One that has heart disease and one that doesn't have heart disease. Then we use the iterative method we just talked about to make a good guess about the missing values. These are the guesses that we came up with.,19.720000000000027
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,648.6,675.12, These are the guesses that we came up with. Then we run the two samples down the trees in the forest. And we see which of the two is correctly labeled by the random forest the most times. This option was correctly labeled yes in all three trees. This option was only correctly labeled no in one tree. This option wins because it was correctly labeled more than the other option.,26.519999999999982
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,675.12,700.56, This option wins because it was correctly labeled more than the other option. We filled in the missing data and we've classified our sample. Hey we've made it to the end of another exciting stat quest. If you like this stat quest and want to see more please subscribe. And if you want to support stat quest consider contributing to my Patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just,25.43999999999994
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,700.56,711.6, a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below. Alright until next time quest on.,11.040000000000077
StatQuest: Random Forests Part 2: Missing data and clustering,https://www.youtube.com/watch?v=sQ870aTKqiM,sQ870aTKqiM,708.4,711.6, Alright until next time quest on.,3.2000000000000455
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,0.0,28.0," You don't need a ukulele to do statistics, but it makes it more fun. Hello, I'm Josh Starmer and welcome to Stack Quest. Today we're going to talk about how to build, use and evaluate random forests in R. This stack quest builds on two stack quests that I've already created that demonstrate the theory behind random forests. So if you're not familiar with it, check them out. Just so you know, you can download all the code that I described in this tutorial using the link in the description below.",28.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,28.0,60.0," Just so you know, you can download all the code that I described in this tutorial using the link in the description below. The first thing we do is load in GGplot 2 so we can draw fancy graphs. And when I do that, our prints out a little message, it's no big deal. Then we load CalPlot which just improves some of GGplot 2's default settings. It overrides the GG save function and that's fine with me, so no worries here. The last library we need to load is random forest.",32.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,60.0,85.0," The last library we need to load is random forest. So we can make random forests. It also prints out some stuff in red, but it's no big deal we can move on from here. For this example, we're going to get a real data set from the UCI machine learning repository. Specifically, we want the heart disease data set. So we make a variable called URL and set it to the location of the data we want for our random forest.",25.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,85.0,108.0," So we make a variable called URL and set it to the location of the data we want for our random forest. And this is how we read the data set into R from the URL. The head function shows us the first six rows of data. Unfortunately, none of the columns are labeled. Wow, wow. So we name the columns after the names that were listed on the UCI website.",23.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,108.0,128.0," So we name the columns after the names that were listed on the UCI website. The UCI website actually lists a whole lot of information about this data. So it's worth checking out if you haven't done that already. Hey. Now when we look at the first six rows with the head function, things look a lot better. However, the stir function, which describes the structure of the data, tells us that some of the columns are messed up.",20.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,129.0,169.0," However, the stir function, which describes the structure of the data, tells us that some of the columns are messed up. Sex is supposed to be a factor where zero represents female and one represents male. CP, aka chest pain, is also supposed to be a factor where levels one through three represent different types of pain and four represents no chest pain. CA and thaw are correctly called factors, but one of the levels is question mark when we need it to be an NA. So we've got some cleaning up to do. The first thing we do is change the question marks to NA's.",40.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,169.0,196.0," The first thing we do is change the question marks to NA's. Then, just to make the data easier on the eyes, we convert the zeros in sex to F for female. And the ones to M for male. Lastly, we convert the column into a factor. Then we convert a bunch of other columns into factors since that's what they're supposed to be. See the UCI website or the sample code on the stat quest blog for more details.",27.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,196.0,223.0," See the UCI website or the sample code on the stat quest blog for more details. Since the CA column originally had a question mark in it, rather than NA, our thinks it's a column of strings. We correct that assumption by telling our it's a column of integers. And then we convert it to a factor. Then we do the exact same thing for thaw. The last thing we need to do to the data is make HD aka heart disease, a factor that is easy on the eyes.",27.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,223.0,259.0," The last thing we need to do to the data is make HD aka heart disease, a factor that is easy on the eyes. Here I'm using a fancy trick with if else to convert the zeros to healthy and the ones to unhealthy. We could have done a similar trick for sex, but I wanted to show you both ways to convert numbers towards. Once we're done fixing up the data, we can check that we've made the appropriate changes with the stir function. Sex is now a factor with levels F and M. And everything else looks good too.",36.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,259.0,289.0," And everything else looks good too. Hey, we're done with the boring part. Now we can have some fun. Since we're going to be randomly sampling things, let's set the seed for the random number generator so that we can reproduce our results. Now we impute values for the NA's and the data set with RF impute. The first argument to RF impute is HD tilde dot. And that means we want the HD aka heart disease column to be predicted by the data in all of the other columns.",30.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,289.0,327.0," And that means we want the HD aka heart disease column to be predicted by the data in all of the other columns. Here's where we specify which data set to use. In this case, there's only one data set and it's called data. Here's where we specify how many random forests RF impute should build to estimate the missing values. In theory, 4 to 6 iterations is enough. Just for fun, I set this parameter, itter equal to 20, but it didn't improve the estimates. Lastly, we save the results. The data set with imputed values instead of NA's as data dot imputed.",38.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,328.0,372.0," Lastly, we save the results. The data set with imputed values instead of NA's as data dot imputed. After each iteration, RF impute prints out the out of bag, OOB, error rate. This should get smaller if the estimates are improving. Since it doesn't, we can conclude that our estimates are as good as they're going to get with this method. Here's where we actually build a proper random forest using the random forest function. Just like when we imputed values for the NA's, we want to predict HD aka heart disease using all of the other columns in the data set. However, this time we specify data dot imputed as the data set.",44.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,372.0,407.0," However, this time we specify data dot imputed as the data set. We also want random forest to return the proximity matrix, who will use this to cluster the samples at the end of the stat quest. Lastly, we save the random forest and associated data like the proximity matrix as model. To get a summary of the random forest and how well it performed, we can just type model on the command prompt and then hit enter. Here's what gets printed to the screen. The first thing is the original call to random forest.",35.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,408.0,441.0," The first thing is the original call to random forest. Next, we see that the random forest was built to classify samples. If we had used the random forest to predict weight or height, it would say regression. And if we had omitted the thing, the random forest was supposed to predict entirely, it would say unsupervised. Then it tells us how many trees are in the random forest. The default value is 500. Later, we will check to see if 500 trees is enough for optimal classification.",33.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,441.0,474.0," Later, we will check to see if 500 trees is enough for optimal classification. Then it tells us how many variables or columns of data were considered at each internal node. Classification trees have a default setting of the square root of the number of variables. Regression trees have a default setting of the number of variables divided by 3. Since we don't know if 3 is the best value, we'll fiddle with this parameter later on. Here's the out of bag, OOB, error estimate.",33.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,474.0,509.0," Here's the out of bag, OOB, error estimate. This means that 83.5% of the OOB samples were correctly classified by the random forest. Lastly, we have a confusion matrix. There were 141 healthy patients that were correctly labeled healthy. Prey, there were 27 unhealthy patients that were incorrectly classified as healthy. There were 23 healthy patients that were incorrectly classified on healthy.",35.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,509.0,548.0," There were 23 healthy patients that were incorrectly classified on healthy. Lastly, there were 112 unhealthy patients that were correctly classified on healthy. Prey, to see if 500 trees is enough for optimal classification, we can plot the error rates. Here, we created data frame that formats the error rate information so that GG plot 2 will be happy. This is kind of complicated, so let me walk you through it. For the most part, this is all based on a matrix within model called ER.rate.",39.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,548.0,586.0," For the most part, this is all based on a matrix within model called ER.rate. This is what the ER.rate matrix looks like. There's one column for the out of bag error rate. One column for the healthy error rate, i.e. how frequently healthy patients are misclassified. Each row reflects the error rates at different stages of creating the random forest. The first row contains the error rates after making the first tree.",38.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,586.0,614.0, The first row contains the error rates after making the first tree. The second row contains the error rates after making the first two trees. The last row contains the error rates after making all 500 trees. So what we're doing here is making a data frame that looks like this. There's one column for the number of trees. There's one column for the type of error.,28.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,614.0,634.0, There's one column for the type of error. And one column for the actual error value. And here's the call to GG plot. Bam. The blue line shows the error rate when classifying unhealthy patients. The green line shows the overall out of bag error rate.,20.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,634.0,661.0," The green line shows the overall out of bag error rate. The red line shows the error rate when classifying healthy patients. In general, we see the error rates decrease when our random forest has more trees. If we added more trees with the error rate go down further. To test this hypothesis, we make a random forest with 1000 trees. The out of bag error rate is the same as before.",27.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,661.0,686.0," The out of bag error rate is the same as before. And the confusion matrix shows that we didn't do a better job classifying patients. And we can plot the error rates just like before. Double bam. And we see that the error rates stabilize right after 500 trees. So adding more trees didn't help, but we would not have known this unless we used more trees.",25.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,686.0,716.0," So adding more trees didn't help, but we would not have known this unless we used more trees. Now we need to make sure we are considering the optimal number of variables at each internal node in the tree. We start by making an empty vector that can hold 10 values. And then we create a loop that tests different numbers of variables at each step. Each time we go through the loop, I increase by 1. It starts at 1 and ends after 10.",30.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,716.0,748.0," It starts at 1 and ends after 10. In this line, we are building a random forest using I to determine the number of variables to try at each step. Specifically, we are setting Mtri equals I and I equals values between 1 and 10. This is where we store the out of bag error rate after we build each random forest that uses a different value for Mtri. This is a bit of complex code. Here's what's going on.",32.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,748.0,774.0," Here's what's going on. Temp.model contains a matrix called Er.rate, just like model did before. And we want to access the value in the last row and in the first column. Ie, the out of bag error rate when all 1000 trees have been made. Now we can print out the out of bag error rates for different values for Mtri. The third value corresponding to Mtri equals 3, which is the default in this case, has the lowest out of bag error rate.",26.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,774.0,805.0," The third value corresponding to Mtri equals 3, which is the default in this case, has the lowest out of bag error rate. So the default value was optimal, but we wouldn't have known that unless we tried other values. Lastly, we want to use the random forest to draw an Mds plot with samples. This will show us how they are related to each other. If you don't know what an Mds plot is, don't freak out, just check out the stack quest on. We start by using the dist function to make a distance matrix from 1 minus the proximity matrix.",31.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,805.0,835.0," We start by using the dist function to make a distance matrix from 1 minus the proximity matrix. Then we run CMD scale on the distance matrix. CMD scale stands for classical multi-dimensional scaling. Then we calculate the percentage of variation in the distance matrix that the X and Y axes account for. Again, see the other stack quest for details. Then we format the data for Gg plot.",30.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,835.0,854.0," Then we format the data for Gg plot. And then we draw the graph with Gg plot. Triple BAM. Unhealthy samples are on the left side. Healthy samples are on the right side. I wonder if patient 253 was misdiagnosed, and actually has heart disease.",19.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,854.0,887.0," I wonder if patient 253 was misdiagnosed, and actually has heart disease. Note, the X axis accounts for 47% of the variation in the distance matrix. The Y axis only accounts for 14% of the variation in the distance matrix. That means that the big differences are along the X axis. Lastly, if we got a new patient and didn't know if they had heart disease and they clustered down here, we'd be pretty confident that they had heart disease.",33.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,887.0,909.0," we'd be pretty confident that they had heart disease. Her A, we've made it to the end of another exciting stack quest. If you like this stack quest and want to see more of them, please subscribe. And if you have any suggestions for future stack quests, well, put them in the comments below. Until next time, quest on.",22.0
StatQuest: Random Forests in R,https://www.youtube.com/watch?v=6EXPYzbfLCE,6EXPYzbfLCE,906.0,909.0," Until next time, quest on.",3.0
